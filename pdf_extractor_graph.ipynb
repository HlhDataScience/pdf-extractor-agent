{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30306196-59a7-41cf-91dc-620804dcef45",
   "metadata": {},
   "source": [
    "# Testing Notebook for the app different parts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87d09a5-55bc-40bf-b7f9-20d83a85e338",
   "metadata": {},
   "source": [
    "## 1. Pdf Extractor and test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b2e944-28de-455e-9d89-814971ac489a",
   "metadata": {},
   "source": [
    "IN this part we created a function that host the pdf extractor part. we tested the function with unitest and after checking it, we created the full function within the PdfExtractor.py and within the test.py file we introduced the test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b9c8c74-b06c-49d9-a1df-868b93df016e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0ad7959-8c16-4aa1-9583-35c6225990e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "extractor = PyPDFLoader(file_path=\"2501.00663v1.pdf\")\n",
    "docs = extractor.lazy_load()\n",
    "docs_list = [doc for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45f80d5d-e6a1-4fdd-b06f-33149a32adc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.documents.base.Document'>\n",
      "<class 'langchain_core.documents.base.Document'>\n"
     ]
    }
   ],
   "source": [
    "for doc in docs_list[:2]:\n",
    "    print(type(doc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c763e47-46cb-4b98-bd82-9f7c0ce46b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain_core.documents.base import Document\n",
    "def pdf_extractor(path_pdf: str, extractor: PyPDFLoader)-> List[Document]:\n",
    "    loader = extractor(file_path= path_pdf)\n",
    "    docs = loader.load()\n",
    "    return docs\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac304b19-4392-4015-8ccc-185891c203e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Titans: Learning to Memorize at Test Time\n",
      "Ali Behrouz\n",
      "†\n",
      ", Peilin Zhong\n",
      "†\n",
      ", and Vahab Mirrokni\n",
      "†\n",
      "†\n",
      "Google Research\n",
      "{alibehrouz, peilinz, mirrokni}@google.com\n",
      "Abstract\n",
      "Over more than a decade there has been an extensive research effort of how effectively utilize recurrent models and\n",
      "attentions. While recurrent models aim to compress the data into a fixed-size memory (called hidden state), attention allows\n",
      "attending to the entire context window, capturing the direct dependencies of all tokens. This more accurate modeling\n",
      "of dependencies, however, comes with a quadratic cost, limiting the model to a fixed-length context. We present a new\n",
      "neural long-term memory module that learns to memorize historical context and helps an attention to attend to the\n",
      "current context while utilizing long past information. We show that this neural memory has the advantage of a fast\n",
      "parallelizable training while maintaining a fast inference. From a memory perspective, we argue that attention due to its\n",
      "limited context but accurate dependency modeling performs as a short-term memory, while neural memory due to its\n",
      "ability to memorize the data, acts as a long-term, more persistent, memory. Based on these two modules, we introduce\n",
      "a new family of architectures, called Titans, and present three variants to address how one can effectively incorporate\n",
      "memory into this architecture. Our experimental results on language modeling, common-sense reasoning, genomics,\n",
      "and time series tasks show that Titans are more effective than Transformers and recent modern linear recurrent models.\n",
      "They further can effectively scale to larger than 2M context window size with higher accuracy in needle-in-haystack tasks\n",
      "compared to baselines.\n",
      "1 Introduction\n",
      "“The true art of memory is the art of attention!\"\n",
      "— Samuel Johnson, 1787\n",
      "T\n",
      "ransformers, pure attention-based architectures (Vaswani et al. 2017), have been firmly established as state-of-\n",
      "the-art models in sequence modeling, mainly due to their in-context learning and ability to learn at scale (Kaplan\n",
      "et al. 2020). The primary building blocks of Transformers–attention modules—function as associative memory\n",
      "blocks (Bietti et al. 2024), where they learn to store key-value associations and retrieve them by computing pairwise\n",
      "similarity between queries (i.e., search signals) and keys (i.e., contexts). Accordingly, by design, the output of a Transformer\n",
      "is exclusively conditioned on the direct dependencies of tokens in the current context window. This accurate modeling of\n",
      "dependencies, however, comes with quadratic time and memory complexity in terms of the context length. In complex\n",
      "real-world tasks (e.g., language modeling (N. F. Liu et al. 2024), video understanding (C.-Y. Wu et al. 2019), long-term time\n",
      "series forecasting (H. Zhou et al. 2021)), the context window can become extremely large, making the applicability of\n",
      "Transformers challenging in these downstream tasks.\n",
      "To overcome the scalability issue of Transformers, recent studies aim to design different variants of linear Transform-\n",
      "ers (Kacham, Mirrokni, and P. Zhong 2024; Katharopoulos et al. 2020; S. Yang, B. Wang, Shen, et al. 2024), where softmax is\n",
      "replaced by a kernel function in the attention (see §2.1 for details), resulting in a significant drop in memory consumption.\n",
      "Despite efficiency and the ability to scale to longer context, linear Transformers do not show competitive performance\n",
      "compared to Transformers as the kernel trick makes the model a linear recurrent network, in which the data is compressed\n",
      "into a matrix-valued states (Katharopoulos et al. 2020). This, however, brings a contradictory fact about linear recurrent (or\n",
      "linear Transformers) models: On one hand, we use these linear models to enhance scalability and efficiency (linear vs.\n",
      "quadratic complexity), whose advantages is appeared for very long context; On the other hand, a very long context cannot\n",
      "be properly compressed in a small vector-valued or matrix-valued states (S. Wang 2024).\n",
      "1\n",
      "arXiv:2501.00663v1  [cs.LG]  31 Dec 2024' metadata={'source': '2501.00663v1.pdf', 'page': 0}\n"
     ]
    }
   ],
   "source": [
    "docs = pdf_extractor(path_pdf=\"2501.00663v1.pdf\", extractor=PyPDFLoader)\n",
    "for doc in docs[:1]:\n",
    "    print(doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd771ef9-3d11-4bee-bffb-bf16b3f22dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "from unittest.mock import MagicMock\n",
    "\n",
    "class TestPDFExtractor(unittest.TestCase):\n",
    "    def test_pdf_extractor(self):\n",
    "        # Arrange\n",
    "        mock_path = \"sample.pdf\"\n",
    "        mock_content = \"This is a test document.\"\n",
    "        \n",
    "        # Create a mock PyPDFLoader\n",
    "        mock_loader = MagicMock(spec=PyPDFLoader)\n",
    "        mock_loader_instance = MagicMock()\n",
    "        mock_loader.return_value = mock_loader_instance\n",
    "        \n",
    "        # Mock the loader's load method\n",
    "        mock_doc = Document(page_content=mock_content)\n",
    "        mock_loader_instance.load.return_value = [mock_doc]\n",
    "        \n",
    "        # Act\n",
    "        result = pdf_extractor(path_pdf=mock_path, extractor=mock_loader)\n",
    "        \n",
    "        # Assert\n",
    "        self.assertIsInstance(result, list, \"Result should be a list.\")\n",
    "        self.assertGreater(len(result), 0, \"Result list should not be empty.\")\n",
    "        self.assertIsInstance(result[0], Document, \"Result should contain Document objects.\")\n",
    "        self.assertEqual(result[0].page_content, mock_content, \"Document content should match expected content.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e33afb56-6b9f-4770-82fe-87d75f81f27f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.003s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=1 errors=0 failures=0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unittest.TextTestRunner().run(unittest.TestLoader().loadTestsFromTestCase(TestPDFExtractor))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ae3f69-74c8-426d-9104-cd92c7142d4d",
   "metadata": {},
   "source": [
    "## LLM model and test for it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e24aab0-dbc0-457a-8cfa-58e822964b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "\n",
    "_set_env(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2810b87f-6042-4500-9007-0744503d86e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.LlmModel import State, process_pdf, extract_information\n",
    "from src.PydanticSchema import BigQueryEntry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb97538a-6a8f-43b3-9648-96dbdcbf6d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path =\"2501.00663v1.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c12140f-c5ed-4cd7-b5c3-7050f34ed95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_state = State(\n",
    "    pdf_text = \"\",\n",
    "    extracted_info = None,\n",
    "    error=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dab277be-d557-4190-b816-f609d9ee6c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_state = process_pdf(state=initial_state, pdf_path= pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "561bdcaa-be4a-439d-b04a-7b85cbed6090",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted = extract_information(state=text_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ca3dd67-6ada-47d9-8e20-9fc721830cd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'document_id': 'arXiv_2501.00663v1',\n",
       " 'title': 'Titans: Learning to Memorize at Test Time',\n",
       " 'publication_date': '2024-12-31',\n",
       " 'authors': ['Ali Behrouz', 'Peilin Zhong', 'Vahab Mirrokni'],\n",
       " 'Key_words': ['Titans',\n",
       "  'neural memory',\n",
       "  'long-term memory',\n",
       "  'attention',\n",
       "  'language modeling',\n",
       "  'sequence modeling',\n",
       "  'test-time learning'],\n",
       " 'key_points': ['Introduces a new neural long-term memory module that learns to memorize historical context and supports attention mechanisms.',\n",
       "  'Proposes Titans, a family of architectures combining short-term attention and long-term neural memory.',\n",
       "  \"Demonstrates Titans' scalability to context windows larger than 2 million tokens.\",\n",
       "  'Experimental results show Titans outperform Transformers and modern linear recurrent models in language modeling, reasoning, genomics, and time series tasks.',\n",
       "  'Highlights the importance of interconnected memory modules inspired by human memory systems.'],\n",
       " 'summary': 'The paper presents Titans, a novel family of architectures that integrate a neural long-term memory module with attention mechanisms to address challenges in sequence modeling. Titans are designed to scale efficiently to extremely long contexts while maintaining high accuracy. The study demonstrates their superiority over existing models in various tasks, including language modeling and reasoning.',\n",
       " 'methodology': 'The authors designed a neural long-term memory module inspired by human memory systems, incorporating mechanisms for surprise-based learning, momentum, and adaptive forgetting. They evaluated Titans on diverse benchmarks, including language modeling, reasoning, genomics, and time series forecasting, comparing them against state-of-the-art models.',\n",
       " 'processed_timestamp': '2024-01-21T10:00:00.000Z'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted[\"extracted_info\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a203c738-fe92-402b-aa11-cc5e66f7a33f",
   "metadata": {},
   "source": [
    "## 3. Graph model and test for it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e844fd51-e313-4b12-87b7-b28e86635370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY:  ········\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "\n",
    "_set_env(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f11b2a8-6df7-4f6f-9dc9-42b441339fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.GraphModel import workflow_run, create_extraction_pdf_graph\n",
    "from src.LlmModel import process_pdf, extract_information, State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ace2313-6e7d-4402-bd1f-f284f6bd3bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path =\"2501.00663v1.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d28fad4c-c4b3-4a43-b515-187c1384ff99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36;1m\u001b[1;3m[-1:checkpoint]\u001b[0m \u001b[1mState at the end of step -1:\n",
      "\u001b[0m{}\n",
      "\u001b[36;1m\u001b[1;3m[0:tasks]\u001b[0m \u001b[1mStarting 1 task for step 0:\n",
      "\u001b[0m- \u001b[32;1m\u001b[1;3mprocess_pdf\u001b[0m -> {'pdf_path': '2501.00663v1.pdf',\n",
      " 'state': {'error': None, 'extracted_info': None, 'pdf_text': ''}}\n",
      "\u001b[36;1m\u001b[1;3m[0:writes]\u001b[0m \u001b[1mFinished step 0 with writes to 1 channel:\n",
      "\u001b[0m- \u001b[33;1m\u001b[1;3mprocess_pdf\u001b[0m -> {'state': {'error': None,\n",
      "           'extracted_info': None,\n",
      "           'pdf_text': 'Titans: Learning to Memorize at Test Time\\n'\n",
      "                       'Ali Behrouz\\n'\n",
      "                       '†\\n'\n",
      "                       ', Peilin Zhong\\n'\n",
      "                       '†\\n'\n",
      "                       ', and Vahab Mirrokni\\n'\n",
      "                       '†\\n'\n",
      "                       '†\\n'\n",
      "                       'Google Research\\n'\n",
      "                       '{alibehrouz, peilinz, mirrokni}@google.com\\n'\n",
      "                       'Abstract\\n'\n",
      "                       'Over more than a decade there has been an extensive '\n",
      "                       'research effort of how effectively utilize recurrent '\n",
      "                       'models and\\n'\n",
      "                       'attentions. While recurrent models aim to compress the '\n",
      "                       'data into a fixed-size memory (called hidden state), '\n",
      "                       'attention allows\\n'\n",
      "                       'attending to the entire context window, capturing the '\n",
      "                       'direct dependencies of all tokens. This more accurate '\n",
      "                       'modeling\\n'\n",
      "                       'of dependencies, however, comes with a quadratic cost, '\n",
      "                       'limiting the model to a fixed-length context. We '\n",
      "                       'present a new\\n'\n",
      "                       'neural long-term memory module that learns to memorize '\n",
      "                       'historical context and helps an attention to attend to '\n",
      "                       'the\\n'\n",
      "                       'current context while utilizing long past information. '\n",
      "                       'We show that this neural memory has the advantage of a '\n",
      "                       'fast\\n'\n",
      "                       'parallelizable training while maintaining a fast '\n",
      "                       'inference. From a memory perspective, we argue that '\n",
      "                       'attention due to its\\n'\n",
      "                       'limited context but accurate dependency modeling '\n",
      "                       'performs as a short-term memory, while neural memory '\n",
      "                       'due to its\\n'\n",
      "                       'ability to memorize the data, acts as a long-term, '\n",
      "                       'more persistent, memory. Based on these two modules, '\n",
      "                       'we introduce\\n'\n",
      "                       'a new family of architectures, called Titans, and '\n",
      "                       'present three variants to address how one can '\n",
      "                       'effectively incorporate\\n'\n",
      "                       'memory into this architecture. Our experimental '\n",
      "                       'results on language modeling, common-sense reasoning, '\n",
      "                       'genomics,\\n'\n",
      "                       'and time series tasks show that Titans are more '\n",
      "                       'effective than Transformers and recent modern linear '\n",
      "                       'recurrent models.\\n'\n",
      "                       'They further can effectively scale to larger than 2M '\n",
      "                       'context window size with higher accuracy in '\n",
      "                       'needle-in-haystack tasks\\n'\n",
      "                       'compared to baselines.\\n'\n",
      "                       '1 Introduction\\n'\n",
      "                       '“The true art of memory is the art of attention!\"\\n'\n",
      "                       '— Samuel Johnson, 1787\\n'\n",
      "                       'T\\n'\n",
      "                       'ransformers, pure attention-based architectures '\n",
      "                       '(Vaswani et al. 2017), have been firmly established as '\n",
      "                       'state-of-\\n'\n",
      "                       'the-art models in sequence modeling, mainly due to '\n",
      "                       'their in-context learning and ability to learn at '\n",
      "                       'scale (Kaplan\\n'\n",
      "                       'et al. 2020). The primary building blocks of '\n",
      "                       'Transformers–attention modules—function as associative '\n",
      "                       'memory\\n'\n",
      "                       'blocks (Bietti et al. 2024), where they learn to store '\n",
      "                       'key-value associations and retrieve them by computing '\n",
      "                       'pairwise\\n'\n",
      "                       'similarity between queries (i.e., search signals) and '\n",
      "                       'keys (i.e., contexts). Accordingly, by design, the '\n",
      "                       'output of a Transformer\\n'\n",
      "                       'is exclusively conditioned on the direct dependencies '\n",
      "                       'of tokens in the current context window. This accurate '\n",
      "                       'modeling of\\n'\n",
      "                       'dependencies, however, comes with quadratic time and '\n",
      "                       'memory complexity in terms of the context length. In '\n",
      "                       'complex\\n'\n",
      "                       'real-world tasks (e.g., language modeling (N. F. Liu '\n",
      "                       'et al. 2024), video understanding (C.-Y. Wu et al. '\n",
      "                       '2019), long-term time\\n'\n",
      "                       'series forecasting (H. Zhou et al. 2021)), the context '\n",
      "                       'window can become extremely large, making the '\n",
      "                       'applicability of\\n'\n",
      "                       'Transformers challenging in these downstream tasks.\\n'\n",
      "                       'To overcome the scalability issue of Transformers, '\n",
      "                       'recent studies aim to design different variants of '\n",
      "                       'linear Transform-\\n'\n",
      "                       'ers (Kacham, Mirrokni, and P. Zhong 2024; '\n",
      "                       'Katharopoulos et al. 2020; S. Yang, B. Wang, Shen, et '\n",
      "                       'al. 2024), where softmax is\\n'\n",
      "                       'replaced by a kernel function in the attention (see '\n",
      "                       '§2.1 for details), resulting in a significant drop in '\n",
      "                       'memory consumption.\\n'\n",
      "                       'Despite efficiency and the ability to scale to longer '\n",
      "                       'context, linear Transformers do not show competitive '\n",
      "                       'performance\\n'\n",
      "                       'compared to Transformers as the kernel trick makes the '\n",
      "                       'model a linear recurrent network, in which the data is '\n",
      "                       'compressed\\n'\n",
      "                       'into a matrix-valued states (Katharopoulos et al. '\n",
      "                       '2020). This, however, brings a contradictory fact '\n",
      "                       'about linear recurrent (or\\n'\n",
      "                       'linear Transformers) models: On one hand, we use these '\n",
      "                       'linear models to enhance scalability and efficiency '\n",
      "                       '(linear vs.\\n'\n",
      "                       'quadratic complexity), whose advantages is appeared '\n",
      "                       'for very long context; On the other hand, a very long '\n",
      "                       'context cannot\\n'\n",
      "                       'be properly compressed in a small vector-valued or '\n",
      "                       'matrix-valued states (S. Wang 2024).\\n'\n",
      "                       '1\\n'\n",
      "                       'arXiv:2501.00663v1  [cs.LG]  31 Dec 2024\\n'\n",
      "                       'Furthermore, beyond efficiency, most existing '\n",
      "                       'architectures–ranging from Hopfield Networks (Hopfield '\n",
      "                       '1982) to LSTMs (Jür-\\n'\n",
      "                       'gen Schmidhuber and Hochreiter 1997) and Transformers '\n",
      "                       '(Vaswani et al. 2017)–face challenges when dealing '\n",
      "                       'with general-\\n'\n",
      "                       'ization, length extrapolation, and/or reasoning (Anil '\n",
      "                       'et al. 2022; Qin, Y. Zhong, and Deng 2024), all of '\n",
      "                       'which are inseparable\\n'\n",
      "                       'parts of many hard real-world tasks. Although these '\n",
      "                       'architectures draw inspiration from the human brain, '\n",
      "                       'each of which\\n'\n",
      "                       'are missing: (1) a crucial component for learning '\n",
      "                       'process—such as short-term memory, long-term memory, '\n",
      "                       'meta-memory,\\n'\n",
      "                       'attending to current context, etc. (Cowan 2008); (2) '\n",
      "                       'how these components are interconnected systems that '\n",
      "                       'can operate\\n'\n",
      "                       'independently; and/or (3) the ability to actively '\n",
      "                       'learn from data and memorize the abstraction of past '\n",
      "                       'history. We argue\\n'\n",
      "                       'that in an effective learning paradigm, similar to '\n",
      "                       'human brain, there aredistinct yet interconnected '\n",
      "                       'modules, each of which\\n'\n",
      "                       'is responsible for a component crucial to the learning '\n",
      "                       'process.\\n'\n",
      "                       'Memory Perspective\\n'\n",
      "                       'Memory is a fundamental mental process and is an '\n",
      "                       'inseparable component of human learning (Terry 2017). '\n",
      "                       'Without\\n'\n",
      "                       'a properly functioning memory system, humans and '\n",
      "                       'animals would be restricted to basic reflexes and '\n",
      "                       'stereotyped\\n'\n",
      "                       'behaviors. Accordingly, memory has been the '\n",
      "                       'inspiration for many seminal research in machine '\n",
      "                       'learning literature; e.g.,\\n'\n",
      "                       'Hopfield Networks (Hopfield 1982), LSTMs (Jürgen '\n",
      "                       'Schmidhuber and Hochreiter 1997), and Transformers '\n",
      "                       '(Vaswani et al.\\n'\n",
      "                       '2017).\\n'\n",
      "                       'Taking inspiration from the common definitions of '\n",
      "                       'memory and learning in neuropsychology literature '\n",
      "                       '(Okano, Hirano,\\n'\n",
      "                       'and Balaban 2000), most existing architectures '\n",
      "                       'consider memory as a neural update caused by an input, '\n",
      "                       'and define learning\\n'\n",
      "                       'as a process for acquiring effective and useful '\n",
      "                       'memory, given an objective. In this perspective, '\n",
      "                       'Recurrent Neural Networks\\n'\n",
      "                       '(RNNs) (Williams and Zipser 1989) can be defined as '\n",
      "                       'models with a vector-valued memory module M(also '\n",
      "                       'called hidden\\n'\n",
      "                       'state) with two main steps: Given a new input 𝑥𝑡 at '\n",
      "                       'time 𝑡, the model (1) updates the memory using a '\n",
      "                       'function 𝑓(M𝑡−1,𝑥𝑡)\\n'\n",
      "                       '(with compression); and (2) retrieves the '\n",
      "                       'corresponding memory of input using a function '\n",
      "                       '𝑔(M𝑡,𝑥𝑡)(see §2.1 for details).\\n'\n",
      "                       'Similarly, Transformers can be seen as architectures '\n",
      "                       'with a growing memory and two similar steps. That is, '\n",
      "                       'the pair of key\\n'\n",
      "                       'and value matrices acts as the model’s memory, and the '\n",
      "                       'model: (1) updates the memory by appending the key and '\n",
      "                       'value to\\n'\n",
      "                       'the memory (without compression), and (2) retrieves '\n",
      "                       'query vectors’ corresponding memory by finding the '\n",
      "                       'similarity of\\n'\n",
      "                       'query and key vectors, which is then used to weight '\n",
      "                       'the value vectors for the output.\\n'\n",
      "                       'This perspective, can help us better understand '\n",
      "                       'existing paradigms, their critical differences, and '\n",
      "                       'design more effective\\n'\n",
      "                       'architectures. For example, the main difference '\n",
      "                       'between Transformers (Vaswani et al. 2017) and linear '\n",
      "                       'Transform-\\n'\n",
      "                       'ers (Katharopoulos et al. 2020) is the memory '\n",
      "                       'structure as well as the memory updating step, in '\n",
      "                       'which linear Transformers\\n'\n",
      "                       'compress the historical data into a fixed-size '\n",
      "                       'matrix-valued memory while Transformers keep all '\n",
      "                       'historical data (within\\n'\n",
      "                       'the context length) without any compression. While '\n",
      "                       'both linear Transformers and linear RNNs (including '\n",
      "                       'state space\\n'\n",
      "                       'models) compress the information in memory update '\n",
      "                       'step, the critical difference lies in the structure of '\n",
      "                       'the memory,\\n'\n",
      "                       'where linear RNNs (vs. linear Transformers) use a '\n",
      "                       'vector-valued memory (vs. matrix-valued memory). '\n",
      "                       'Therefore, this\\n'\n",
      "                       'perspective motivates us to ask: (Q1) What constitute '\n",
      "                       'a good structure for the memory? (Q2) What is a proper '\n",
      "                       'memory\\n'\n",
      "                       'update mechanism? and (Q3) What is a good memory '\n",
      "                       'retrieval process?\\n'\n",
      "                       'Revisiting our understanding of human memory, it is '\n",
      "                       'neither a unitary process nor it serves a single '\n",
      "                       'function (Cowan\\n'\n",
      "                       '2008). In fact, memory is a confederation of '\n",
      "                       'systems–e.g., short-term, working, and long-term '\n",
      "                       'memory–each serving a\\n'\n",
      "                       'different function with different neural structures, '\n",
      "                       'and each capable of operating independently '\n",
      "                       '(Willingham 1997). This\\n'\n",
      "                       'fact motivates us to ask: (Q4) How to design an '\n",
      "                       'efficient architecture that incorporates different '\n",
      "                       'interconnected memory\\n'\n",
      "                       'modules. Finally, storing a memory is a neural process '\n",
      "                       'that requires to encode and store the abstraction of '\n",
      "                       'the past. It can\\n'\n",
      "                       'be over-simplification to assume a single vector or a '\n",
      "                       'matrix, whose parameters are encoding the data in a '\n",
      "                       'linear manner,\\n'\n",
      "                       'are enough for storing long-term history. (Q5) Is a '\n",
      "                       'deep memory module needed to effectively '\n",
      "                       'store/remember long\\n'\n",
      "                       'past?\\n'\n",
      "                       'Contributions and Roadmap\\n'\n",
      "                       'In this paper, we aim to answer the above five '\n",
      "                       'questions by designing a long-term neural memory '\n",
      "                       'module, that can\\n'\n",
      "                       'efficiently and effectively learn to memorize at test '\n",
      "                       'time. Building upon its design, we discuss how it can '\n",
      "                       'be incorporated\\n'\n",
      "                       'into an architecture.\\n'\n",
      "                       'Neural Memory (§3). We present a (deep) neural '\n",
      "                       'long-term memory that (as a meta in-context model) '\n",
      "                       'learns how to\\n'\n",
      "                       'memorize/store the data into its parameters at test '\n",
      "                       'time. Inspired by human long-term memory system '\n",
      "                       '(Mandler 2014),\\n'\n",
      "                       '2\\n'\n",
      "                       'we design this memory module so an event that violates '\n",
      "                       'the expectations (being surprising) is more memorable. '\n",
      "                       'To this\\n'\n",
      "                       'end, we measure the surprise of an input with the '\n",
      "                       'gradient of the neural network with respect to the '\n",
      "                       'input in associative\\n'\n",
      "                       'memory loss (see §3.1 for details). To better handle '\n",
      "                       'the limited memory, we present a decaying mechanism '\n",
      "                       'that consider the\\n'\n",
      "                       'proportion of memory size and the amount of data '\n",
      "                       'surprise, resulting in better memory management. We '\n",
      "                       'show that this\\n'\n",
      "                       'decay mechanism is in fact the generalization of '\n",
      "                       'forgetting mechanism in modern recurrent models (Dao '\n",
      "                       'and Gu 2024; Gu\\n'\n",
      "                       'and Dao 2024; S. Yang, Kautz, and Hatamizadeh 2024). '\n",
      "                       'Interestingly, we find that this mechanism is '\n",
      "                       'equivalent to optimizing\\n'\n",
      "                       'a meta neural network with mini-batch gradient '\n",
      "                       'descent, momentum, and weight decay. Building upon '\n",
      "                       'tensorizing\\n'\n",
      "                       'mini-batch gradient descent to use more matmul '\n",
      "                       'operations (Yu Sun et al. 2024), we present a fast and '\n",
      "                       'parallelizable\\n'\n",
      "                       'algorithm to train our deep neural long-term memory.\\n'\n",
      "                       'Titans Architectures (§4). After designing the '\n",
      "                       'long-term neural memory, an important remaining '\n",
      "                       'question is how to\\n'\n",
      "                       'effectively and efficiently incorporate memory into a '\n",
      "                       'deep learning architecture. We present Titans, a '\n",
      "                       'family of deep models\\n'\n",
      "                       'that consists of three hyper-heads: (1) Core: this '\n",
      "                       'module consists of the short-term memory, and is '\n",
      "                       'responsible for the main\\n'\n",
      "                       'flow of processing the data (we use attention with '\n",
      "                       'limited window size); (2) Long-term Memory: this '\n",
      "                       'branch is our neural\\n'\n",
      "                       'long-term memory module that is responsible to '\n",
      "                       'store/remember long past; (3) Persistent Memory: this '\n",
      "                       'is a set of learnable\\n'\n",
      "                       'but date-independent parameters that encodes the '\n",
      "                       'knowledge about a task. Finally, as a proof of '\n",
      "                       'concept, we present three\\n'\n",
      "                       'variants of Titans, in which we incorporate memory as: '\n",
      "                       '(i) a context, (ii) a layer, and (iii) a gated '\n",
      "                       'branch.\\n'\n",
      "                       'Experimental Results (§5). We perform experimental '\n",
      "                       'evaluations on language modeling, commonsense '\n",
      "                       'reasoning, recall-\\n'\n",
      "                       'intensive, needle in haystack, time series '\n",
      "                       'forecasting, and DNA modeling tasks. We observe that '\n",
      "                       'our Titan architecture\\n'\n",
      "                       'outperforms all modern recurrent models as well as '\n",
      "                       'their hybrid variants (combining with sliding-window '\n",
      "                       'attention) across\\n'\n",
      "                       'a comprehensive set of benchmarks. Furthermore, Titans '\n",
      "                       'outperforms Transformers with the same context window, '\n",
      "                       'and\\n'\n",
      "                       'show competitive performance with Transformers that '\n",
      "                       'use the entire context. This results are achieved '\n",
      "                       'while, contrary to\\n'\n",
      "                       'Transformers, Titans scale to larger than 2M context '\n",
      "                       'window size.\\n'\n",
      "                       '2 Preliminaries\\n'\n",
      "                       'I\\n'\n",
      "                       'n this section, we discuss the notation and some '\n",
      "                       'background concepts that we use though the paper. We '\n",
      "                       'let\\n'\n",
      "                       '𝑥 ∈R𝑁×𝑑in be the input, Mbe a neural network (neural '\n",
      "                       'memory module), Q,K,V be the query, key and value\\n'\n",
      "                       'of the attention mechanism, and M be the attention '\n",
      "                       'mask. When segmenting the sequence, we use S(𝑖)to '\n",
      "                       'refer to\\n'\n",
      "                       'the 𝑖-th segment. Through the paper, we abuse the '\n",
      "                       'notation and use subscripts to refer to a specific '\n",
      "                       'element of a matrix,\\n'\n",
      "                       'vector, or segments. For example, we let S(𝑖)\\n'\n",
      "                       '𝑗 be the 𝑗-th token in the 𝑖-th segment. The only '\n",
      "                       'exception is subscripts with 𝑡,\\n'\n",
      "                       'which we reserved to index recurrence over time, or '\n",
      "                       'the state of a neural network at time𝑡. Given a neural '\n",
      "                       'network Nand\\n'\n",
      "                       'a data sample 𝑥, we use N(𝑥)(resp. N∗(𝑥)) to refer to '\n",
      "                       'the forward pass with (resp. without) weight '\n",
      "                       'adjustment. Also, we\\n'\n",
      "                       'abuse the notation and use N(𝑘)to refer to the 𝑘-th '\n",
      "                       'layer of the neural network. In the following, we '\n",
      "                       'first, discuss the\\n'\n",
      "                       'backgrounds for attention and its efficient variants '\n",
      "                       'followed by a review of modern linear RNNs. Finally, '\n",
      "                       'we discuss a\\n'\n",
      "                       'memory perspective of these architectures that '\n",
      "                       'motivates us to design Titans.\\n'\n",
      "                       '2.1 Backgrounds\\n'\n",
      "                       'Attention. Transformers (Vaswani et al. 2017) as the '\n",
      "                       'de facto backbone for many deep learning models are '\n",
      "                       'based on\\n'\n",
      "                       'attention mechanism. Given input 𝑥 ∈R𝑁×𝑑in , causal '\n",
      "                       'attention computes output y ∈R𝑁×𝑑in based on softmax '\n",
      "                       'over input\\n'\n",
      "                       'dependent key, value, and query matrices:\\n'\n",
      "                       'Q = 𝑥WQ, K = 𝑥WK, V = 𝑥WV, (1)\\n'\n",
      "                       'y𝑖 =\\n'\n",
      "                       '𝑖∑︁\\n'\n",
      "                       '𝑗=1\\n'\n",
      "                       'exp\\n'\n",
      "                       '\\x10\\n'\n",
      "                       'Q⊤\\n'\n",
      "                       '𝑖 K𝑗/√𝑑in\\n'\n",
      "                       '\\x11\\n'\n",
      "                       'V𝑗\\n'\n",
      "                       'Í𝑖\\n'\n",
      "                       'ℓ=1 exp\\n'\n",
      "                       '\\x10\\n'\n",
      "                       'Q⊤\\n'\n",
      "                       '𝑖 Kℓ/√𝑑in\\n'\n",
      "                       '\\x11, (2)\\n'\n",
      "                       'where WQ,WK,and WV ∈R𝑑in ×𝑑in are learnable '\n",
      "                       'parameters. Despite the power and effectiveness in '\n",
      "                       'recall, transformers\\n'\n",
      "                       'need at least 𝑁 ×𝑑 operators to calculate the output, '\n",
      "                       'resulting in larger memory consumption and '\n",
      "                       'lower-throughput for\\n'\n",
      "                       'longer sequences.\\n'\n",
      "                       'Efficient Attentions. To improve the memory '\n",
      "                       'consumption and throughput of softmax attention for '\n",
      "                       'longer sequences,\\n'\n",
      "                       'various studies focused on I/O aware implementations '\n",
      "                       'of attention (Dao 2024; Dao, D. Fu, et al. 2022), '\n",
      "                       'designing more\\n'\n",
      "                       '3\\n'\n",
      "                       'efficient attention mechanisms by sparsifying the '\n",
      "                       'attention matrix (B. Chen et al. 2021; Choromanski et '\n",
      "                       'al. 2021; Dai et al.\\n'\n",
      "                       '2019), approximating the softmax (Arora et al. 2024), '\n",
      "                       'or developing kernel-based (linear) attentions '\n",
      "                       '(Aksenov et al. 2024;\\n'\n",
      "                       'Kacham, Mirrokni, and P. Zhong 2024; Schlag, Irie, and '\n",
      "                       'Jürgen Schmidhuber 2021; S. Yang, B. Wang, Shen, et '\n",
      "                       'al. 2024). In\\n'\n",
      "                       'this part, we focus on the later, i.e., linear '\n",
      "                       'attentions, where the softmax in standard attention is '\n",
      "                       'replaced with an alternative\\n'\n",
      "                       'kernel function 𝜙(.,.), such that 𝜙(𝑥,𝑦)= 𝜙(𝑥)𝜙(𝑦). '\n",
      "                       'Accordingly, the attention can be written as:\\n'\n",
      "                       'y𝑖 =\\n'\n",
      "                       '𝑖∑︁\\n'\n",
      "                       '𝑗=1\\n'\n",
      "                       '𝜙(𝑄⊤\\n'\n",
      "                       '𝑖 𝐾𝑗)\\n'\n",
      "                       'Í𝑖\\n'\n",
      "                       'ℓ=1 𝜙(𝑄⊤\\n'\n",
      "                       '𝑖 𝐾ℓ)\\n'\n",
      "                       '𝑉𝑗 =\\n'\n",
      "                       '𝑖∑︁\\n'\n",
      "                       '𝑗=1\\n'\n",
      "                       '𝜙(𝑄𝑖)⊤𝜙(𝐾𝑗)\\n'\n",
      "                       'Í𝑖\\n'\n",
      "                       'ℓ=1 𝜙(𝑄𝑖)⊤𝜙(𝐾ℓ)\\n'\n",
      "                       '𝑉𝑗 =\\n'\n",
      "                       '𝜙(𝑄𝑖)⊤Í𝑖\\n'\n",
      "                       '𝑗=1 𝜙(𝐾𝑗)𝑉𝑗\\n'\n",
      "                       '𝜙(𝑄𝑖)⊤Í𝑖\\n'\n",
      "                       'ℓ=1 𝜙(𝐾ℓ)\\n'\n",
      "                       ', (3)\\n'\n",
      "                       'resulting in a higher-throughput as terms Í𝑖\\n'\n",
      "                       '𝑗=1 𝜙(𝐾𝑗)and Í𝑖\\n'\n",
      "                       'ℓ=1 𝜙(𝐾ℓ)are re-using in each step. When choosing the '\n",
      "                       'kernel\\n'\n",
      "                       'as identity matrix (Yutao Sun et al. 2023), the above '\n",
      "                       'formulation can also be written in a recurrent '\n",
      "                       'format:\\n'\n",
      "                       'M𝑡 = M𝑡−1 +𝐾⊤\\n'\n",
      "                       '𝑡 𝑉𝑡 , (4)\\n'\n",
      "                       'y𝑡 = 𝑄𝑡M𝑡 , (5)\\n'\n",
      "                       'which allows efficient inference for linear '\n",
      "                       'attentions.\\n'\n",
      "                       'Modern Linear Models and Their Memory Perspective. As '\n",
      "                       'discussed earlier, one can define learning as a '\n",
      "                       'process for\\n'\n",
      "                       'acquiring effective and useful memory. Building upon '\n",
      "                       'this, one can see the hidden state of Recurrent Neural '\n",
      "                       'Networks\\n'\n",
      "                       '(RNNs) as a memory unit, which the model aims to '\n",
      "                       'compress the information into. Accordingly, in a '\n",
      "                       'general form of\\n'\n",
      "                       'recurrent neural network, the hidden state can be '\n",
      "                       'treated as a memory unit and the recurrence process '\n",
      "                       'can be split into the\\n'\n",
      "                       'read and write operations in the memory unit. That is, '\n",
      "                       'we let 𝑥 ∈R𝑁×𝑑in be the input, M∈ R𝑑 is the memory '\n",
      "                       'unit, and\\n'\n",
      "                       'y ∈R𝑑in is the output, then the general form of the '\n",
      "                       'recurrent neural network is defined as:\\n'\n",
      "                       'M𝑡 = 𝑓(M𝑡−1,𝑥𝑡), Write Operation (6)\\n'\n",
      "                       'y𝑡 = 𝑔(M𝑡,𝑥𝑡), Read Operation (7)\\n'\n",
      "                       'where 𝑓(.,.)is the read and 𝑔(.,.)is the write '\n",
      "                       'corresponding functions. Note that here the subscript '\n",
      "                       'of M𝑡 shows the state\\n'\n",
      "                       'of the memory at time 𝑡.\\n'\n",
      "                       'In this perspective, the recurrence formula of linear '\n",
      "                       'Transformers (see Equation 4) is equivalent to '\n",
      "                       'additively compress\\n'\n",
      "                       'and write keys and values, (𝐾𝑡,𝑉𝑡), into a '\n",
      "                       'matrix-valued memory unit M𝑡. Therefore, when dealing '\n",
      "                       'with long context\\n'\n",
      "                       'data, this additive nature of the process results in '\n",
      "                       'memory overflow, significantly damaging the '\n",
      "                       'performance of the model.\\n'\n",
      "                       'To address this, studies have focused on two promising '\n",
      "                       'directions: (1) Adding forget mechanism: several '\n",
      "                       'studies have\\n'\n",
      "                       'presented adaptive (data-dependent) forgetting gate '\n",
      "                       'mechanisms for linear models, where it can erase the '\n",
      "                       'memory when it\\n'\n",
      "                       'is needed. As examples of such models, we refer to GLA '\n",
      "                       '(S. Yang, B. Wang, Shen, et al. 2024), LRU (Orvieto et '\n",
      "                       'al. 2023),\\n'\n",
      "                       'Griffin (De et al. 2024), xLSTM (Beck et al. 2024), '\n",
      "                       'and Mamba2 (Dao and Gu 2024), which the later is also '\n",
      "                       'connected to the\\n'\n",
      "                       'discretized version of traditional state space models '\n",
      "                       '(Gu and Dao 2024).(2) Improving the write operation: '\n",
      "                       'To overcome the\\n'\n",
      "                       'additive nature of memory write operation in '\n",
      "                       'traditional recurrent models, Widrow and Hoff (1988) '\n",
      "                       'presented Delta Rule,\\n'\n",
      "                       'in which before adding a memory (i.e., a pair of key '\n",
      "                       'and value), the model first removes its past value. To '\n",
      "                       'enhance the\\n'\n",
      "                       'parallelizable training and scaling, S. Yang, B. Wang, '\n",
      "                       'Yu Zhang, et al. (2024) present a fast paralellizable '\n",
      "                       'algorithm. Finally,\\n'\n",
      "                       'very recently, S. Yang, Kautz, and Hatamizadeh (2024) '\n",
      "                       'improved the DeltaNets by adding a forget gate.\\n'\n",
      "                       'Memory Modules. Memory has always been one of the core '\n",
      "                       'parts of the neural network designs (Graves, Wayne,\\n'\n",
      "                       'and Danihelka 2014; JH Schmidhuber 1992; Jürgen '\n",
      "                       'Schmidhuber and Hochreiter 1997; J. Zhang et al. '\n",
      "                       '2024). The idea of\\n'\n",
      "                       'seeing linear layers as the key-value (associative) '\n",
      "                       'memory system backs to fast weight programs, in which '\n",
      "                       'dynamic fast\\n'\n",
      "                       'programs are incorporated into recurrent neural '\n",
      "                       'networks to serve as writable memory (JH Schmidhuber '\n",
      "                       '1992). The two\\n'\n",
      "                       'learning rules of Hebbian (Hebb 2005) and delta '\n",
      "                       '(Prados and Kak 1989) are the most popular learning '\n",
      "                       'rules for fast weight\\n'\n",
      "                       'programs, which have been extensively explored in '\n",
      "                       'various studies (Irie, Schlag, et al. 2021; '\n",
      "                       'Munkhdalai, Sordoni, et al.\\n'\n",
      "                       '2019; Munkhdalai and H. Yu 2017; Schlag, Irie, and '\n",
      "                       'Jürgen Schmidhuber 2021; JH Schmidhuber 1992; S. Yang, '\n",
      "                       'Kautz, and\\n'\n",
      "                       'Hatamizadeh 2024; S. Yang, B. Wang, Yu Zhang, et al. '\n",
      "                       '2024). All these models, however, are based on '\n",
      "                       'momentary surprise,\\n'\n",
      "                       'missing the token flow in the sequences (see Section '\n",
      "                       '3.1), and most of them lacks a forgetting gate, '\n",
      "                       'resulting in a poor\\n'\n",
      "                       'memory management.\\n'\n",
      "                       'We further discuss the connection of our architectures '\n",
      "                       'with recent models in Appendix C. Additional related '\n",
      "                       'work are\\n'\n",
      "                       'discussed in Appendix A.\\n'\n",
      "                       '4\\n'\n",
      "                       '3 Learning to Memorize at Test Time\\n'\n",
      "                       'T\\n'\n",
      "                       'o overcome the lack of long-term memory and to enable '\n",
      "                       'the model to learn, forget, and retrieve information, '\n",
      "                       'in\\n'\n",
      "                       'this section, we present a neural long-term memory '\n",
      "                       'module, which is a meta models that learns to memorize '\n",
      "                       'at\\n'\n",
      "                       'test time. In Section 3.1, we first discuss the '\n",
      "                       'motivation and the design of the neural memory. In '\n",
      "                       'Section 3.2, we\\n'\n",
      "                       'discuss how our architecture design can benefit from a '\n",
      "                       'fast and parallelizable training. Finally, in Section '\n",
      "                       '3.3, we augment\\n'\n",
      "                       'our architecture using persistent memory module, in '\n",
      "                       'which we use learnable but data-independent parameters '\n",
      "                       'to learn\\n'\n",
      "                       'meta information about the task.\\n'\n",
      "                       '3.1 Long-term Memory\\n'\n",
      "                       'To design a neural long-term memory module, we need a '\n",
      "                       'model that can encode the abstraction of the past '\n",
      "                       'history into its\\n'\n",
      "                       'parameters. An example of this can be LLMs that are '\n",
      "                       'shown to be memorizing their training data (Leybzon '\n",
      "                       'and Kervadec\\n'\n",
      "                       '2024; Schwarzschild et al. 2024; Staab et al. 2024). '\n",
      "                       'Therefore, a simple idea is to train a neural network '\n",
      "                       'and expect it to\\n'\n",
      "                       'memorize its training data. Memorization, however, has '\n",
      "                       'almost always been known as an undesirable phenomena '\n",
      "                       'in\\n'\n",
      "                       'neural networks as it limits the model generalization '\n",
      "                       '(Bayat et al. 2024), causes privacy concerns (Staab et '\n",
      "                       'al. 2024), and\\n'\n",
      "                       'so results in poor performance at test time. Moreover, '\n",
      "                       'the memorization of the training data might not be '\n",
      "                       'helpful at test\\n'\n",
      "                       'time, in which the data might be out-of-distribution. '\n",
      "                       'We argue that, we need an online meta-model that '\n",
      "                       'learns how to\\n'\n",
      "                       'memorize/forget the data at test time. In this setup, '\n",
      "                       'the model is learning a function that is capable of '\n",
      "                       'memorization, but it\\n'\n",
      "                       'is not overfitting to the training data, resulting in '\n",
      "                       'a better generalization at test time.\\n'\n",
      "                       'Learning Process and Surprise Metric. The key idea to '\n",
      "                       'train a long-term memory is to treat its training as '\n",
      "                       'an online\\n'\n",
      "                       'learning problem, in which we aim to compress the past '\n",
      "                       'information 𝑥1,...,𝑥 𝑡−1 into the parameters of our '\n",
      "                       'long-term\\n'\n",
      "                       'neural memory module M𝑡. As discussed earlier, an '\n",
      "                       'event that violates the expectations (i.e., is '\n",
      "                       'surprising) is more\\n'\n",
      "                       'memorable for humans (Mandler 2014). Inspired by this, '\n",
      "                       'a simple definition of surprise for a model can be its '\n",
      "                       'gradient with\\n'\n",
      "                       'respect to the input. The larger the gradient is, the '\n",
      "                       'more different the input data is from the past data. '\n",
      "                       'Accordingly, using\\n'\n",
      "                       'this surprise score, we can update the memory as:\\n'\n",
      "                       'M𝑡 = M𝑡−1 −𝜃𝑡 ∇ℓ(M𝑡−1; 𝑥𝑡)|           {z           }\\n'\n",
      "                       'Surprise\\n'\n",
      "                       '. (8)\\n'\n",
      "                       'This surprise metric, however, can result in missing '\n",
      "                       'important information that comes after a big '\n",
      "                       'surprising moment.\\n'\n",
      "                       'That is, the gradient can become extremely small after '\n",
      "                       'several surprising steps, leading to stocking in a '\n",
      "                       'flat area (i.e., local\\n'\n",
      "                       'minima), and missing information about some parts of '\n",
      "                       'the sequence. From the human memory perspective, an '\n",
      "                       'event might\\n'\n",
      "                       'not consistently surprise us through a long-period of '\n",
      "                       'time although it is memorable. The reason is that the '\n",
      "                       'initial moment\\n'\n",
      "                       'is surprising enough to get our attention through a '\n",
      "                       'long time frame, leading to memorizing the entire time '\n",
      "                       'frame. To\\n'\n",
      "                       'improve the above surprise metric (Equation 8), we '\n",
      "                       'break the surprise metric into (1) past surprise , '\n",
      "                       'which measures the\\n'\n",
      "                       'surprise amount of a very recent past; and (2) '\n",
      "                       'momentary surprise , which measures the surprise of '\n",
      "                       'incoming data:\\n'\n",
      "                       'M𝑡 = M𝑡−1 +𝑆𝑡, (9)\\n'\n",
      "                       '𝑆𝑡 = 𝜂𝑡 𝑆𝑡−1\\n'\n",
      "                       '|{z}\\n'\n",
      "                       'Past Surprise\\n'\n",
      "                       '−𝜃𝑡 ∇ℓ(𝑀𝑡−1; 𝑥𝑡)|          {z          }\\n'\n",
      "                       'Momentary Surprise\\n'\n",
      "                       '. (10)\\n'\n",
      "                       'Interestingly, this formulation is similar to gradient '\n",
      "                       'descent with momentum, where𝑆𝑡 is the momentum '\n",
      "                       'element. Therefore,\\n'\n",
      "                       'the momentum here act as a memory of surprise across '\n",
      "                       'time (sequence length). In this formulation, the term '\n",
      "                       '𝜂𝑡 is a\\n'\n",
      "                       'data-dependent surprise decay (a function of 𝑥𝑡), '\n",
      "                       'controlling how surprise decays over time, and the '\n",
      "                       'term 𝜃𝑡 is controlling\\n'\n",
      "                       'how much of momentary surprise should be incorporated '\n",
      "                       'into the final surprise metric in a data-dependent '\n",
      "                       'manner. This\\n'\n",
      "                       'data-dependency is particularly important in this '\n",
      "                       'design: While surprise of previous tokens might be '\n",
      "                       'needed to affect\\n'\n",
      "                       'the surprise of the next token, it is mostly valid if '\n",
      "                       'all tokens are relevant and are in the same context. '\n",
      "                       'Accordingly, a\\n'\n",
      "                       'data-dependent 𝜂can control if memory needs to: (1) '\n",
      "                       'ignore the last surprise by setting 𝜂𝑡 →0 (possibly '\n",
      "                       'due to the change\\n'\n",
      "                       'of context), or (2) fully incorporate the last '\n",
      "                       'surprise by setting 𝜂𝑡 →1 (possibly as the token is '\n",
      "                       'highly relevant to its recent\\n'\n",
      "                       'past tokens).\\n'\n",
      "                       'Objective. Our above surprise metric is based on a '\n",
      "                       'loss function ℓ(.; .), which is the objective that our '\n",
      "                       'memory is learning\\n'\n",
      "                       'to act as it at test time. That is, our memory module '\n",
      "                       'is a meta model that learns a function based on the '\n",
      "                       'loss function ℓ(.; .).\\n'\n",
      "                       '5\\n'\n",
      "                       'In this work, we focus on associative memory , in '\n",
      "                       'which we aim to store the past data as the pairs of '\n",
      "                       'keys and values. Given\\n'\n",
      "                       '𝑥𝑡, similar to Transformers (Vaswani et al. 2017), we '\n",
      "                       'use two linear layers to project𝑥𝑡 into a key and '\n",
      "                       'value:\\n'\n",
      "                       'k𝑡 = 𝑥𝑡𝑊𝐾, v𝑡 = 𝑥𝑡𝑊𝑉, (11)\\n'\n",
      "                       'where 𝑊𝐾 and 𝑊𝑉 ∈R𝑑in ×𝑑in . Next, we expect our '\n",
      "                       'memory module to learn the associations between keys '\n",
      "                       'and values. To\\n'\n",
      "                       'this end, we define the loss as follows:\\n'\n",
      "                       'ℓ(M𝑡−1; 𝑥𝑡)= ∥M𝑡−1 (k𝑡)−v𝑡∥2\\n'\n",
      "                       '2 (12)\\n'\n",
      "                       'By optimizing the above loss function in the '\n",
      "                       'inner-loop of our meta model (memory), the model '\n",
      "                       'learns how to memorize\\n'\n",
      "                       'the mapping between keys and values at test time. Note '\n",
      "                       'that, similar to meta-learning models (Nichol 2018; '\n",
      "                       'Zintgraf et al.\\n'\n",
      "                       '2019), training of the memory is in the inner-loop, '\n",
      "                       'and so parameters 𝑊𝐾 and 𝑊𝑉 are hyperparameters in the '\n",
      "                       'above loss\\n'\n",
      "                       'function. Accordingly, in the inner loop, we optimize '\n",
      "                       'M’s weights, while in the outer-loop, we optimize '\n",
      "                       'other parameters\\n'\n",
      "                       'of the entire architecture.\\n'\n",
      "                       'Forgetting Mechanism. When dealing with very large '\n",
      "                       'sequences (e.g., millions of tokens), it is crucial to '\n",
      "                       'manage which\\n'\n",
      "                       'past information should be forgotten–even with a deep '\n",
      "                       'or a very large matrix-valued memory. To this end, we '\n",
      "                       'use an\\n'\n",
      "                       'adaptive forgetting mechanism that allows the memory '\n",
      "                       'to forget the information that is not needed anymore, '\n",
      "                       'resulting in\\n'\n",
      "                       'better managing the memory’s limited capacity. That '\n",
      "                       'is, given the next token 𝑥𝑡, we modify the update rule '\n",
      "                       'as:\\n'\n",
      "                       'M𝑡 = (1 −𝛼𝑡)M𝑡−1 +𝑆𝑡, (13)\\n'\n",
      "                       '𝑆𝑡 = 𝜂𝑡𝑆𝑡−1 −𝜃𝑡 ∇ℓ(𝑀𝑡−1; 𝑥𝑡), (14)\\n'\n",
      "                       'where 𝛼𝑡 ∈[0,1]is the gating mechanism that flexibly '\n",
      "                       'controls the memory; i.e., decides how much '\n",
      "                       'information should be\\n'\n",
      "                       'forgotten. For example, it can update the memory '\n",
      "                       'without affecting the past abstraction by letting 𝛼𝑡 '\n",
      "                       '→0, and can clear\\n'\n",
      "                       'the entire memory by letting 𝛼𝑡 →1. Later in this '\n",
      "                       'section, we show that this weight decay mechanism is '\n",
      "                       'closely related to\\n'\n",
      "                       'the gating mechanism in modern RNNs (Dao and Gu 2024; '\n",
      "                       'Orvieto et al. 2023).\\n'\n",
      "                       'Memory Architecture. In this paper, we focus on simple '\n",
      "                       'MLPs with 𝐿M ≥1 layers as the architecture of our '\n",
      "                       'long-term\\n'\n",
      "                       'memory. The main reason behind this choice is that we '\n",
      "                       'want to focus on better motivating the design of the '\n",
      "                       'long-term\\n'\n",
      "                       'memory and ways that it can be incorporated into an '\n",
      "                       'architecture. However, our formulation and '\n",
      "                       'architectural design\\n'\n",
      "                       'opens a new research direction to design neural '\n",
      "                       'architectures that are more effective and efficient in '\n",
      "                       'memorization of data.\\n'\n",
      "                       'Recently, there has been a promising line of work to '\n",
      "                       'design such architectures (Berges et al. 2024; Cetin '\n",
      "                       'et al. 2024; J. Zhang\\n'\n",
      "                       'et al. 2024), which incorporating them into our '\n",
      "                       'framework (i.e., replacing simple MLPs with such '\n",
      "                       'architectures) can be an\\n'\n",
      "                       'interesting future work.\\n'\n",
      "                       'When using vector-valued or matrix-valued memory (De '\n",
      "                       'et al. 2024; Orvieto et al. 2023; S. Yang, B. Wang, '\n",
      "                       'Shen, et\\n'\n",
      "                       'al. 2024), the memory module is compressing the past '\n",
      "                       'data and fit it into a line. That is, from the meta '\n",
      "                       'learning or\\n'\n",
      "                       'online learning perspective (Yu Sun et al. 2024), '\n",
      "                       'using a matrix-valued memory M = 𝑊 ∈R𝑑in ×𝑑in is '\n",
      "                       'equivalent to\\n'\n",
      "                       'optimize ℓ(𝑊𝑡−1; 𝑥𝑡)= ∥𝑊𝑡−1k𝑡 −v𝑡∥2\\n'\n",
      "                       '2, which is an online linear regression objective and '\n",
      "                       'so the optimal solution assumes\\n'\n",
      "                       'the underlying dependency of historical data is '\n",
      "                       'linear. On the other hand, we argue that deep memory '\n",
      "                       'modules (i.e.,\\n'\n",
      "                       '𝐿M ≥2) . Aligning with the theoretical results that '\n",
      "                       'MLPs with at least two layers are strictly more '\n",
      "                       'expressive than linear\\n'\n",
      "                       'models (Hornik, Stinchcombe, and White 1989), in '\n",
      "                       'Section 5.5, we show that deep memory modules are more '\n",
      "                       'effective in\\n'\n",
      "                       'practice.\\n'\n",
      "                       'Retrieving a Memory. In the above, we discuss how one '\n",
      "                       'can design and train a long-term memory module that '\n",
      "                       'learns to\\n'\n",
      "                       'memorize at test time. A key remaining question is: '\n",
      "                       'How one can retrieve information from the memory? We '\n",
      "                       'simply use the\\n'\n",
      "                       'forward pass without weight update (i.e., inference) '\n",
      "                       'to retrieve a memory correspond to a query. Formally, '\n",
      "                       'given an input\\n'\n",
      "                       '𝑥𝑡, we use a linear layer 𝑊𝑄 to project the input, '\n",
      "                       'i.e., q𝑡 = 𝑥𝑡𝑊𝑄 and retrieve the corresponding (or '\n",
      "                       'useful) information\\n'\n",
      "                       'from the memory 𝑦𝑡 by:\\n'\n",
      "                       '𝑦𝑡 = M∗(q𝑡). (15)\\n'\n",
      "                       '6\\n'\n",
      "                       'Figure 1: The illustration of how the training of '\n",
      "                       'neural memory can be done in parallel and using '\n",
      "                       'matmuls.\\n'\n",
      "                       '3.2 How to Parallelize the Long-term Memory Training\\n'\n",
      "                       'As discussed above, the design of our long-term memory '\n",
      "                       'module is equivalent to training a meta model by '\n",
      "                       'optimizing\\n'\n",
      "                       'associative memory loss function ℓ(M𝑡−1; 𝑥𝑡)= ∥M𝑡−1 '\n",
      "                       '(k𝑡)−v𝑡∥2\\n'\n",
      "                       '2 using gradient descent with momentum and weight\\n'\n",
      "                       'decay. Therefore, in theory, the training of long-term '\n",
      "                       'memory module requires O(𝑁)FLOPs, where 𝑁 is the '\n",
      "                       'sequence\\n'\n",
      "                       'length. However, in practice, we need to parallelize '\n",
      "                       'the training process and to fully take advantage of '\n",
      "                       'hardware accelerators\\n'\n",
      "                       '(e.g., TPUs, GPUs), we need to tensorize the process '\n",
      "                       'and use more matmuls.\\n'\n",
      "                       'Next, we show that calculating the weights in the '\n",
      "                       'inner loop with mini-batch gradient descent, '\n",
      "                       'data-dependent learning\\n'\n",
      "                       'rate, and weight decay can be reformulated so that it '\n",
      "                       'uses only matmuls and sum. We build upon the work of '\n",
      "                       'Yu Sun et al.\\n'\n",
      "                       '(2024) that shows forward pass of a model optimizing '\n",
      "                       'with the mini-batch gradient descent (with constant '\n",
      "                       'learning rate)\\n'\n",
      "                       'can be calculated using matmuls. We can split the '\n",
      "                       'sequence into chunks of size 𝑏 ≥1, and write the '\n",
      "                       'mini-batch gradient\\n'\n",
      "                       'descent as:\\n'\n",
      "                       'M𝑡 = (1 −𝛼𝑡)M𝑡−1 −𝜃𝑡∇ℓ(M𝑡−1; 𝑥𝑡)= 𝛽𝑡M0 −\\n'\n",
      "                       '𝑡∑︁\\n'\n",
      "                       '𝑖=1\\n'\n",
      "                       '𝜃𝑖\\n'\n",
      "                       '𝛽𝑡\\n'\n",
      "                       '𝛽𝑖\\n'\n",
      "                       '∇ℓ(M𝑡′; 𝑥𝑖), (16)\\n'\n",
      "                       'where 𝑡′= 𝑡 −mod(𝑡,𝑏), and 𝛽𝑖 = Î𝑖\\n'\n",
      "                       '𝑗=1 (1 −𝛼𝑗). For the sake of simplicity, we focus on '\n",
      "                       'the first chunk, i.e., 𝑡 = 𝑏and so\\n'\n",
      "                       '𝑡′= 0. Also, we explain the process for the case that '\n",
      "                       'M𝑡 = 𝑊𝑡 is linear. The process for MLPs with 𝑁𝑝 ≥2 is '\n",
      "                       'similar. Using\\n'\n",
      "                       'our loss function, we have:\\n'\n",
      "                       '∇ℓ(𝑊0; 𝑥𝑡)= (𝑊0𝑥𝑡 −𝑥𝑡)𝑥⊤\\n'\n",
      "                       '𝑡 ⇒\\n'\n",
      "                       '𝑏∑︁\\n'\n",
      "                       '𝑖=1\\n'\n",
      "                       '𝜃𝑖\\n'\n",
      "                       '𝛽𝑏\\n'\n",
      "                       '𝛽𝑖\\n'\n",
      "                       '∇ℓ(𝑊0; 𝑥𝑖)= Θ𝑏B𝑏(𝑊0𝑋 −𝑋)𝑋⊤, (17)\\n'\n",
      "                       'where Θ𝑏 = diag \\x00\\x02𝜃1 𝜃2 ... 𝜃 𝑏\\n'\n",
      "                       '\\x03\\x01 and B𝑏 is defined analogously on 𝛽𝑏\\n'\n",
      "                       '𝛽𝑖\\n'\n",
      "                       's. Note that, we do not need to store allΘ𝑘𝑏 and\\n'\n",
      "                       'B𝑘𝑏 for 𝑘 = 1,...,𝑁 /𝑏, instead, we store these '\n",
      "                       'matrices for each chunk, resulting in using less '\n",
      "                       'memory. Next, we extend\\n'\n",
      "                       'this representation so we can also incorporate the '\n",
      "                       'momentum term. In a chunk wise gradient descent with '\n",
      "                       'momentum, if\\n'\n",
      "                       'we look at the momentum term, we have:\\n'\n",
      "                       '𝑆𝑡 = 𝜂𝑡𝑆𝑡−1 −𝜃𝑡 𝑢𝑡, (18)\\n'\n",
      "                       'where 𝑢𝑡 = ∇ℓ(𝑀𝑡′; 𝑥𝑡). Note that, we can compute all '\n",
      "                       '𝑢𝑡 at the same time, and so Equation 18 is a linear '\n",
      "                       'recurrence\\n'\n",
      "                       'with 𝑢𝑡 as an input, 𝑆𝑡 as the hidden state, and 𝜂𝑡 as '\n",
      "                       'input-dependent transition value. Accordingly, we can '\n",
      "                       'use parallel\\n'\n",
      "                       'associative scan (J. T. Smith, Warrington, and '\n",
      "                       'Linderman 2023) to calculate𝑆𝑡s in this chunk.\\n'\n",
      "                       'Parameters as the Function of Chunks. Instead of '\n",
      "                       'making parameters like𝛼𝑡,𝜃𝑡, and 𝜂𝑡 input-dependent '\n",
      "                       '(i.e., a function\\n'\n",
      "                       'of token 𝑥𝑡), we can make them functions of their '\n",
      "                       'chunk. Despite losing expressive power, this '\n",
      "                       'formulation can help to\\n'\n",
      "                       'make the training even faster. In this case, we are '\n",
      "                       'using the same value for each of 𝛼, 𝜃, and 𝜂in each '\n",
      "                       'chunk. Accordingly,\\n'\n",
      "                       'in Equation 17, we can store Θ using a single scaler. '\n",
      "                       'Similarly we can make Equation 18 faster. That is, '\n",
      "                       'when 𝜂and 𝜃 are\\n'\n",
      "                       'learnable but time-invariant inside each chunk, this '\n",
      "                       'equation becomes a linear time-invariant system (LTI), '\n",
      "                       'which can be\\n'\n",
      "                       'computed by a global convolution (Gu, Goel, and Re '\n",
      "                       '2022). In our experiments, we make these parameters as '\n",
      "                       'the functions\\n'\n",
      "                       'of tokens. However, such simplifications (i.e., as the '\n",
      "                       'function of chunks) can be the interest of future work '\n",
      "                       'to training\\n'\n",
      "                       'larger models in more efficient manner.\\n'\n",
      "                       '7\\n'\n",
      "                       'Figure 2: Memory as a Context (MAC) Architecture. This '\n",
      "                       'architecture includes three branches of (1) core, (2) '\n",
      "                       'contextual\\n'\n",
      "                       '(long-term) memory, and (3) persistent memory. The '\n",
      "                       'core branch concatenates the corresponding long-term '\n",
      "                       'and persistent\\n'\n",
      "                       'memories with the input sequence. Next, attention '\n",
      "                       'performs on the sequence and decides what part of the '\n",
      "                       'information\\n'\n",
      "                       'should store in the long-term memory. At the test '\n",
      "                       'time, parameters corresponds to contextual memory are '\n",
      "                       'still learning,\\n'\n",
      "                       'parameters corresponds to the core branch are '\n",
      "                       'responsible for in-context learning, and parameters of '\n",
      "                       'persistent memory\\n'\n",
      "                       'are responsible to store the knowledge about tasks and '\n",
      "                       'so are fixed.\\n'\n",
      "                       '3.3 Persistent Memory\\n'\n",
      "                       'Our long-term memory can also be seen as a contextual '\n",
      "                       'memory, meaning that the output is fully depend on the '\n",
      "                       'context.\\n'\n",
      "                       'Therefore, in addition to our long-term memory, we '\n",
      "                       'also use a set of learnable but input-independent '\n",
      "                       'parameters to act as\\n'\n",
      "                       'task-related memory. This type of memory has been '\n",
      "                       'referred to as persistent or meta-memory in the '\n",
      "                       'literature (X. Dong\\n'\n",
      "                       'et al. 2024; Sukhbaatar, Grave, et al. 2019). Given 𝑁𝑝 '\n",
      "                       '≥1, we use learnable parameters 𝑃 =\\n'\n",
      "                       '\\x02𝑝1 𝑝2 ... 𝑝 𝑁𝑝\\n'\n",
      "                       '\\x03\\n'\n",
      "                       'and\\n'\n",
      "                       'append it to the start of our sequence: i.e., given a '\n",
      "                       'context window size of 𝑁, we modify the input as:\\n'\n",
      "                       '𝑥new =\\n'\n",
      "                       '\\x02𝑝1 𝑝2 ... 𝑝 𝑁𝑝\\n'\n",
      "                       '\\x03\\n'\n",
      "                       '|| 𝑥, (19)\\n'\n",
      "                       'where ||is concatenation. Next, we discuss the '\n",
      "                       'motivation of persistent memory from three '\n",
      "                       'perspective:\\n'\n",
      "                       'Memory Perspective. As discussed earlier, our neural '\n",
      "                       'long-term memory is a contextual memory, in which all '\n",
      "                       'parameters\\n'\n",
      "                       'are input-dependent. An effective memory system, '\n",
      "                       'however, also needs input-independent parameters to '\n",
      "                       'store the\\n'\n",
      "                       'abstraction of the task knowledge. That is, mastering '\n",
      "                       'a task requires the memorization of the knowledge that '\n",
      "                       'how the task\\n'\n",
      "                       'can be done, and these parameters are responsible for '\n",
      "                       'storing such knowledge.\\n'\n",
      "                       'Feedforward Network Perspective. In the Transformer '\n",
      "                       'architectures, there are fully connected layers after '\n",
      "                       'the attention\\n'\n",
      "                       'module, which are shown to be similar to attention '\n",
      "                       'weights but with data-independent parameters. That is, '\n",
      "                       'Sukhbaatar,\\n'\n",
      "                       'Grave, et al. (2019) showed that replacing the ReLU in '\n",
      "                       'fully connected layers with Softmax can results in an '\n",
      "                       'attention-like\\n'\n",
      "                       'weights, in which weights are data-independent:\\n'\n",
      "                       '𝐹𝐹𝑁 (𝑥)= 𝑊𝑉 Softmax (𝑊𝐾𝑥). (20)\\n'\n",
      "                       'In fact, 𝑊𝐾 and 𝑊𝑉 are acting similar to 𝐾 and 𝑉 '\n",
      "                       'matrices in attention module when they are '\n",
      "                       'input-independent. The\\n'\n",
      "                       'persistent memory weights are expected to have the '\n",
      "                       'same functionality, meaning that using them in the '\n",
      "                       'first part of the\\n'\n",
      "                       'sequence leads to having input-independent attention '\n",
      "                       'weights (Sukhbaatar, Grave, et al. 2019).\\n'\n",
      "                       'Technical Perspective. Attention with causal mask has '\n",
      "                       'implicit bias toward initial tokens in the sequence, '\n",
      "                       'and so attention\\n'\n",
      "                       'weights are almost always highly active for initial '\n",
      "                       'tokens, resulting in performance damage. From the '\n",
      "                       'technical perspective,\\n'\n",
      "                       'these learnable parameters at the start of the '\n",
      "                       'sequence can mitigate such effect by redistributing '\n",
      "                       'the attention weights\\n'\n",
      "                       'more effectively (Han et al. 2024; Xiao et al. 2024).\\n'\n",
      "                       '8\\n'\n",
      "                       '(a) Memory as a Context (MAC). We segment the '\n",
      "                       'sequence\\n'\n",
      "                       'and use full causal attention in each window. Again, '\n",
      "                       'the first\\n'\n",
      "                       '𝑁𝑝 tokens are persistent memory and the next 𝑁𝑙 are '\n",
      "                       'long-term\\n'\n",
      "                       'memory tokens\\n'\n",
      "                       '(b) Memory as Gating (MAG). We use sliding window '\n",
      "                       'attention\\n'\n",
      "                       '(SWA) as a short-term memory and our neural memory '\n",
      "                       'module\\n'\n",
      "                       'as a long-term memory, combining by a gating.\\n'\n",
      "                       'Figure 3: Attention masks for different variants of '\n",
      "                       'Titans.\\n'\n",
      "                       '4 How to Incorporate Memory?\\n'\n",
      "                       'A\\n'\n",
      "                       'n important question that remained unanswered is: How '\n",
      "                       'one can effectively and efficiently incorporate the\\n'\n",
      "                       'designed neural memory into a deep learning '\n",
      "                       'architecture? As discussed earlier, from a memory '\n",
      "                       'perspective,\\n'\n",
      "                       'the pair of K and V matrices in transformers can be '\n",
      "                       'interpreted as an associative memory block. Due to '\n",
      "                       'their\\n'\n",
      "                       'accurate modeling of dependencies and so their limited '\n",
      "                       'context window, we interpret them as short-term memory '\n",
      "                       'modules,\\n'\n",
      "                       'attending to the current context window size. On the '\n",
      "                       'other hand, our neural memory with the ability to '\n",
      "                       'continuously\\n'\n",
      "                       'learn from data and store it in its weights can play '\n",
      "                       'the role of a a long-term memory. In this section, we '\n",
      "                       'aim to answer\\n'\n",
      "                       'the above question by proposing three different '\n",
      "                       'variants of Titans. Later in our experiments, we show '\n",
      "                       'that each of these\\n'\n",
      "                       'variants has its own advantages/disadvantages and also '\n",
      "                       'can show a trade-off between the efficiency and '\n",
      "                       'effectiveness in\\n'\n",
      "                       'very long-contexts.\\n'\n",
      "                       '4.1 Memory as a Context\\n'\n",
      "                       'In the first architecture design (see Figure 2), we '\n",
      "                       'treat the memory as a context to the current '\n",
      "                       'information. That is, given\\n'\n",
      "                       'a long sequence 𝑥 ∈R𝑁×𝑑in , we first chunk the '\n",
      "                       'sequence into fixed-size segments S(𝑖) for 𝑖 = 1,...,𝑁 '\n",
      "                       '/𝐶. Given the\\n'\n",
      "                       'incoming segment S(𝑡), we consider it as the current '\n",
      "                       'context and its past segment as the historical '\n",
      "                       'information. Therefore,\\n'\n",
      "                       'let M𝑡−1 be the state of long-term memory before '\n",
      "                       'segment S(𝑡), we use the input context as the query to '\n",
      "                       'the memory\\n'\n",
      "                       'M𝑡−1 to retrieve the corresponding information from '\n",
      "                       'the long-term memory. That is, we retrieve the past '\n",
      "                       'information that\\n'\n",
      "                       'corresponds to S(𝑡)as:\\n'\n",
      "                       'ℎ𝑡 = M∗\\n'\n",
      "                       '𝑡−1 (q𝑡), (21)\\n'\n",
      "                       'where q𝑡 = S(𝑡)𝑊𝑄. Next, we use this historical '\n",
      "                       'information along with our persistent memory '\n",
      "                       'parameters as the input\\n'\n",
      "                       'sequence to the attention module:\\n'\n",
      "                       '˜S\\n'\n",
      "                       '(𝑡)\\n'\n",
      "                       '=\\n'\n",
      "                       '\\x02𝑝1 𝑝2 ... 𝑝 𝑁𝑝\\n'\n",
      "                       '\\x03\\n'\n",
      "                       '|| ℎ𝑡 || S(𝑡), (22)\\n'\n",
      "                       '𝑦𝑡 = Attn\\n'\n",
      "                       '\\x10\\n'\n",
      "                       '˜S\\n'\n",
      "                       '(𝑡)\\x11\\n'\n",
      "                       '. (23)\\n'\n",
      "                       'The structure of the attention map over the entire '\n",
      "                       'sequence is shown in Figure 3a. We then use 𝑦𝑡 to '\n",
      "                       'update the long-term\\n'\n",
      "                       'memory module for the next segment and the final '\n",
      "                       'output:\\n'\n",
      "                       'M𝑡 = M𝑡−1 (𝑦𝑡), (24)\\n'\n",
      "                       '𝑜𝑡 = 𝑦𝑡 ⊗M∗\\n'\n",
      "                       '𝑡 (𝑦𝑡). (25)\\n'\n",
      "                       'Note that, in the above, we are updating the weight of '\n",
      "                       'M𝑡−1 through forward pass.\\n'\n",
      "                       'This architecture has two key advantages: (1) '\n",
      "                       'Attention by having both historical and current '\n",
      "                       'context, has the ability to\\n'\n",
      "                       'decides whether given the current data, the long-term '\n",
      "                       'memory information is needed. (2) The attention module '\n",
      "                       'helps\\n'\n",
      "                       '9\\n'\n",
      "                       'Figure 4: Memory as a Gate (MAG) Architecture. This '\n",
      "                       'architecture, similarly, has the three branches of (1) '\n",
      "                       'core, (2)\\n'\n",
      "                       'contextual memory, and (3) persistent memory. It, '\n",
      "                       'however, incorporates only persistent memory into the '\n",
      "                       'context and\\n'\n",
      "                       'combine memory with the core branch using a gating '\n",
      "                       'mechanism. At test time, the behavior is the same as '\n",
      "                       'Figure 2.\\n'\n",
      "                       'the long-term memory to store only useful information '\n",
      "                       'from the current context. That is, not all tokens in '\n",
      "                       'each segment\\n'\n",
      "                       'are useful and memorizing all of them can result in '\n",
      "                       'memory overflow. Therefore, attention is helping the '\n",
      "                       'memory to\\n'\n",
      "                       'understand which information is useful, better '\n",
      "                       'managing the memory capacity. (3) At test time: (i) '\n",
      "                       'persistent memory\\n'\n",
      "                       'parameters are fixed as they encodes the knowledge '\n",
      "                       'about the task, which should not be changed; (ii) the '\n",
      "                       'attention module\\n'\n",
      "                       'weights are in-context learner; and (iii) the '\n",
      "                       'long-term memory module is still learning (memorizing) '\n",
      "                       'the information at test\\n'\n",
      "                       'time. That is, we update the weights of the neural '\n",
      "                       'memory even at test time as weights are encoding the '\n",
      "                       'abstraction of\\n'\n",
      "                       'long past.\\n'\n",
      "                       '4.2 Gated Memory\\n'\n",
      "                       'In the next variant (see Figure 4), in one branch, we '\n",
      "                       'directly use the input data to update the long-term '\n",
      "                       'memory, and in the\\n'\n",
      "                       'second branch, we use a sliding window attention '\n",
      "                       '(SWA):\\n'\n",
      "                       '˜𝑥 =\\n'\n",
      "                       '\\x02𝑝1 𝑝2 ... 𝑝 𝑁𝑝\\n'\n",
      "                       '\\x03\\n'\n",
      "                       '|| 𝑥, (26)\\n'\n",
      "                       '𝑦 = SW-Attn∗(˜𝑥), (27)\\n'\n",
      "                       '𝑜 = 𝑦⊗M( ˜𝑥), (28)\\n'\n",
      "                       'where SW-Attn∗is sliding window attention with prefix '\n",
      "                       '(see Figure 3b). Note that, contrary to the previous '\n",
      "                       'design, we are\\n'\n",
      "                       'not segmenting the input data. Also, we abuse the '\n",
      "                       'notation and use M(𝑥)to refer to the final output of '\n",
      "                       'the memory after\\n'\n",
      "                       'all recursion over the tokens of the sequence. In the '\n",
      "                       'above equation, ⊗can be any non-linear gating. In our '\n",
      "                       'experiments,\\n'\n",
      "                       'we normalize the outputs 𝑦and M(˜𝑥)using learnable '\n",
      "                       'vector-valued weights, followed by a non-linearity '\n",
      "                       '𝜎(.).\\n'\n",
      "                       'The overall attention mask of this design is shown in '\n",
      "                       'Figure 3b. In this design, sliding window attention is '\n",
      "                       'act as a precise\\n'\n",
      "                       'short-term memory, while the neural memory module is '\n",
      "                       'acting as a fading memory for the model. This '\n",
      "                       'architecture design\\n'\n",
      "                       'can also be seen as a multi-head architecture where '\n",
      "                       'the structure of heads are different (X. Dong et al. '\n",
      "                       '2024).\\n'\n",
      "                       '4.3 Memory as a Layer\\n'\n",
      "                       'The last variant uses the neural Memory As a Layer '\n",
      "                       '(MAL) of a deep neural network (see Figure 5). This '\n",
      "                       'architecture\\n'\n",
      "                       'design is more common in the literature, where the '\n",
      "                       'hybrid models stack recurrent models with full or '\n",
      "                       'sliding window\\n'\n",
      "                       'attentions. Given input 𝑥, we have:\\n'\n",
      "                       '˜𝑥 =\\n'\n",
      "                       '\\x02𝑝1 𝑝2 ... 𝑝 𝑁𝑝\\n'\n",
      "                       '\\x03\\n'\n",
      "                       '|| 𝑥, (29)\\n'\n",
      "                       '𝑦 = M(˜𝑥), (30)\\n'\n",
      "                       '𝑜 = SW-Attn (𝑦), (31)\\n'\n",
      "                       '10\\n'\n",
      "                       'Figure 5: Memory as a Layer (MAL) Architecture. In '\n",
      "                       'this architecture, the memory layer is responsible to '\n",
      "                       'compress the\\n'\n",
      "                       'past and current context before the attention module.\\n'\n",
      "                       'where SW-Attn is sliding window attention. The main '\n",
      "                       'drawback of this design is that the power of the model '\n",
      "                       'is limited by\\n'\n",
      "                       'each of the layers and so it cannot take advantage of '\n",
      "                       'the complementary data processing of attention and '\n",
      "                       'neural memory\\n'\n",
      "                       'module. In our experiments, for evaluating memory in '\n",
      "                       'this design, we use a similar architecture as H3 (D. '\n",
      "                       'Y. Fu et al. 2023),\\n'\n",
      "                       'where we replace the the sequence model with our '\n",
      "                       'neural memory module (LMM).\\n'\n",
      "                       'Memory Without Attention. Although in the above, we '\n",
      "                       'discussed MAL as the combination of LMMs and attention '\n",
      "                       'in\\n'\n",
      "                       'a sequential manner, one simple variant of MAL is to '\n",
      "                       'treat LMM as a sequence model without any attention. '\n",
      "                       'From the\\n'\n",
      "                       'memory perspective, as discussed in Section 1, we '\n",
      "                       'expect each part of the memory system to work '\n",
      "                       'independently, even if\\n'\n",
      "                       'other components are disturbed. Therefore, a long-term '\n",
      "                       'memory module should still be a powerful model even '\n",
      "                       'without\\n'\n",
      "                       'short-term memory (i.e., attention). We refer to this '\n",
      "                       'variant as LMM or Titans (LMM) in our experiments. We '\n",
      "                       'provide\\n'\n",
      "                       'additional discussions on the connection of Titans and '\n",
      "                       'other modern recurrent models in Appendix C.\\n'\n",
      "                       '4.4 Architectural Details\\n'\n",
      "                       'For the sake of simplicity and presentation, we avoid '\n",
      "                       'discussing the implementation details like using '\n",
      "                       'residual connection,\\n'\n",
      "                       'gating with linear layer, and normalization. In all '\n",
      "                       'blocks, we use residual connections. In our '\n",
      "                       'implementation, we use\\n'\n",
      "                       'SiLU(.) activation (Elfwing, Uchibe, and Doya 2018) as '\n",
      "                       'the non-linear activation for computing query, key, '\n",
      "                       'and values and\\n'\n",
      "                       'normalize queries and keys using ℓ2-norm.\\n'\n",
      "                       'Convolution. Following the recent modern linear '\n",
      "                       'recurrent models (Gu and Dao 2024; S. Yang, Kautz, and '\n",
      "                       'Hatamizadeh\\n'\n",
      "                       '2024), we incorporate a 1D depthwise-separable '\n",
      "                       'convolution layer after each of the query, key, and '\n",
      "                       'value projections.\\n'\n",
      "                       'While not significantly affect the performance, these '\n",
      "                       '1D convolutions have shown performance improvement and '\n",
      "                       'are also\\n'\n",
      "                       'computationally efficient.\\n'\n",
      "                       'Gating. We also follow the recent architectures that '\n",
      "                       'use normalization and gating with a linear layer '\n",
      "                       'before the final\\n'\n",
      "                       'output projection (Mehta et al. 2023).\\n'\n",
      "                       'Theorem 4.1. Contrary to Transformers, diagonal linear '\n",
      "                       'recurrent models, and DeltaNet, all of which are '\n",
      "                       'limited to TC0 (Merrill,\\n'\n",
      "                       'Petty, and Sabharwal 2024), Titans are capable of '\n",
      "                       'solving problems beyond TC 0, meaning that Titans are '\n",
      "                       'theoretically more\\n'\n",
      "                       'expressive than Transformers and most modern linear '\n",
      "                       'recurrent models in state tracking tasks.\\n'\n",
      "                       '5 Experiments\\n'\n",
      "                       'N\\n'\n",
      "                       'ext, we evaluate the performance of Titans and its '\n",
      "                       'variants in language modeling, commonsense reasoning, '\n",
      "                       'needle\\n'\n",
      "                       'in haystack, DNA modeling, and time series forecasting '\n",
      "                       'tasks1. In more details, in this section, we answer '\n",
      "                       'the\\n'\n",
      "                       'following empirical questions: (1) How do Titans '\n",
      "                       'perform compared to baselines in downstream tasks? '\n",
      "                       '(see §5.2,\\n'\n",
      "                       '1In the first version of the work, we aim to provide '\n",
      "                       'insights/evidences about why the learning paradigms of '\n",
      "                       'Titans are effective. We are working on\\n'\n",
      "                       'finalizing the results of larger models and will '\n",
      "                       'report them in the next version.\\n'\n",
      "                       '11\\n'\n",
      "                       '§5.6, and §5.7); (2) What is the actual context length '\n",
      "                       'of Titans? (see §5.3 and §5.4); (3) How do Titans '\n",
      "                       'scale with respect to\\n'\n",
      "                       'context length? (see §5.8); (4) How the depth of '\n",
      "                       'memory can affect both performance and efficiency? '\n",
      "                       '(see §5.5); and (5)\\n'\n",
      "                       'What is the contribution of each Titans’ component in '\n",
      "                       'its performance? (see §5.9).\\n'\n",
      "                       '5.1 Experimental Setup\\n'\n",
      "                       'Models. In our experiments, we focus on the three '\n",
      "                       'variants of Titans, which we refer to as: Titans with '\n",
      "                       '(1) Memory as a\\n'\n",
      "                       'Context (MAC), (2) Memory as a Gate (MAG), and (3) '\n",
      "                       'Memory as a Layer (MAL) as well as (4) neural memory '\n",
      "                       'module\\n'\n",
      "                       'alone. The reason behind using our long-term memory as '\n",
      "                       'a separate module is based on our definition of '\n",
      "                       'learning. As\\n'\n",
      "                       'discussed in Section 1, we define learning a process '\n",
      "                       'for acquiring effective and useful memory. '\n",
      "                       'Accordingly, we expect our\\n'\n",
      "                       'long-term memory to effectively learn from data, even '\n",
      "                       'without attention. For each of these models, we '\n",
      "                       'consider four scales\\n'\n",
      "                       'with: (i) 170M, (ii) 340M, (iii) 400M, and (iv) 760M '\n",
      "                       'parameters. While the first three are trained on 15B '\n",
      "                       'tokens sampled\\n'\n",
      "                       'from FineWeb-Edu dataset (Penedo et al. 2024), the '\n",
      "                       'last one is trained on 30B tokens from the same '\n",
      "                       'dataset.\\n'\n",
      "                       'Baselines. We compare our models with the '\n",
      "                       'state-of-the-art linear recurrent models, '\n",
      "                       'Transformers, and hybrid models\\n'\n",
      "                       '(recurrent + attention). More specifically in language '\n",
      "                       'tasks, we compare with Transformer++ (Touvron et al. '\n",
      "                       '2023),\\n'\n",
      "                       'RetNet (Yutao Sun et al. 2023), Gated Linear Attention '\n",
      "                       '(GLA) (S. Yang, B. Wang, Shen, et al. 2024), Mamba (Gu '\n",
      "                       'and Dao\\n'\n",
      "                       '2024), Mamba2 (Dao and Gu 2024), DeltaNet (S. Yang, B. '\n",
      "                       'Wang, Yu Zhang, et al. 2024), TTT (Yu Sun et al. '\n",
      "                       '2024), and Gated\\n'\n",
      "                       'DeltaNet (S. Yang, Kautz, and Hatamizadeh 2024). In '\n",
      "                       'needle in haystack tasks, we also compare with GPT4 '\n",
      "                       '(Achiam et al.\\n'\n",
      "                       '2023), Llama3 with RAG (Touvron et al. 2023), '\n",
      "                       'RecurrentGemma2-9B (Botev et al. 2024), and Mistral '\n",
      "                       '(Jiang et al. 2023)\\n'\n",
      "                       'models, all of which are provided in the benchmark '\n",
      "                       '(Yuri Kuratov et al. 2024). In time series tasks, we '\n",
      "                       'compare with\\n'\n",
      "                       'Mamba-based (Behrouz, Santacatterina, and Zabih 2024), '\n",
      "                       'Transformer-based (Y. Liu et al. 2023; Nie et al. '\n",
      "                       '2022; Yunhao\\n'\n",
      "                       'Zhang and Yan 2023), and linear models (Das et al. '\n",
      "                       '2023; Z. Li et al. 2023; H. Wu et al. 2023; Zeng et '\n",
      "                       'al. 2023).\\n'\n",
      "                       'Training. In the training, we follow the training '\n",
      "                       'procedure of S. Yang, Kautz, and Hatamizadeh (2024), '\n",
      "                       'and use LLama 2\\n'\n",
      "                       'tokenizer with a vocabulary size of 32K and use '\n",
      "                       'training length of 4K tokens. We employ AdamW '\n",
      "                       'optimizer with learning\\n'\n",
      "                       'rate of 4𝑒-4 with cosine annealing schedule with batch '\n",
      "                       'size of 0.5M tokens, and weight decay of 0.1.\\n'\n",
      "                       '5.2 Language Modeling\\n'\n",
      "                       'We first focus on the perplexity in language modeling '\n",
      "                       'and also commonsense reasoning tasks. The results for '\n",
      "                       'Titans’\\n'\n",
      "                       'variants and also baselines with three different sizes '\n",
      "                       'of 340M, 400M, and 760M are reported in Table 1. Among '\n",
      "                       'non-hybrid\\n'\n",
      "                       'models, including Transformer++, our neural memory '\n",
      "                       'module achieves the best performance in both '\n",
      "                       'perplexity and\\n'\n",
      "                       'accuracy measures. Comparing our neural memory module '\n",
      "                       'and TTT, which is also a gradient-based recurrent '\n",
      "                       'model can\\n'\n",
      "                       'show us the importance of our weight decay as well as '\n",
      "                       'the momentum. As discussed earlier, the weight decay '\n",
      "                       'can be\\n'\n",
      "                       'interpreted as a gating mechanism to forget the past '\n",
      "                       'data, when it is needed. Also, momentum can help us '\n",
      "                       'better manage\\n'\n",
      "                       'the memory by providing additional memory for the '\n",
      "                       'surprise metric. While some baselines also take '\n",
      "                       'advantage of gating\\n'\n",
      "                       'mechanism, e.g., Mamba, Mamba2, and Gated DeltaNet, '\n",
      "                       'the superior performance of our neural memory module '\n",
      "                       'shows\\n'\n",
      "                       'the importance of both our surprise mechanism and '\n",
      "                       'having deep and non-linear memory. We further discuss '\n",
      "                       'the later in\\n'\n",
      "                       'Section 5.5.\\n'\n",
      "                       'Comparing the hybrid models, we found that all three '\n",
      "                       'variants of Titans (MAC, MAG, and MAL) outperform both '\n",
      "                       'Samba\\n'\n",
      "                       '(Mamba + attention) and Gated DeltaNet-H2 (Gated '\n",
      "                       'DeltaNet + atttention). We attribute the superior '\n",
      "                       'performance of Titans\\n'\n",
      "                       '(MAL) to the power of neural memory module as the '\n",
      "                       'architecture design and used attention are all the '\n",
      "                       'same. Comparing\\n'\n",
      "                       'Titans (MAG) and (MAC), we find that while their '\n",
      "                       'performance are close, MAC performs better when '\n",
      "                       'dealing with longer\\n'\n",
      "                       'dependencies in the data. Interestingly, both MAG and '\n",
      "                       'MAC outperform MAL variant, which due to using the '\n",
      "                       'same\\n'\n",
      "                       'modules, we attribute this to the architecture design '\n",
      "                       'of these models. This finding is particularly '\n",
      "                       'important as the current\\n'\n",
      "                       'hybrid models (except Hymba (X. Dong et al. 2024)) in '\n",
      "                       'the literature are using MAL-style combination of '\n",
      "                       'recurrent models\\n'\n",
      "                       'and attention.\\n'\n",
      "                       '5.3 Needle in a Haystack\\n'\n",
      "                       'Scaling a model to longer context window is not always '\n",
      "                       'equivalent to being effective for very long sequences '\n",
      "                       '(Hsieh\\n'\n",
      "                       'et al. 2024). The needle-in-a-haystack (NIAH) task is '\n",
      "                       'designed to measure the actual effective context '\n",
      "                       'length of models.\\n'\n",
      "                       'In this task, we evaluate the model on retrieving a '\n",
      "                       'piece of information (i.e., the “needle”) from long '\n",
      "                       'distractor texts (i.e.,\\n'\n",
      "                       '12\\n'\n",
      "                       'Table 1: Performance of Titans and recurrent- and '\n",
      "                       'Transformer-based baselines on language modeling and '\n",
      "                       'common-sense\\n'\n",
      "                       'reasoning tasks. Hybrid models are marked with ∗. The '\n",
      "                       'best results among simple and hybrid models are '\n",
      "                       'highlighted.\\n'\n",
      "                       'Model Wiki. LMB. LMB. PIQA Hella. Wino. ARC-e ARC-c '\n",
      "                       'SIQA BoolQ Avg.\\n'\n",
      "                       'ppl↓ ppl↓ acc↑ acc↑ acc_n↑ acc↑ acc↑ acc_n↑ acc↑ acc↑ '\n",
      "                       '↑\\n'\n",
      "                       '340M params / 15B tokens\\n'\n",
      "                       'Transformer++ 31.52 41.08 30.76 62.98 34.76 50.53 '\n",
      "                       '45.21 24.05 36.81 58.24 42.92\\n'\n",
      "                       'RetNet 32.50 49.73 28.24 62.61 34.15 50.91 44.27 23.62 '\n",
      "                       '36.79 59.72 42.54\\n'\n",
      "                       'GLA 28.51 43.02 28.73 64.05 35.96 50.00 54.19 24.29 '\n",
      "                       '37.13 58.39 44.09\\n'\n",
      "                       'Mamba 30.83 40.21 29.94 63.79 35.88 49.82 49.24 24.56 '\n",
      "                       '35.41 60.07 43.59\\n'\n",
      "                       'DeltaNet 28.65 47.30 28.43 63.52 35.95 49.63 52.68 '\n",
      "                       '25.37 37.96 58.79 44.04\\n'\n",
      "                       'TTT 27.44 34.19 30.06 63.97 35.71 50.08 53.01 26.11 '\n",
      "                       '37.32 59.83 44.51\\n'\n",
      "                       'Gated DeltaNet 27.01 30.94 34.11 63.08 38.12 51.60 '\n",
      "                       '55.28 26.77 34.89 59.54 45.42\\n'\n",
      "                       'Titans (LMM) 26.18 29.97 34.98 64.73 39.61 51.85 55.60 '\n",
      "                       '28.14 34.52 59.99 46.17\\n'\n",
      "                       'Titans (MAC)∗ 25.43 28.13 36.00 65.32 40.35 51.21 '\n",
      "                       '58.17 29.00 38.63 60.18 47.36\\n'\n",
      "                       'Titans (MAG)∗ 25.07 28.72 36.71 64.88 40.56 52.49 '\n",
      "                       '57.72 28.16 39.75 60.01 47.54\\n'\n",
      "                       'Titans (MAL)∗ 24.69 28.80 35.74 64.97 39.44 51.97 '\n",
      "                       '56.58 28.21 38.14 57.32 46.55\\n'\n",
      "                       '400M params / 15B tokens\\n'\n",
      "                       'Transformer++ 30.63 37.37 29.64 64.27 37.72 51.53 '\n",
      "                       '54.95 27.36 38.07 61.59 45.64\\n'\n",
      "                       'RetNet 29.92 46.83 29.16 65.23 36.97 51.85 56.01 27.55 '\n",
      "                       '37.30 59.66 45.47\\n'\n",
      "                       'HGRN2 32.33 47.14 26.12 64.52 35.45 52.24 55.97 25.51 '\n",
      "                       '37.35 59.02 44.52\\n'\n",
      "                       'GLA 27.96 36.66 27.86 65.94 37.41 49.56 56.01 26.36 '\n",
      "                       '38.94 59.84 45.24\\n'\n",
      "                       'Mamba 29.22 39.88 29.82 65.72 37.93 50.11 58.37 26.70 '\n",
      "                       '37.76 61.13 45.94\\n'\n",
      "                       'Mamba2 26.34 33.19 32.03 65.77 39.73 52.48 59.00 27.64 '\n",
      "                       '37.92 60.72 46.91\\n'\n",
      "                       'DeltaNet 27.69 44.04 29.96 64.52 37.03 50.82 56.77 '\n",
      "                       '27.13 38.22 60.09 45.57\\n'\n",
      "                       'TTT 26.11 31.52 33.25 65.70 39.11 51.68 58.04 28.99 '\n",
      "                       '38.26 59.87 46.86\\n'\n",
      "                       'Gated DeltaNet 25.47 29.24 34.40 65.94 40.46 51.46 '\n",
      "                       '59.80 28.58 37.43 60.03 47.26\\n'\n",
      "                       'Samba∗ 25.32 29.47 36.86 66.09 39.24 51.45 60.12 27.20 '\n",
      "                       '38.68 58.22 47.23\\n'\n",
      "                       'Gated DeltaNet-H2∗ 24.19 28.09 36.77 66.43 40.79 52.17 '\n",
      "                       '59.55 29.09 39.04 58.56 47.69\\n'\n",
      "                       'Titans (LMM) 25.03 28.99 35.21 65.85 40.91 52.19 59.97 '\n",
      "                       '29.20 38.74 60.85 47.83\\n'\n",
      "                       'Titans (MAC)∗ 25.61 27.73 36.92 66.39 41.18 52.80 '\n",
      "                       '60.24 29.69 40.07 61.93 48.65\\n'\n",
      "                       'Titans (MAG)∗ 23.59 27.81 37.24 66.80 40.92 53.21 '\n",
      "                       '60.01 29.45 39.91 61.28 48.60\\n'\n",
      "                       'Titans (MAL)∗ 23.93 27.89 36.84 66.29 40.74 52.26 '\n",
      "                       '59.85 29.71 38.92 58.40 47.87\\n'\n",
      "                       '760M params / 30B tokens\\n'\n",
      "                       'Transformer++ 25.21 27.64 35.78 66.92 42.19 51.95 '\n",
      "                       '60.38 32.46 39.51 60.37 48.69\\n'\n",
      "                       'RetNet 26.08 24.45 34.51 67.19 41.63 52.09 63.17 32.78 '\n",
      "                       '38.36 57.92 48.46\\n'\n",
      "                       'Mamba 28.12 23.96 32.80 66.04 39.15 52.38 61.49 30.34 '\n",
      "                       '37.96 57.62 47.22\\n'\n",
      "                       'Mamba2 22.94 28.37 33.54 67.90 42.71 49.77 63.48 31.09 '\n",
      "                       '40.06 58.15 48.34\\n'\n",
      "                       'DeltaNet 24.37 24.60 37.06 66.93 41.98 50.65 64.87 '\n",
      "                       '31.39 39.88 59.02 48.97\\n'\n",
      "                       'TTT 24.17 23.51 34.74 67.25 43.92 50.99 64.53 33.81 '\n",
      "                       '40.16 59.58 47.32\\n'\n",
      "                       'Gated DeltaNet 21.18 22.09 35.54 68.01 44.95 50.73 '\n",
      "                       '66.87 33.09 39.21 59.14 49.69\\n'\n",
      "                       'Samba∗ 20.63 22.71 39.72 69.19 47.35 52.01 66.92 33.20 '\n",
      "                       '38.98 61.24 51.08\\n'\n",
      "                       'Gated DeltaNet-H2∗ 19.88 20.83 39.18 68.95 48.22 52.57 '\n",
      "                       '67.01 35.49 39.39 61.11 51.49\\n'\n",
      "                       'Titans (LMM) 20.04 21.96 37.40 69.28 48.46 52.27 66.31 '\n",
      "                       '35.84 40.13 62.76 51.56\\n'\n",
      "                       'Titans (MAC) 19.93 20.12 39.62 70.46 49.01 53.18 67.86 '\n",
      "                       '36.01 41.87 62.05 52.51\\n'\n",
      "                       'Titans (MAG) 18.61 19.86 40.98 70.25 48.94 52.89 68.23 '\n",
      "                       '36.19 40.38 62.11 52.50\\n'\n",
      "                       'Titans (MAL) 19.07 20.33 40.05 69.99 48.82 53.02 67.54 '\n",
      "                       '35.65 30.98 61.72 50.97\\n'\n",
      "                       'the “haystack”). In this part, we use Single NIAH '\n",
      "                       '(S-NIAH) task from RULER benchmark (Hsieh et al. 2024) '\n",
      "                       'and evaluate\\n'\n",
      "                       'Titans and baselines on sequences with length 2K, 4K, '\n",
      "                       '8K, and 16K. The results are reported in Table 2. '\n",
      "                       'Neural Memory\\n'\n",
      "                       'module achieves the best results compare to baselines '\n",
      "                       'in all three tasks. We attribute this superior '\n",
      "                       'performance to three\\n'\n",
      "                       'key differences of Titans with existing sequence '\n",
      "                       'models: (1) Compared to TTT, our Neural Memory can '\n",
      "                       'better handle the\\n'\n",
      "                       'memory capacity by using momentum and also the '\n",
      "                       'forgetting mechanism (i.e., weight decay). Therefore, '\n",
      "                       'with increasing\\n'\n",
      "                       'the sequence length, the performance of Neural Memory '\n",
      "                       'does not drop and show a consistent trend; (2) '\n",
      "                       'Compared to\\n'\n",
      "                       'Mamba2, which has the gating (forgetting) mechanism, '\n",
      "                       'Titans have deep non-linear memory, resulting in '\n",
      "                       'better memory\\n'\n",
      "                       'management. Also, contrary to our neural memory and '\n",
      "                       'DeltaNet, Mamba2 is not capable of removing a memory '\n",
      "                       'and so\\n'\n",
      "                       '13\\n'\n",
      "                       'Table 2: Performance of Titans and baselines on S-NIAH '\n",
      "                       'task from RULER benchmark. The best results among '\n",
      "                       'simple\\n'\n",
      "                       'and hybrid models are highlighted.\\n'\n",
      "                       'Model S-NIAH-PK S-NIAH-N S-NIAH-W\\n'\n",
      "                       '2K 4K 8K 16K 2K 4K 8K 16K 2K 4K 8K 16K\\n'\n",
      "                       'TTT 98.4 98.8 98.0 88.4 60.2 36.6 10.2 4.4 78.8 28.0 '\n",
      "                       '4.4 0.0\\n'\n",
      "                       'Mamba2 98.6 61.4 31.0 5.4 98.4 55.8 14.2 0.0 42.2 4.2 '\n",
      "                       '0.0 0.0\\n'\n",
      "                       'DeltaNet 96.8 98.8 98.6 71.4 47.2 15.4 12.8 5.4 46.2 '\n",
      "                       '20.0 1.6 0.0\\n'\n",
      "                       'Titans (LMM)99.8 98.4 98.2 96.2 100.0 99.8 93.4 80.2 '\n",
      "                       '90.4 89.4 85.8 80.6\\n'\n",
      "                       'Titans (MAC) 99.2 98.8 99.0 98.4 99.6 98.2 97.6 97.4 '\n",
      "                       '98.2 98.2 95.6 95.2\\n'\n",
      "                       'Titans (MAG)99.4 98.0 97.4 97.4 99.2 98.8 97.2 98.6 '\n",
      "                       '98.0 98.0 90.2 88.2\\n'\n",
      "                       'Titans (MAL) 98.8 98.6 98.8 97.8 99.8 98.1 96.8 96.4 '\n",
      "                       '98.0 97.4 92.0 90.4\\n'\n",
      "                       '(a) Few-shot Setup\\n'\n",
      "                       ' (b) Fine-Tuning Setup\\n'\n",
      "                       'Figure 6: Performance of Titans and baselines on '\n",
      "                       'BABILong benchmark. Titans (MAC) outperforms all '\n",
      "                       'baselines, including\\n'\n",
      "                       'extremely large models, e.g., GPT4.\\n'\n",
      "                       'we can see a significant drop in performance when '\n",
      "                       'increasing the sequence length; (3) Compared to '\n",
      "                       'DeltaNet, although it\\n'\n",
      "                       'is capable of removing memory using delta rule, it '\n",
      "                       'cannot erase the memory, lacking forgetting mechanism. '\n",
      "                       'Finally, As\\n'\n",
      "                       'expected we can see on par or better results when '\n",
      "                       'using Titans variants, where the best results '\n",
      "                       'correspond to MAC.\\n'\n",
      "                       '5.4 BABILong Benchmark\\n'\n",
      "                       'In the previous section we discussed the results on a '\n",
      "                       'simple NIAH tasks where a single needle needs to be '\n",
      "                       'retrieved.\\n'\n",
      "                       'Although Titans showed better performance compared to '\n",
      "                       'baselines, their true advantage over very long '\n",
      "                       'sequences is still\\n'\n",
      "                       'hidden. To this end, in this section, we use a harder '\n",
      "                       'task from BABILong benchmark (Yuri Kuratov et al. '\n",
      "                       '2024), in which\\n'\n",
      "                       'the model needs to reason across facts distributed in '\n",
      "                       'extremely long documents. We follow the original '\n",
      "                       'experimental setup\\n'\n",
      "                       'and training process in the benchmark. There are two '\n",
      "                       'settings: (1) Few-shot setting, in which we use large '\n",
      "                       'pre-trained\\n'\n",
      "                       'models, and (2) fine-tuning setting, where we '\n",
      "                       'fine-tune the MAC variant of Titans to compare it with '\n",
      "                       'other fine-tuned\\n'\n",
      "                       'baselines. The results for few-shot setting are '\n",
      "                       'reported in Figure 6a. In this setup, we can see '\n",
      "                       'Titans outperform all\\n'\n",
      "                       'baselines–i.e., Mamba2.8B (Gu and Dao 2024), RWKV-6-7B '\n",
      "                       '(Peng, Goldstein, et al. 2024), RecurrentGemma-9B '\n",
      "                       '(Botev et al.\\n'\n",
      "                       '2024), Gemma-9B (Team et al. 2024), Llama3.1-8B '\n",
      "                       '(Touvron et al. 2023), GPT-4, and GPT4o-mini (Achiam '\n",
      "                       'et al. 2023). These\\n'\n",
      "                       'results are achieved while Titans (MAC) is having much '\n",
      "                       'less number of parameters than baselines.\\n'\n",
      "                       'In the fine-tuning setup, we compare the small '\n",
      "                       'fine-tuned version of Titans (MAC) with: (i) the '\n",
      "                       'fine-tuned version of small\\n'\n",
      "                       'models (almost the same number of parameters as '\n",
      "                       'Titans) such as Mamba (Gu and Dao 2024), RMT (Bulatov, '\n",
      "                       'Yury Kuratov,\\n'\n",
      "                       'and Burtsev 2022), (ii) large models with '\n",
      "                       'Retrieval-Augmented Generation (RAG) (P. Lewis et al. '\n",
      "                       '2020) such as Llama3.1-\\n'\n",
      "                       '8B (Touvron et al. 2023), and (iii) extremely large '\n",
      "                       'models such as GPT-4 (Achiam et al. 2023), GPT4o-mini, '\n",
      "                       'Qwen2.5-72B (A.\\n'\n",
      "                       'Yang et al. 2024), and Llama3.1-70B (Touvron et al. '\n",
      "                       '2023). Baseline results are reported by (Yuri Kuratov '\n",
      "                       'et al. 2024). The\\n'\n",
      "                       'results of Titans and baselines are reported in Figure '\n",
      "                       '6b. Titans outperform all models even extremely large '\n",
      "                       'models like\\n'\n",
      "                       'GPT4. Also, compared to Transformer-based with memory '\n",
      "                       'models like RMT, Titans show better performance mainly '\n",
      "                       'due\\n'\n",
      "                       'to their powerful memory. That is, RMT compress the '\n",
      "                       'historical data into 16 size vector-valued memory, '\n",
      "                       'while Titans with\\n'\n",
      "                       'in-context online memory learner are capable of '\n",
      "                       'encoding the past into the parameters of the model. '\n",
      "                       'Interestingly, even\\n'\n",
      "                       '14\\n'\n",
      "                       '(a) 170M Parameters\\n'\n",
      "                       ' (b) 360M Parameters\\n'\n",
      "                       ' (c) 760M Parameters\\n'\n",
      "                       'Figure 7: The effect of memory depth on the '\n",
      "                       'perplexity. Deeper long-term memory results in better '\n",
      "                       'scaling in longer\\n'\n",
      "                       'sequences.\\n'\n",
      "                       'Table 3: Performance on long-term forecasting. The '\n",
      "                       'best results are highlighted .\\n'\n",
      "                       'Neural MemorySimba iTransformerRLinear '\n",
      "                       'PatchTSTCrossformerTiDE TimesNet DLinear\\n'\n",
      "                       'MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE '\n",
      "                       'MAE MSE MAE MSE MAE\\n'\n",
      "                       'ETTm1 0.358 0.387 0.3830.3960.407 0.410 '\n",
      "                       '0.4140.4070.3870.4000.5130.496 '\n",
      "                       '0.4190.4190.4000.4060.4030.407\\n'\n",
      "                       'ETTm2 0.261 0.309 0.2710.3270.288 0.332 '\n",
      "                       '0.2860.3270.2810.3260.7570.610 '\n",
      "                       '0.3580.4040.2910.3330.3500.401\\n'\n",
      "                       'ETTh1 0.420 0.421 0.4410.4320.454 0.447 '\n",
      "                       '0.4460.4340.4690.4540.5290.522 '\n",
      "                       '0.5410.5070.4580.4500.4560.452\\n'\n",
      "                       'ETTh2 0.336 0.382 0.3610.3910.383 0.407 '\n",
      "                       '0.3740.3980.3870.4070.9420.684 '\n",
      "                       '0.6110.5500.4140.4270.5590.515\\n'\n",
      "                       'ECL 0.162 0.261 0.1690.2740.178 0.270 '\n",
      "                       '0.2190.2980.2050.2900.2440.334 '\n",
      "                       '0.2510.3440.1920.2950.2120.300\\n'\n",
      "                       'Traffic 0.415 0.289 0.4930.2910.428 0.282 '\n",
      "                       '0.6260.3780.4810.3040.5500.304 '\n",
      "                       '0.7600.4730.6200.3360.6250.383\\n'\n",
      "                       'Weather0.231 0.265 0.2550.2800.258 0.278 '\n",
      "                       '0.2720.2910.2590.2810.2590.315 '\n",
      "                       '0.2710.3200.2590.2870.2650.317\\n'\n",
      "                       'augmenting Llama3.1-8B model with RAG performs worse '\n",
      "                       'than Titans with about ×70 less parameters.\\n'\n",
      "                       '5.5 The Effect of Deep Memory\\n'\n",
      "                       'In this section, we evaluate the effect of deep memory '\n",
      "                       'in both wall-clock training time and model '\n",
      "                       'performance2. To this\\n'\n",
      "                       'end, we focus on different variants of our neural '\n",
      "                       'memory module, where 𝐿M= 1,2,3,4. We also use Mamba as '\n",
      "                       'a baseline\\n'\n",
      "                       'for the model performance. For a fair comparison, we '\n",
      "                       'use the same training process for all models and train '\n",
      "                       'them on a\\n'\n",
      "                       'subset of the Pile dataset (L. Gao et al. 2020).\\n'\n",
      "                       'We report the perplexity of our models and baselines '\n",
      "                       'as the function of the sequence length in Figure 7. '\n",
      "                       'Interestingly, with\\n'\n",
      "                       'the increase of memory depth, 𝐿M, the model can '\n",
      "                       'achieve better perplexity over all sequence length. '\n",
      "                       'Also, deeper memory\\n'\n",
      "                       'modules are more robust to the sequence length when '\n",
      "                       'the model has less number of parameters. With the '\n",
      "                       'increase of the\\n'\n",
      "                       'number of parameters, all models show better '\n",
      "                       'performance on longer sequences.\\n'\n",
      "                       'Figure 8: The effect of memory depth on\\n'\n",
      "                       'training throughput\\n'\n",
      "                       'We also evaluate the effect of memory depth ( 𝐿M = '\n",
      "                       '1,2,3,4) on the training\\n'\n",
      "                       'throughput. We report the training throughput (the '\n",
      "                       'number of tokens per\\n'\n",
      "                       'second) as the function of sequence length in Figure '\n",
      "                       '8. All models scale linearly\\n'\n",
      "                       'with respect to the context length (i.e., constant '\n",
      "                       'trend in the number of tokens\\n'\n",
      "                       'per second with respect to sequence length). Also, by '\n",
      "                       'increasing the memory\\n'\n",
      "                       'depth, as expected, we can see a linear trend that a '\n",
      "                       'deeper memory results in\\n'\n",
      "                       'a slower training. Therefore, it is not always '\n",
      "                       'efficient to use deeper memory\\n'\n",
      "                       'modules, showing a trade-off between effectiveness and '\n",
      "                       'efficiency.\\n'\n",
      "                       '5.6 Time Series Forecasting\\n'\n",
      "                       'To show the effectiveness of our memory module in a '\n",
      "                       'broader tasks, we also evaluate its performance in '\n",
      "                       'time series\\n'\n",
      "                       'forecasting tasks. To this end, we use Simba framework '\n",
      "                       '(Patro and Agneeswaran 2024) for time series '\n",
      "                       'forecasting, and\\n'\n",
      "                       '2Note that, in this experiment, we only focus on the '\n",
      "                       'neural memory module to evaluate the effect of memory '\n",
      "                       'depth in the memorization process.\\n'\n",
      "                       'Combining neural memory with attention as we do in '\n",
      "                       'Titans variants, can additionally enhance the '\n",
      "                       'performance of the model over long sequences.\\n'\n",
      "                       '15\\n'\n",
      "                       'Table 4: Downstream evaluation of pre-trained DNA '\n",
      "                       'models on GenomicsBenchmarks (Grešová et al. 2023). We '\n",
      "                       'report\\n'\n",
      "                       'top-1 classification accuracy (%).\\n'\n",
      "                       'Model Enhancer Cohn Enhancer Ens Human Reg. Non-TATA '\n",
      "                       'Promoters Human OCR Ens.\\n'\n",
      "                       'CNN 69.5 68.9 93.3 84.6 68.0\\n'\n",
      "                       'DNABERT 74.0 85.7 88.1 85.6 75.1\\n'\n",
      "                       'GPT 70.5 83.5 91.5 87.7 73.0\\n'\n",
      "                       'HyenaDNA 74.2 89.2 93.8 96.6 80.9\\n'\n",
      "                       'Transformer++ 73.4 89.5 89.9 94.4 79.5\\n'\n",
      "                       'Mamba 73.0 - - 96.6 -\\n'\n",
      "                       'Based 74.6 89.5 89.5 96.8 79.0\\n'\n",
      "                       'Neural Memory Module 75.2 89.6 89.3 96.6 79.9\\n'\n",
      "                       'replace its Mamba module with our neural memory. We '\n",
      "                       'report the results on common time series forecasting '\n",
      "                       'benchmark\\n'\n",
      "                       'datasets–ETT, ECL, Traffic, and Weather (H. Zhou et '\n",
      "                       'al. 2021). The results are reported in Table 3. Our '\n",
      "                       'neural memory\\n'\n",
      "                       'module is outperforming all baselines, including '\n",
      "                       'Mamba-based, linear-based, and Transformer-based '\n",
      "                       'architectures.\\n'\n",
      "                       '5.7 DNA Modeling\\n'\n",
      "                       'In order to understand the capability of Titans beyond '\n",
      "                       'natural language, we further evaluate the performance '\n",
      "                       'of our\\n'\n",
      "                       'neural memory module on DNA modeling tasks. To this '\n",
      "                       'end, we evaluate pre-trained models on the downstream '\n",
      "                       'tasks\\n'\n",
      "                       'in GenomicsBenchmarks (Grešová et al. 2023). We follow '\n",
      "                       'the same experimental setups from Nguyen et al. '\n",
      "                       '(2024), and\\n'\n",
      "                       're-use the reported results of baselines by Arora et '\n",
      "                       'al. (2024). The performance of Titans (LMM) and '\n",
      "                       'baselines are reported\\n'\n",
      "                       'in Table 4. We find that LMM is competitive with '\n",
      "                       'state-of-the-art architectures across different '\n",
      "                       'downstream genomics\\n'\n",
      "                       'tasks.\\n'\n",
      "                       '5.8 Efficiency\\n'\n",
      "                       'Figure 9: Training throughput compari-\\n'\n",
      "                       'son of Titans and baselines.\\n'\n",
      "                       'In this part, we compare the efficiency of our neural '\n",
      "                       'memory as well as Titans\\n'\n",
      "                       'with state-of-the-art sequence models. The training '\n",
      "                       'throughput of models for\\n'\n",
      "                       'different sequence length ×batch size are reported in '\n",
      "                       'Figure 9. Comparing\\n'\n",
      "                       'recurrent models, including our neural memory module, '\n",
      "                       'we can see our memory\\n'\n",
      "                       'module is slightly slower than Mamba2 and Gated '\n",
      "                       'DeltaNet, mainly due to: (1)\\n'\n",
      "                       'having deep memory and more expressive transition '\n",
      "                       'process (memory update),\\n'\n",
      "                       'and (2) highly optimized kernel in the implementation '\n",
      "                       'of Mamba2. Interestingly,\\n'\n",
      "                       'Titans (MAL) are faster than baselines as well as the '\n",
      "                       'memory module. The\\n'\n",
      "                       'main reason for this better throughput is the highly '\n",
      "                       'optimized kernel of Flash-\\n'\n",
      "                       'Attention (Dao 2024), which is used for implementing '\n",
      "                       'SWA and full attention\\n'\n",
      "                       'module in Titans.\\n'\n",
      "                       '5.9 Ablation Study\\n'\n",
      "                       'Finally, we perform ablation studies on the different '\n",
      "                       'architectural choices in Titans. We consider our '\n",
      "                       'neural memory\\n'\n",
      "                       'module as a base model and then changing one component '\n",
      "                       'at a time: (1) replacing deep memory with linear '\n",
      "                       'memory,\\n'\n",
      "                       'removing (2) convolution, (3) momentum in the surprise '\n",
      "                       'measure, (4) weight decay (or forgot mechanism), and '\n",
      "                       '(5) persistent\\n'\n",
      "                       'memory. The results are reported in Table 5. All '\n",
      "                       'components of neural memory design are positively '\n",
      "                       'contributing to its\\n'\n",
      "                       'performance, where the greatest contribution comes '\n",
      "                       'from weight decay, momentum, convolution, and '\n",
      "                       'persistent memory,\\n'\n",
      "                       'respectively.\\n'\n",
      "                       'The Effect of Architectural Design. To evaluate the '\n",
      "                       'effect of architecture design, we compare the '\n",
      "                       'performance of three\\n'\n",
      "                       'represented variants of Titans in three aspects of (i) '\n",
      "                       'language modeling, (ii) commen-sense reasoning, and '\n",
      "                       '(iii) long context\\n'\n",
      "                       'NIAH (BABILong) tasks. The results are reported in '\n",
      "                       'Table 5. We find that MAC and MAG have close '\n",
      "                       'performance in\\n'\n",
      "                       'language modeling and common-sense reasoning tasks, '\n",
      "                       'while MAC achieve significantly better performance in '\n",
      "                       'long-context\\n'\n",
      "                       'NIAH. Both of these models achieve better performance '\n",
      "                       'than MAL. These results along with Figure 9, show a '\n",
      "                       'trade-off\\n'\n",
      "                       'between fast training and more expressive design.\\n'\n",
      "                       '16\\n'\n",
      "                       'Table 5: Ablation Study on Titans. All components of '\n",
      "                       'Titans are positively contributing to its '\n",
      "                       'performance.\\n'\n",
      "                       'Model Language Modeling Reasoning Long Context\\n'\n",
      "                       'ppl↓ acc↑ acc↑\\n'\n",
      "                       'LMM 27.01 47.83 92.68\\n'\n",
      "                       '+Attn(MAC) 26.67 48.65 97.95\\n'\n",
      "                       '+Attn(MAG) 25.70 48.60 96.70\\n'\n",
      "                       '+Attn(MAL) 25.91 47.87 96.91\\n'\n",
      "                       'Linear Memory 28.49 46.97 85.34\\n'\n",
      "                       'w/o Convolution 28.73 45.82 90.28\\n'\n",
      "                       'w/o Momentum 28.98 45.49 87.12\\n'\n",
      "                       'w/o Weight Decay 29.04 45.11 85.60\\n'\n",
      "                       'w/o Persistent Memory 27.63 46.35 92.49\\n'\n",
      "                       '6 Conclusion\\n'\n",
      "                       'In this paper, we present a neural long-term memory '\n",
      "                       'that, as a meta in-context learner, learns to memorize '\n",
      "                       'at test time.\\n'\n",
      "                       'The neural memory module is a recurrent model in '\n",
      "                       'nature, and is adaptively memorizing tokens that are '\n",
      "                       'more surprising\\n'\n",
      "                       'or are close to surprising tokens. Comparing to modern '\n",
      "                       'recurrent models, it has more expressive memory update '\n",
      "                       'and\\n'\n",
      "                       'storing mechanism. Using this memory, we present '\n",
      "                       'Titans architectures, and its three variants, in which '\n",
      "                       'we suggest to\\n'\n",
      "                       'incorporate the memory module as (1) a context, (2) '\n",
      "                       'gating, and (3) a layer. Our experimental evaluation '\n",
      "                       'on diverse tasks\\n'\n",
      "                       'tasks validate that Titans are more effective than '\n",
      "                       'Transformers and recent modern linear recurrent '\n",
      "                       'models, specifically for\\n'\n",
      "                       'long context. That is, Titans can scale to larger than '\n",
      "                       '2M context window size with better accuracy than '\n",
      "                       'baselines.\\n'\n",
      "                       'Titans are implemented in Pytorch and JAX and we '\n",
      "                       'intend to make the code we used to train and evaluate '\n",
      "                       'our models\\n'\n",
      "                       'available soon.\\n'\n",
      "                       '17\\n'\n",
      "                       'References\\n'\n",
      "                       '[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama '\n",
      "                       'Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo\\n'\n",
      "                       'Almeida, Janko Altenschmidt, Sam Altman, Shyamal '\n",
      "                       'Anadkat, et al. “Gpt-4 technical report”. In: arXiv '\n",
      "                       'preprint\\n'\n",
      "                       'arXiv:2303.08774 (2023).\\n'\n",
      "                       '[2] Yaroslav Aksenov, Nikita Balagansky, Sofia Maria '\n",
      "                       'Lo Cicero Vaina, Boris Shaposhnikov, Alexey '\n",
      "                       'Gorbatovski, and\\n'\n",
      "                       'Daniil Gavrilov. “Linear Transformers with Learnable '\n",
      "                       'Kernel Functions are Better In-Context Models”. In: '\n",
      "                       'arXiv\\n'\n",
      "                       'preprint arXiv:2402.10644 (2024).\\n'\n",
      "                       '[3] Marcin Andrychowicz, Misha Denil, Sergio Gomez, '\n",
      "                       'Matthew W Hoffman, David Pfau, Tom Schaul, Brendan\\n'\n",
      "                       'Shillingford, and Nando De Freitas. “Learning to learn '\n",
      "                       'by gradient descent by gradient descent”. In: Advances '\n",
      "                       'in\\n'\n",
      "                       'neural information processing systems 29 (2016).\\n'\n",
      "                       '[4] Cem Anil, Yuhuai Wu, Anders Andreassen, Aitor '\n",
      "                       'Lewkowycz, Vedant Misra, Vinay Ramasesh, Ambrose '\n",
      "                       'Slone,\\n'\n",
      "                       'Guy Gur-Ari, Ethan Dyer, and Behnam Neyshabur. '\n",
      "                       '“Exploring length generalization in large language '\n",
      "                       'models”. In:\\n'\n",
      "                       'Advances in Neural Information Processing Systems 35 '\n",
      "                       '(2022), pp. 38546–38556.\\n'\n",
      "                       '[5] Simran Arora, Sabri Eyuboglu, Michael Zhang, Aman '\n",
      "                       'Timalsina, Silas Alberti, James Zou, Atri Rudra, and '\n",
      "                       'Christo-\\n'\n",
      "                       'pher Re. “Simple linear attention language models '\n",
      "                       'balance the recall-throughput tradeoff”. '\n",
      "                       'In:Forty-first International\\n'\n",
      "                       'Conference on Machine Learning . 2024. url: '\n",
      "                       'https://openreview.net/forum?id=e93ffDcpH3.\\n'\n",
      "                       '[6] Dzmitry Bahdanau. “Neural machine translation by '\n",
      "                       'jointly learning to align and translate”. In: arXiv '\n",
      "                       'preprint\\n'\n",
      "                       'arXiv:1409.0473 (2014).\\n'\n",
      "                       '[7] Reza Bayat, Mohammad Pezeshki, Elvis Dohmatob, '\n",
      "                       'David Lopez-Paz, and Pascal Vincent. “The Pitfalls of '\n",
      "                       'Memo-\\n'\n",
      "                       'rization: When Memorization Hurts Generalization”. In: '\n",
      "                       'arXiv preprint arXiv:2412.07684 (2024).\\n'\n",
      "                       '[8] Maximilian Beck, Korbinian Pöppel, Markus '\n",
      "                       'Spanring, Andreas Auer, Oleksandra Prudnikova, Michael '\n",
      "                       'Kopp,\\n'\n",
      "                       'Günter Klambauer, Johannes Brandstetter, and Sepp '\n",
      "                       'Hochreiter. “xLSTM: Extended Long Short-Term Memory”. '\n",
      "                       'In:\\n'\n",
      "                       'arXiv preprint arXiv:2405.04517 (2024).\\n'\n",
      "                       '[9] Ali Behrouz, Michele Santacatterina, and Ramin '\n",
      "                       'Zabih. “Mambamixer: Efficient selective state space '\n",
      "                       'models with\\n'\n",
      "                       'dual token and channel selection”. In: arXiv preprint '\n",
      "                       'arXiv:2403.19888 (2024).\\n'\n",
      "                       '[10] Vincent-Pierre Berges, Barlas Oğuz, Daniel '\n",
      "                       'Haziza, Wen-tau Yih, Luke Zettlemoyer, and Gargi Gosh. '\n",
      "                       '“Memory\\n'\n",
      "                       'Layers at Scale”. In: arXiv preprint arXiv:2412.09764 '\n",
      "                       '(2024).\\n'\n",
      "                       '[11] Alberto Bietti, Vivien Cabannes, Diane '\n",
      "                       'Bouchacourt, Herve Jegou, and Leon Bottou. “Birth of a '\n",
      "                       'transformer: A\\n'\n",
      "                       'memory viewpoint”. In: Advances in Neural Information '\n",
      "                       'Processing Systems 36 (2024).\\n'\n",
      "                       '[12] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin '\n",
      "                       'Choi, et al. “Piqa: Reasoning about physical '\n",
      "                       'commonsense in\\n'\n",
      "                       'natural language”. In: Proceedings of the AAAI '\n",
      "                       'conference on artificial intelligence . Vol. 34. 05. '\n",
      "                       '2020, pp. 7432–7439.\\n'\n",
      "                       '[13] Aleksandar Botev, Soham De, Samuel L Smith, '\n",
      "                       'Anushan Fernando, George-Cristian Muraru, Ruba Haroun, '\n",
      "                       'Leonard\\n'\n",
      "                       'Berrada, Razvan Pascanu, Pier Giuseppe Sessa, Robert '\n",
      "                       'Dadashi, et al. “RecurrentGemma: Moving Past '\n",
      "                       'Transformers\\n'\n",
      "                       'for Efficient Open Language Models”. In: arXiv '\n",
      "                       'preprint arXiv:2404.07839 (2024).\\n'\n",
      "                       '[14] Léon Bottou and Vladimir Vapnik. “Local learning '\n",
      "                       'algorithms”. In: Neural computation 4.6 (1992), pp. '\n",
      "                       '888–900.\\n'\n",
      "                       '[15] Aydar Bulatov, Yuri Kuratov, Yermek Kapushev, and '\n",
      "                       'Mikhail S Burtsev. “Scaling transformer to 1m tokens '\n",
      "                       'and\\n'\n",
      "                       'beyond with rmt”. In: arXiv preprint arXiv:2304.11062 '\n",
      "                       '(2023).\\n'\n",
      "                       '[16] Aydar Bulatov, Yury Kuratov, and Mikhail Burtsev. '\n",
      "                       '“Recurrent memory transformer”. In: Advances in '\n",
      "                       'Neural\\n'\n",
      "                       'Information Processing Systems 35 (2022), pp. '\n",
      "                       '11079–11091.\\n'\n",
      "                       '[17] Edoardo Cetin, Qi Sun, Tianyu Zhao, and Yujin '\n",
      "                       'Tang. “An Evolved Universal Transformer Memory”. In: '\n",
      "                       'arXiv\\n'\n",
      "                       'preprint arXiv:2410.13166 (2024).\\n'\n",
      "                       '[18] Beidi Chen, Tri Dao, Eric Winsor, Zhao Song, Atri '\n",
      "                       'Rudra, and Christopher Ré. “Scatterbrain: Unifying '\n",
      "                       'sparse and\\n'\n",
      "                       'low-rank attention”. In: Advances in Neural '\n",
      "                       'Information Processing Systems 34 (2021), pp. '\n",
      "                       '17413–17426.\\n'\n",
      "                       '[19] Krzysztof Marcin Choromanski, Valerii '\n",
      "                       'Likhosherstov, David Dohan, Xingyou Song, Andreea '\n",
      "                       'Gane, Tamas Sarlos,\\n'\n",
      "                       'Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, '\n",
      "                       'Lukasz Kaiser, David Benjamin Belanger, Lucy J '\n",
      "                       'Colwell, and\\n'\n",
      "                       'Adrian Weller. “Rethinking Attention with Performers”. '\n",
      "                       'In: International Conference on Learning '\n",
      "                       'Representations .\\n'\n",
      "                       '2021. url: '\n",
      "                       'https://openreview.net/forum?id=Ua6zuk0WRH.\\n'\n",
      "                       '[20] Christopher Clark, Kenton Lee, Ming-Wei Chang, '\n",
      "                       'Tom Kwiatkowski, Michael Collins, and Kristina '\n",
      "                       'Toutanova.\\n'\n",
      "                       '“BoolQ: Exploring the Surprising Difficulty of Natural '\n",
      "                       'Yes/No Questions”. In: Proceedings of the 2019 '\n",
      "                       'Conference\\n'\n",
      "                       'of the North American Chapter of the Association for '\n",
      "                       'Computational Linguistics: Human Language '\n",
      "                       'Technologies,\\n'\n",
      "                       'Volume 1 (Long and Short Papers) . Ed. by Jill '\n",
      "                       'Burstein, Christy Doran, and Thamar Solorio. '\n",
      "                       'Minneapolis, Minnesota:\\n'\n",
      "                       'Association for Computational Linguistics, June 2019, '\n",
      "                       'pp. 2924–2936. doi: 10.18653/v1/N19-1300. url: https:\\n'\n",
      "                       '//aclanthology.org/N19-1300/.\\n'\n",
      "                       '18\\n'\n",
      "                       '[21] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar '\n",
      "                       'Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind '\n",
      "                       'Tafjord.\\n'\n",
      "                       '“Think you have solved question answering? try arc, '\n",
      "                       'the ai2 reasoning challenge”. In:arXiv preprint '\n",
      "                       'arXiv:1803.05457\\n'\n",
      "                       '(2018).\\n'\n",
      "                       '[22] Nelson Cowan. “What are the differences between '\n",
      "                       'long-term, short-term, and working memory?” In: '\n",
      "                       'Progress in\\n'\n",
      "                       'brain research 169 (2008), pp. 323–338.\\n'\n",
      "                       '[23] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G. '\n",
      "                       'Carbonell, Quoc Viet Le, and Ruslan Salakhutdinov. '\n",
      "                       '“Transformer-\\n'\n",
      "                       'XL: Attentive Language Models beyond a Fixed-Length '\n",
      "                       'Context”. In: ACL (1). Ed. by Anna Korhonen, David R.\\n'\n",
      "                       'Traum, and Lluís Màrquez. Association for '\n",
      "                       'Computational Linguistics, 2019, pp. 2978–2988.isbn: '\n",
      "                       '978-1-950737-48-2.\\n'\n",
      "                       '[24] Tri Dao. “FlashAttention-2: Faster Attention with '\n",
      "                       'Better Parallelism and Work Partitioning”. In: The '\n",
      "                       'Twelfth Inter-\\n'\n",
      "                       'national Conference on Learning Representations . '\n",
      "                       '2024. url: '\n",
      "                       'https://openreview.net/forum?id=mZn2Xyh9Ec.\\n'\n",
      "                       '[25] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and '\n",
      "                       'Christopher Ré. “FlashAttention: Fast and '\n",
      "                       'Memory-Efficient\\n'\n",
      "                       'Exact Attention with IO-Awareness”. In:Advances in '\n",
      "                       'Neural Information Processing Systems . Ed. by S. '\n",
      "                       'Koyejo, S.\\n'\n",
      "                       'Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh. '\n",
      "                       'Vol. 35. Curran Associates, Inc., 2022, pp. '\n",
      "                       '16344–16359. url:\\n'\n",
      "                       'https://proceedings.neurips.cc/paper_files/paper/2022/file/67d57c32e20fd0a7a302cb81d36e40d5-\\n'\n",
      "                       'Paper-Conference.pdf.\\n'\n",
      "                       '[26] Tri Dao and Albert Gu. “Transformers are SSMs: '\n",
      "                       'Generalized models and efficient algorithms through '\n",
      "                       'structured\\n'\n",
      "                       'state space duality”. In: arXiv preprint '\n",
      "                       'arXiv:2405.21060 (2024).\\n'\n",
      "                       '[27] Abhimanyu Das, Weihao Kong, Andrew Leach, Shaan K '\n",
      "                       'Mathur, Rajat Sen, and Rose Yu. “Long-term '\n",
      "                       'Forecasting\\n'\n",
      "                       'with TiDE: Time-series Dense Encoder”. In: '\n",
      "                       'Transactions on Machine Learning Research (2023). '\n",
      "                       'issn: 2835-8856. url:\\n'\n",
      "                       'https://openreview.net/forum?id=pCbC3aQB5W.\\n'\n",
      "                       '[28] Soham De, Samuel L Smith, Anushan Fernando, '\n",
      "                       'Aleksandar Botev, George Cristian-Muraru, Albert Gu, '\n",
      "                       'Ruba\\n'\n",
      "                       'Haroun, Leonard Berrada, Yutian Chen, Srivatsan '\n",
      "                       'Srinivasan, et al. “Griffin: Mixing gated linear '\n",
      "                       'recurrences with\\n'\n",
      "                       'local attention for efficient language models”. In: '\n",
      "                       'arXiv preprint arXiv:2402.19427 (2024).\\n'\n",
      "                       '[29] Juechu Dong, Boyuan Feng, Driss Guessous, Yanbo '\n",
      "                       'Liang, and Horace He. “Flex Attention: A Programming '\n",
      "                       'Model\\n'\n",
      "                       'for Generating Optimized Attention Kernels”. In: arXiv '\n",
      "                       'preprint arXiv:2412.05496 (2024).\\n'\n",
      "                       '[30] Xin Dong, Yonggan Fu, Shizhe Diao, Wonmin Byeon, '\n",
      "                       'Zijia Chen, Ameya Sunil Mahabaleshwarkar, Shih-Yang '\n",
      "                       'Liu,\\n'\n",
      "                       'Matthijs Van Keirsbilck, Min-Hung Chen, Yoshi Suhara, '\n",
      "                       'et al. “Hymba: A Hybrid-head Architecture for Small\\n'\n",
      "                       'Language Models”. In: arXiv preprint arXiv:2411.13676 '\n",
      "                       '(2024).\\n'\n",
      "                       '[31] Stefan Elfwing, Eiji Uchibe, and Kenji Doya. '\n",
      "                       '“Sigmoid-weighted linear units for neural network '\n",
      "                       'function approxi-\\n'\n",
      "                       'mation in reinforcement learning”. In: Neural networks '\n",
      "                       '107 (2018), pp. 3–11.\\n'\n",
      "                       '[32] Yukun Feng, Feng Li, Ziang Song, Boyuan Zheng, '\n",
      "                       'and Philipp Koehn. “Learn to remember: Transformer '\n",
      "                       'with\\n'\n",
      "                       'recurrent memory for document-level machine '\n",
      "                       'translation”. In: arXiv preprint arXiv:2205.01546 '\n",
      "                       '(2022).\\n'\n",
      "                       '[33] Daniel Y Fu, Tri Dao, Khaled Kamal Saab, Armin W '\n",
      "                       'Thomas, Atri Rudra, and Christopher Re. “Hungry '\n",
      "                       'Hungry\\n'\n",
      "                       'Hippos: Towards Language Modeling with State Space '\n",
      "                       'Models”. In:The Eleventh International Conference on '\n",
      "                       'Learning\\n'\n",
      "                       'Representations. 2023. url: '\n",
      "                       'https://openreview.net/forum?id=COZDy0WYGg.\\n'\n",
      "                       '[34] Yossi Gandelsman, Yu Sun, Xinlei Chen, and Alexei '\n",
      "                       'Efros. “Test-time training with masked autoencoders”. '\n",
      "                       'In:\\n'\n",
      "                       'Advances in Neural Information Processing Systems 35 '\n",
      "                       '(2022), pp. 29374–29385.\\n'\n",
      "                       '[35] Leo Gao, Stella Biderman, Sid Black, Laurence '\n",
      "                       'Golding, Travis Hoppe, Charles Foster, Jason Phang, '\n",
      "                       'Horace He,\\n'\n",
      "                       'Anish Thite, Noa Nabeshima, et al. “The pile: An 800gb '\n",
      "                       'dataset of diverse text for language modeling”. In: '\n",
      "                       'arXiv\\n'\n",
      "                       'preprint arXiv:2101.00027 (2020).\\n'\n",
      "                       '[36] Felix A Gers, Jürgen Schmidhuber, and Fred '\n",
      "                       'Cummins. “Learning to forget: Continual prediction '\n",
      "                       'with LSTM”. In:\\n'\n",
      "                       'Neural computation 12.10 (2000), pp. 2451–2471.\\n'\n",
      "                       '[37] Alex Graves, Greg Wayne, and Ivo Danihelka. '\n",
      "                       'Neural Turing Machines . 2014. arXiv: 1410.5401 '\n",
      "                       '[cs.NE]. url:\\n'\n",
      "                       'https://arxiv.org/abs/1410.5401.\\n'\n",
      "                       '[38] Klaus Greff, Rupesh K Srivastava, Jan Koutník, '\n",
      "                       'Bas R Steunebrink, and Jürgen Schmidhuber. “LSTM: A '\n",
      "                       'search space\\n'\n",
      "                       'odyssey”. In: IEEE transactions on neural networks and '\n",
      "                       'learning systems 28.10 (2016), pp. 2222–2232.\\n'\n",
      "                       '[39] Katarína Grešová, Vlastimil Martinek, David '\n",
      "                       'Čechák, Petr Šimeček, and Panagiotis Alexiou. “Genomic '\n",
      "                       'benchmarks:\\n'\n",
      "                       'a collection of datasets for genomic sequence '\n",
      "                       'classification”. In: BMC Genomic Data 24.1 (2023), p. '\n",
      "                       '25.\\n'\n",
      "                       '[40] Albert Gu and Tri Dao. “Mamba: Linear-Time '\n",
      "                       'Sequence Modeling with Selective State Spaces”. In: '\n",
      "                       'First Conference\\n'\n",
      "                       'on Language Modeling . 2024. url: '\n",
      "                       'https://openreview.net/forum?id=tEYskw1VY2.\\n'\n",
      "                       '[41] Albert Gu, Karan Goel, and Christopher Re. '\n",
      "                       '“Efficiently Modeling Long Sequences with Structured '\n",
      "                       'State Spaces”.\\n'\n",
      "                       'In: International Conference on Learning '\n",
      "                       'Representations . 2022. url: https : / / openreview . '\n",
      "                       'net / forum ? id =\\n'\n",
      "                       'uYLFoz1vlAC.\\n'\n",
      "                       '19\\n'\n",
      "                       '[42] Chi Han, Qifan Wang, Hao Peng, Wenhan Xiong, Yu '\n",
      "                       'Chen, Heng Ji, and Sinong Wang. “LM-Infinite: '\n",
      "                       'Zero-Shot\\n'\n",
      "                       'Extreme Length Generalization for Large Language '\n",
      "                       'Models”. In: Proceedings of the 2024 Conference of the '\n",
      "                       'North\\n'\n",
      "                       'American Chapter of the Association for Computational '\n",
      "                       'Linguistics: Human Language Technologies (Volume 1: '\n",
      "                       'Long\\n'\n",
      "                       'Papers). Ed. by Kevin Duh, Helena Gomez, and Steven '\n",
      "                       'Bethard. Mexico City, Mexico: Association for '\n",
      "                       'Computational\\n'\n",
      "                       'Linguistics, June 2024, pp. 3991–4008. doi: '\n",
      "                       '10.18653/v1/2024.naacl-long.222. url: '\n",
      "                       'https://aclanthology.\\n'\n",
      "                       'org/2024.naacl-long.222.\\n'\n",
      "                       '[43] Ramin Hasani, Mathias Lechner, Tsun-Hsuan Wang, '\n",
      "                       'Makram Chahine, Alexander Amini, and Daniela Rus. '\n",
      "                       '“Liquid\\n'\n",
      "                       'Structural State-Space Models”. In: The Eleventh '\n",
      "                       'International Conference on Learning Representations . '\n",
      "                       '2023. url:\\n'\n",
      "                       'https://openreview.net/forum?id=g4OTKRKfS7R.\\n'\n",
      "                       '[44] Zexue He, Leonid Karlinsky, Donghyun Kim, Julian '\n",
      "                       'McAuley, Dmitry Krotov, and Rogerio Feris. “CAMELoT:\\n'\n",
      "                       'Towards Large Language Models with Training-Free '\n",
      "                       'Consolidated Associative Memory”. In: arXiv preprint\\n'\n",
      "                       'arXiv:2402.13449 (2024).\\n'\n",
      "                       '[45] Donald Olding Hebb. The organization of behavior: '\n",
      "                       'A neuropsychological theory . Psychology press, 2005.\\n'\n",
      "                       '[46] John J Hopfield. “Neural networks and physical '\n",
      "                       'systems with emergent collective computational '\n",
      "                       'abilities.” In:\\n'\n",
      "                       'Proceedings of the national academy of sciences 79.8 '\n",
      "                       '(1982), pp. 2554–2558.\\n'\n",
      "                       '[47] Kurt Hornik, Maxwell Stinchcombe, and Halbert '\n",
      "                       'White. “Multilayer feedforward networks are universal '\n",
      "                       'approxi-\\n'\n",
      "                       'mators”. In: Neural networks 2.5 (1989), pp. 359–366.\\n'\n",
      "                       '[48] Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, '\n",
      "                       'Shantanu Acharya, Dima Rekesh, Fei Jia, and Boris '\n",
      "                       'Ginsburg.\\n'\n",
      "                       '“RULER: What’s the Real Context Size of Your '\n",
      "                       'Long-Context Language Models?” In: First Conference on '\n",
      "                       'Language\\n'\n",
      "                       'Modeling. 2024. url: '\n",
      "                       'https://openreview.net/forum?id=kIoBbc76Sy.\\n'\n",
      "                       '[49] DeLesley Hutchins, Imanol Schlag, Yuhuai Wu, '\n",
      "                       'Ethan Dyer, and Behnam Neyshabur. “Block-recurrent '\n",
      "                       'transformers”.\\n'\n",
      "                       'In: Advances in neural information processing systems '\n",
      "                       '35 (2022), pp. 33248–33261.\\n'\n",
      "                       '[50] Kazuki Irie, Róbert Csordás, and Jürgen '\n",
      "                       'Schmidhuber. “The dual form of neural networks '\n",
      "                       'revisited: Connecting test\\n'\n",
      "                       'time predictions to training patterns via spotlights '\n",
      "                       'of attention”. In: International Conference on Machine '\n",
      "                       'Learning .\\n'\n",
      "                       'PMLR. 2022, pp. 9639–9659.\\n'\n",
      "                       '[51] Kazuki Irie, Imanol Schlag, Róbert Csordás, and '\n",
      "                       'Jürgen Schmidhuber. “Going beyond linear transformers '\n",
      "                       'with\\n'\n",
      "                       'recurrent fast weight programmers”. In: Advances in '\n",
      "                       'neural information processing systems 34 (2021), pp. '\n",
      "                       '7703–7717.\\n'\n",
      "                       '[52] Vidit Jain and Erik Learned-Miller. “Online '\n",
      "                       'domain adaptation of a pre-trained cascade of '\n",
      "                       'classifiers”. In: CVPR\\n'\n",
      "                       '2011. IEEE. 2011, pp. 577–584.\\n'\n",
      "                       '[53] Albert Q Jiang, Alexandre Sablayrolles, Arthur '\n",
      "                       'Mensch, Chris Bamford, Devendra Singh Chaplot, Diego '\n",
      "                       'de las\\n'\n",
      "                       'Casas, Florian Bressand, Gianna Lengyel, Guillaume '\n",
      "                       'Lample, Lucile Saulnier, et al. “Mistral 7B”. In: '\n",
      "                       'arXiv preprint\\n'\n",
      "                       'arXiv:2310.06825 (2023).\\n'\n",
      "                       '[54] Praneeth Kacham, Vahab Mirrokni, and Peilin '\n",
      "                       'Zhong. “PolySketchFormer: Fast Transformers via '\n",
      "                       'Sketching Polyno-\\n'\n",
      "                       'mial Kernels”. In: Forty-first International '\n",
      "                       'Conference on Machine Learning . 2024. url: '\n",
      "                       'https://openreview.net/\\n'\n",
      "                       'forum?id=ghYrfdJfjK.\\n'\n",
      "                       '[55] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B '\n",
      "                       'Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec\\n'\n",
      "                       'Radford, Jeffrey Wu, and Dario Amodei. “Scaling laws '\n",
      "                       'for neural language models”. In:arXiv preprint '\n",
      "                       'arXiv:2001.08361\\n'\n",
      "                       '(2020).\\n'\n",
      "                       '[56] Angelos Katharopoulos, Apoorv Vyas, Nikolaos '\n",
      "                       'Pappas, and François Fleuret. “Transformers are rnns: '\n",
      "                       'Fast au-\\n'\n",
      "                       'toregressive transformers with linear attention”. In: '\n",
      "                       'International conference on machine learning . PMLR. '\n",
      "                       '2020,\\n'\n",
      "                       'pp. 5156–5165.\\n'\n",
      "                       '[57] Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke '\n",
      "                       'Zettlemoyer, and Mike Lewis. “Generalization through\\n'\n",
      "                       'Memorization: Nearest Neighbor Language Models”. In: '\n",
      "                       'International Conference on Learning Representations . '\n",
      "                       '2020.\\n'\n",
      "                       'url: https://openreview.net/forum?id=HklBjCEKvH.\\n'\n",
      "                       '[58] Yuri Kuratov, Aydar Bulatov, Petr Anokhin, Ivan '\n",
      "                       'Rodkin, Dmitry Igorevich Sorokin, Artyom Sorokin, and '\n",
      "                       'Mikhail\\n'\n",
      "                       'Burtsev. “BABILong: Testing the Limits of LLMs with '\n",
      "                       'Long Context Reasoning-in-a-Haystack”. In: The '\n",
      "                       'Thirty-\\n'\n",
      "                       'eight Conference on Neural Information Processing '\n",
      "                       'Systems Datasets and Benchmarks Track . 2024. url: '\n",
      "                       'https:\\n'\n",
      "                       '//openreview.net/forum?id=u7m2CG84BQ.\\n'\n",
      "                       '[59] Hung Le, Truyen Tran, and Svetha Venkatesh. '\n",
      "                       '“Self-attentive associative memory”. In:International '\n",
      "                       'conference on\\n'\n",
      "                       'machine learning . PMLR. 2020, pp. 5682–5691.\\n'\n",
      "                       '[60] Patrick Lewis, Ethan Perez, Aleksandra Piktus, '\n",
      "                       'Fabio Petroni, Vladimir Karpukhin, Naman Goyal, '\n",
      "                       'Heinrich Küttler,\\n'\n",
      "                       'Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. '\n",
      "                       '“Retrieval-augmented generation for '\n",
      "                       'knowledge-intensive nlp\\n'\n",
      "                       'tasks”. In: Advances in Neural Information Processing '\n",
      "                       'Systems 33 (2020), pp. 9459–9474.\\n'\n",
      "                       '20\\n'\n",
      "                       '[61] Danny Leybzon and Corentin Kervadec. “Learning, '\n",
      "                       'Forgetting, Remembering: Insights From Tracking LLM '\n",
      "                       'Mem-\\n'\n",
      "                       'orization During Training”. In: Proceedings of the 7th '\n",
      "                       'BlackboxNLP Workshop: Analyzing and Interpreting '\n",
      "                       'Neural\\n'\n",
      "                       'Networks for NLP . 2024, pp. 43–57.\\n'\n",
      "                       '[62] Zhe Li, Shiyi Qi, Yiduo Li, and Zenglin Xu. '\n",
      "                       '“Revisiting long-term time series forecasting: An '\n",
      "                       'investigation on linear\\n'\n",
      "                       'mapping”. In: arXiv preprint arXiv:2305.10721 (2023).\\n'\n",
      "                       '[63] Bo Liu, Rui Wang, Lemeng Wu, Yihao Feng, Peter '\n",
      "                       'Stone, and Qiang Liu. “Longhorn: State space models '\n",
      "                       'are amortized\\n'\n",
      "                       'online learners”. In: arXiv preprint arXiv:2407.14207 '\n",
      "                       '(2024).\\n'\n",
      "                       '[64] Nelson F Liu, Kevin Lin, John Hewitt, Ashwin '\n",
      "                       'Paranjape, Michele Bevilacqua, Fabio Petroni, and '\n",
      "                       'Percy Liang.\\n'\n",
      "                       '“Lost in the middle: How language models use long '\n",
      "                       'contexts”. In: Transactions of the Association for '\n",
      "                       'Computational\\n'\n",
      "                       'Linguistics 12 (2024), pp. 157–173.\\n'\n",
      "                       '[65] Yong Liu, Tengge Hu, Haoran Zhang, Haixu Wu, '\n",
      "                       'Shiyu Wang, Lintao Ma, and Mingsheng Long. '\n",
      "                       '“itransformer:\\n'\n",
      "                       'Inverted transformers are effective for time series '\n",
      "                       'forecasting”. In: arXiv preprint arXiv:2310.06625 '\n",
      "                       '(2023).\\n'\n",
      "                       '[66] George Mandler. “The structure of value: '\n",
      "                       'Accounting for taste”. In: Affect and cognition . '\n",
      "                       'Psychology Press, 2014,\\n'\n",
      "                       'pp. 3–36.\\n'\n",
      "                       '[67] Harsh Mehta, Ankit Gupta, Ashok Cutkosky, and '\n",
      "                       'Behnam Neyshabur. “Long Range Language Modeling via\\n'\n",
      "                       'Gated State Spaces”. In: The Eleventh International '\n",
      "                       'Conference on Learning Representations . 2023. url: '\n",
      "                       'https:\\n'\n",
      "                       '//openreview.net/forum?id=5MkYIYCbva.\\n'\n",
      "                       '[68] Stephen Merity, Caiming Xiong, James Bradbury, '\n",
      "                       'and Richard Socher. “Pointer Sentinel Mixture Models”. '\n",
      "                       'In:\\n'\n",
      "                       'International Conference on Learning Representations . '\n",
      "                       '2017. url: https://openreview.net/forum?id=Byj72udxe.\\n'\n",
      "                       '[69] William Merrill, Jackson Petty, and Ashish '\n",
      "                       'Sabharwal. “The Illusion of State in State-Space '\n",
      "                       'Models”. In: Forty-first\\n'\n",
      "                       'International Conference on Machine Learning . 2024. '\n",
      "                       'url: https://openreview.net/forum?id=QZgo9JZpLq.\\n'\n",
      "                       '[70] Ravi Teja Mullapudi, Steven Chen, Keyi Zhang, '\n",
      "                       'Deva Ramanan, and Kayvon Fatahalian. “Online model '\n",
      "                       'distillation\\n'\n",
      "                       'for efficient video inference”. In: Proceedings of the '\n",
      "                       'IEEE/CVF International conference on computer vision . '\n",
      "                       '2019,\\n'\n",
      "                       'pp. 3573–3582.\\n'\n",
      "                       '[71] Tsendsuren Munkhdalai, Manaal Faruqui, and '\n",
      "                       'Siddharth Gopal. “Leave no context behind: Efficient '\n",
      "                       'infinite context\\n'\n",
      "                       'transformers with infini-attention”. In: arXiv '\n",
      "                       'preprint arXiv:2404.07143 (2024).\\n'\n",
      "                       '[72] Tsendsuren Munkhdalai, Alessandro Sordoni, Tong '\n",
      "                       'Wang, and Adam Trischler. “Metalearned neural memory”. '\n",
      "                       'In:\\n'\n",
      "                       'Advances in Neural Information Processing Systems 32 '\n",
      "                       '(2019).\\n'\n",
      "                       '[73] Tsendsuren Munkhdalai and Hong Yu. “Neural '\n",
      "                       'semantic encoders”. In: Proceedings of the conference. '\n",
      "                       'Association for\\n'\n",
      "                       'Computational Linguistics. Meeting . Vol. 1. NIH '\n",
      "                       'Public Access. 2017, p. 397.\\n'\n",
      "                       '[74] Eric Nguyen, Michael Poli, Marjan Faizi, Armin '\n",
      "                       'Thomas, Michael Wornow, Callum Birch-Sykes, Stefano '\n",
      "                       'Massaroli,\\n'\n",
      "                       'Aman Patel, Clayton Rabideau, Yoshua Bengio, et al. '\n",
      "                       '“Hyenadna: Long-range genomic sequence modeling at '\n",
      "                       'single\\n'\n",
      "                       'nucleotide resolution”. In: Advances in neural '\n",
      "                       'information processing systems 36 (2024).\\n'\n",
      "                       '[75] A Nichol. “On first-order meta-learning '\n",
      "                       'algorithms”. In: arXiv preprint arXiv:1803.02999 '\n",
      "                       '(2018).\\n'\n",
      "                       '[76] Yuqi Nie, Nam H Nguyen, Phanwadee Sinthong, and '\n",
      "                       'Jayant Kalagnanam. “A time series is worth 64 words:\\n'\n",
      "                       'Long-term forecasting with transformers”. In: arXiv '\n",
      "                       'preprint arXiv:2211.14730 (2022).\\n'\n",
      "                       '[77] Hideyuki Okano, Tomoo Hirano, and Evan Balaban. '\n",
      "                       '“Learning and memory”. In:Proceedings of the National '\n",
      "                       'Academy\\n'\n",
      "                       'of Sciences 97.23 (2000), pp. 12403–12404.\\n'\n",
      "                       '[78] Antonio Orvieto, Samuel L Smith, Albert Gu, '\n",
      "                       'Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and '\n",
      "                       'Soham De.\\n'\n",
      "                       '“Resurrecting recurrent neural networks for long '\n",
      "                       'sequences”. In: International Conference on Machine '\n",
      "                       'Learning .\\n'\n",
      "                       'PMLR. 2023, pp. 26670–26698.\\n'\n",
      "                       '[79] Denis Paperno, Germán Kruszewski, Angeliki '\n",
      "                       'Lazaridou, Ngoc Quan Pham, Raffaella Bernardi, Sandro '\n",
      "                       'Pezzelle,\\n'\n",
      "                       'Marco Baroni, Gemma Boleda, and Raquel Fernández. “The '\n",
      "                       'LAMBADA dataset: Word prediction requiring a broad\\n'\n",
      "                       'discourse context”. In:Proceedings of the 54th Annual '\n",
      "                       'Meeting of the Association for Computational '\n",
      "                       'Linguistics (Volume\\n'\n",
      "                       '1: Long Papers) . Ed. by Katrin Erk and Noah A. Smith. '\n",
      "                       'Berlin, Germany: Association for Computational '\n",
      "                       'Linguistics,\\n'\n",
      "                       'Aug. 2016, pp. 1525–1534. doi: 10.18653/v1/P16-1144. '\n",
      "                       'url: https://aclanthology.org/P16-1144/.\\n'\n",
      "                       '[80] Badri N. Patro and Vijay S. Agneeswaran. SiMBA: '\n",
      "                       'Simplified Mamba-Based Architecture for Vision and '\n",
      "                       'Multivariate\\n'\n",
      "                       'Time series . 2024. arXiv: 2403.15360 [cs.CV].\\n'\n",
      "                       '[81] Guilherme Penedo, Hynek Kydlíček, Loubna Ben '\n",
      "                       'allal, Anton Lozhkov, Margaret Mitchell, Colin Raffel, '\n",
      "                       'Leandro\\n'\n",
      "                       'Von Werra, and Thomas Wolf. “The FineWeb Datasets: '\n",
      "                       'Decanting the Web for the Finest Text Data at Scale”. '\n",
      "                       'In:\\n'\n",
      "                       'The Thirty-eight Conference on Neural Information '\n",
      "                       'Processing Systems Datasets and Benchmarks Track . '\n",
      "                       '2024. url:\\n'\n",
      "                       'https://openreview.net/forum?id=n6SCkn2QaG.\\n'\n",
      "                       '[82] Bo Peng. RWKV-LM. Version 1.0.0. Aug. 2021. doi: '\n",
      "                       '10.5281/zenodo.5196577 . url: https://github.com/\\n'\n",
      "                       'BlinkDL/RWKV-LM.\\n'\n",
      "                       '21\\n'\n",
      "                       '[83] Bo Peng, Eric Alcaide, Quentin Gregory Anthony, '\n",
      "                       'Alon Albalak, Samuel Arcadinho, Stella Biderman, '\n",
      "                       'Huanqi Cao,\\n'\n",
      "                       'Xin Cheng, Michael Nguyen Chung, Leon Derczynski, '\n",
      "                       'Xingjian Du, Matteo Grella, Kranthi Kiran GV, Xuzheng '\n",
      "                       'He,\\n'\n",
      "                       'Haowen Hou, Przemyslaw Kazienko, Jan Kocon, Jiaming '\n",
      "                       'Kong, Bartłomiej Koptyra, Hayden Lau, Jiaju Lin, '\n",
      "                       'Krishna\\n'\n",
      "                       'Sri Ipsit Mantri, Ferdinand Mom, Atsushi Saito, '\n",
      "                       'Guangyu Song, Xiangru Tang, Johan S. Wind, Stanisław '\n",
      "                       'Woźniak,\\n'\n",
      "                       'Zhenyuan Zhang, Qinghua Zhou, Jian Zhu, and Rui-Jie '\n",
      "                       'Zhu. “RWKV: Reinventing RNNs for the Transformer '\n",
      "                       'Era”.\\n'\n",
      "                       'In: The 2023 Conference on Empirical Methods in '\n",
      "                       'Natural Language Processing . 2023. url: '\n",
      "                       'https://openreview.\\n'\n",
      "                       'net/forum?id=7SaXczaBpG.\\n'\n",
      "                       '[84] Bo Peng, Daniel Goldstein, Quentin Anthony, Alon '\n",
      "                       'Albalak, Eric Alcaide, Stella Biderman, Eugene Cheah, '\n",
      "                       'Xingjian\\n'\n",
      "                       'Du, Teddy Ferdinan, Haowen Hou, et al. “Eagle and '\n",
      "                       'finch: Rwkv with matrix-valued states and dynamic '\n",
      "                       'recurrence”.\\n'\n",
      "                       'In: arXiv preprint arXiv:2404.05892 (2024).\\n'\n",
      "                       '[85] DL Prados and SC Kak. “Neural network capacity '\n",
      "                       'using delta rule”. In: Electronics Letters 25.3 '\n",
      "                       '(1989), pp. 197–199.\\n'\n",
      "                       '[86] Zhen Qin, Yiran Zhong, and Hui Deng. “Exploring '\n",
      "                       'Transformer Extrapolation”. In: Proceedings of the '\n",
      "                       'AAAI\\n'\n",
      "                       'Conference on Artificial Intelligence . Vol. 38. 17. '\n",
      "                       '2024, pp. 18897–18905.\\n'\n",
      "                       '[87] Liliang Ren, Yang Liu, Yadong Lu, Yelong Shen, '\n",
      "                       'Chen Liang, and Weizhu Chen. “Samba: Simple Hybrid '\n",
      "                       'State Space\\n'\n",
      "                       'Models for Efficient Unlimited Context Language '\n",
      "                       'Modeling”. In: arXiv preprint arXiv:2406.07522 '\n",
      "                       '(2024).\\n'\n",
      "                       '[88] Ivan Rodkin, Yuri Kuratov, Aydar Bulatov, and '\n",
      "                       'Mikhail Burtsev. “Associative recurrent memory '\n",
      "                       'transformer”. In:\\n'\n",
      "                       'arXiv preprint arXiv:2407.04841 (2024).\\n'\n",
      "                       '[89] Aurko Roy, Mohammad Saffar, Ashish Vaswani, and '\n",
      "                       'David Grangier. “Efficient content-based sparse '\n",
      "                       'attention with\\n'\n",
      "                       'routing transformers”. In: Transactions of the '\n",
      "                       'Association for Computational Linguistics 9 (2021), '\n",
      "                       'pp. 53–68.\\n'\n",
      "                       '[90] Keisuke Sakaguchi, Ronan Le Bras, Chandra '\n",
      "                       'Bhagavatula, and Yejin Choi. “Winogrande: An '\n",
      "                       'adversarial winograd\\n'\n",
      "                       'schema challenge at scale”. In: Communications of the '\n",
      "                       'ACM 64.9 (2021), pp. 99–106.\\n'\n",
      "                       '[91] Maarten Sap, Hannah Rashkin, Derek Chen, Ronan Le '\n",
      "                       'Bras, and Yejin Choi. “Social IQa: Commonsense '\n",
      "                       'Reasoning\\n'\n",
      "                       'about Social Interactions”. In:Proceedings of the 2019 '\n",
      "                       'Conference on Empirical Methods in Natural Language '\n",
      "                       'Processing\\n'\n",
      "                       'and the 9th International Joint Conference on Natural '\n",
      "                       'Language Processing (EMNLP-IJCNLP) . Ed. by Kentaro '\n",
      "                       'Inui,\\n'\n",
      "                       'Jing Jiang, Vincent Ng, and Xiaojun Wan. Hong Kong, '\n",
      "                       'China: Association for Computational Linguistics, Nov. '\n",
      "                       '2019,\\n'\n",
      "                       'pp. 4463–4473. doi: 10.18653/v1/D19-1454. url: '\n",
      "                       'https://aclanthology.org/D19-1454/.\\n'\n",
      "                       '[92] Imanol Schlag, Kazuki Irie, and Jürgen '\n",
      "                       'Schmidhuber. “Linear transformers are secretly fast '\n",
      "                       'weight programmers”.\\n'\n",
      "                       'In: International Conference on Machine Learning . '\n",
      "                       'PMLR. 2021, pp. 9355–9366.\\n'\n",
      "                       '[93] JH Schmidhuber. “Learning to control fast-weight '\n",
      "                       'memories: An alternative to recurrent nets. Accepted '\n",
      "                       'for\\n'\n",
      "                       'publication in”. In: Neural Computation (1992).\\n'\n",
      "                       '[94] Jürgen Schmidhuber. “Reducing the ratio between '\n",
      "                       'learning complexity and number of time varying '\n",
      "                       'variables\\n'\n",
      "                       'in fully recurrent nets”. In: ICANN’93: Proceedings of '\n",
      "                       'the International Conference on Artificial Neural '\n",
      "                       'Networks\\n'\n",
      "                       'Amsterdam, The Netherlands 13–16 September 1993 3 . '\n",
      "                       'Springer. 1993, pp. 460–463.\\n'\n",
      "                       '[95] Jürgen Schmidhuber and Sepp Hochreiter. “Long '\n",
      "                       'Short-term Memory”. In: Neural Computation MIT-Press '\n",
      "                       '(1997).\\n'\n",
      "                       '[96] Avi Schwarzschild, Zhili Feng, Pratyush Maini, '\n",
      "                       'Zachary C Lipton, and J Zico Kolter. “Rethinking llm '\n",
      "                       'memorization\\n'\n",
      "                       'through the lens of adversarial compression”. In: '\n",
      "                       'arXiv preprint arXiv:2404.15146 (2024).\\n'\n",
      "                       '[97] Jimmy T.H. Smith, Andrew Warrington, and Scott '\n",
      "                       'Linderman. “Simplified State Space Layers for Sequence '\n",
      "                       'Modeling”.\\n'\n",
      "                       'In: The Eleventh International Conference on Learning '\n",
      "                       'Representations . 2023. url: '\n",
      "                       'https://openreview.net/forum?\\n'\n",
      "                       'id=Ai8Hw3AXqks.\\n'\n",
      "                       '[98] Robin Staab, Mark Vero, Mislav Balunovic, and '\n",
      "                       'Martin Vechev. “Beyond Memorization: Violating Privacy '\n",
      "                       'via\\n'\n",
      "                       'Inference with Large Language Models”. In: The Twelfth '\n",
      "                       'International Conference on Learning Representations . '\n",
      "                       '2024.\\n'\n",
      "                       'url: https://openreview.net/forum?id=kmn0BhQk7p.\\n'\n",
      "                       '[99] Sainbayar Sukhbaatar, Edouard Grave, Guillaume '\n",
      "                       'Lample, Herve Jegou, and Armand Joulin. “Augmenting '\n",
      "                       'self-\\n'\n",
      "                       'attention with persistent memory”. In: arXiv preprint '\n",
      "                       'arXiv:1907.01470 (2019).\\n'\n",
      "                       '[100] Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, '\n",
      "                       'et al. “End-to-end memory networks”. In: Advances in '\n",
      "                       'neural\\n'\n",
      "                       'information processing systems 28 (2015).\\n'\n",
      "                       '[101] Yu Sun, Xinhao Li, Karan Dalal, Jiarui Xu, Arjun '\n",
      "                       'Vikram, Genghan Zhang, Yann Dubois, Xinlei Chen, '\n",
      "                       'Xiaolong\\n'\n",
      "                       'Wang, Sanmi Koyejo, et al. “Learning to (learn at test '\n",
      "                       'time): Rnns with expressive hidden states”. In: arXiv '\n",
      "                       'preprint\\n'\n",
      "                       'arXiv:2407.04620 (2024).\\n'\n",
      "                       '[102] Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, '\n",
      "                       'Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei. '\n",
      "                       '“Retentive\\n'\n",
      "                       'network: A successor to transformer for large language '\n",
      "                       'models”. In: arXiv preprint arXiv:2307.08621 (2023).\\n'\n",
      "                       '[103] Gemma Team, Thomas Mesnard, Cassidy Hardin, '\n",
      "                       'Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, '\n",
      "                       'Laurent Sifre,\\n'\n",
      "                       'Morgane Rivière, Mihir Sanjay Kale, Juliette Love, et '\n",
      "                       'al. “Gemma: Open models based on gemini research and\\n'\n",
      "                       'technology”. In: arXiv preprint arXiv:2403.08295 '\n",
      "                       '(2024).\\n'\n",
      "                       '22\\n'\n",
      "                       '[104] W Scott Terry. Learning and memory: Basic '\n",
      "                       'principles, processes, and procedures . Routledge, '\n",
      "                       '2017.\\n'\n",
      "                       '[105] Matteo Tiezzi, Michele Casoni, Alessandro Betti, '\n",
      "                       'Tommaso Guidi, Marco Gori, and Stefano Melacci. “On '\n",
      "                       'the\\n'\n",
      "                       'resurgence of recurrent models for long sequences: '\n",
      "                       'Survey and research opportunities in the transformer '\n",
      "                       'era”. In:\\n'\n",
      "                       'arXiv preprint arXiv:2402.08132 (2024).\\n'\n",
      "                       '[106] Hugo Touvron, Thibaut Lavril, Gautier Izacard, '\n",
      "                       'Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, '\n",
      "                       'Baptiste\\n'\n",
      "                       'Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et '\n",
      "                       'al. “Llama: Open and efficient foundation language '\n",
      "                       'models”.\\n'\n",
      "                       'In: arXiv preprint arXiv:2302.13971 (2023).\\n'\n",
      "                       '[107] Jos Van Der Westhuizen and Joan Lasenby. “The '\n",
      "                       'unreasonable effectiveness of the forget gate”. '\n",
      "                       'In:arXiv preprint\\n'\n",
      "                       'arXiv:1804.04849 (2018).\\n'\n",
      "                       '[108] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob '\n",
      "                       'Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser,\\n'\n",
      "                       'and Illia Polosukhin. “Attention is All you Need”. In: '\n",
      "                       'Advances in Neural Information Processing Systems . '\n",
      "                       'Ed.\\n'\n",
      "                       'by I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. '\n",
      "                       'Fergus, S. Vishwanathan, and R. Garnett. Vol. 30. '\n",
      "                       'Cur-\\n'\n",
      "                       'ran Associates, Inc., 2017. url: https : / / '\n",
      "                       'proceedings . neurips . cc / paper _ files / paper / '\n",
      "                       '2017 / file /\\n'\n",
      "                       '3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.\\n'\n",
      "                       '[109] Shida Wang. “LongSSM: On the Length Extension of '\n",
      "                       'State-space Models in Language Modelling”. In: arXiv '\n",
      "                       'preprint\\n'\n",
      "                       'arXiv:2406.02080 (2024).\\n'\n",
      "                       '[110] Yu Wang, Yifan Gao, Xiusi Chen, Haoming Jiang, '\n",
      "                       'Shiyang Li, Jingfeng Yang, Qingyu Yin, Zheng Li, Xian '\n",
      "                       'Li, Bing Yin,\\n'\n",
      "                       'Jingbo Shang, and Julian McAuley. “MEMORYLLM: Towards '\n",
      "                       'Self-Updatable Large Language Models”. In:Forty-first\\n'\n",
      "                       'International Conference on Machine Learning . 2024. '\n",
      "                       'url: https://openreview.net/forum?id=p0lKWzdikQ.\\n'\n",
      "                       '[111] Yu Wang, Chi Han, Tongtong Wu, Xiaoxin He, '\n",
      "                       'Wangchunshu Zhou, Nafis Sadeq, Xiusi Chen, Zexue He, '\n",
      "                       'Wei Wang,\\n'\n",
      "                       'Gholamreza Haffari, et al. “Towards LifeSpan Cognitive '\n",
      "                       'Systems”. In: arXiv preprint arXiv:2409.13265 (2024).\\n'\n",
      "                       '[112] Zhiwei Wang, Yao Ma, Zitao Liu, and Jiliang '\n",
      "                       'Tang. “R-transformer: Recurrent neural network '\n",
      "                       'enhanced transformer”.\\n'\n",
      "                       'In: arXiv preprint arXiv:1907.05572 (2019).\\n'\n",
      "                       '[113] Jason Weston, Sumit Chopra, and Antoine Bordes. '\n",
      "                       '“Memory networks”. In: arXiv preprint arXiv:1410.3916 '\n",
      "                       '(2014).\\n'\n",
      "                       '[114] Bernard Widrow and Marcian E Hoff. “Adaptive '\n",
      "                       'switching circuits”. In: Neurocomputing: foundations '\n",
      "                       'of research .\\n'\n",
      "                       '1988, pp. 123–134.\\n'\n",
      "                       '[115] Ronald J Williams and David Zipser. “A learning '\n",
      "                       'algorithm for continually running fully recurrent '\n",
      "                       'neural networks”.\\n'\n",
      "                       'In: Neural computation 1.2 (1989), pp. 270–280.\\n'\n",
      "                       '[116] Daniel B Willingham. “Systems of memory in the '\n",
      "                       'human brain”. In: Neuron 18.1 (1997), pp. 5–8.\\n'\n",
      "                       '[117] Chao-Yuan Wu, Christoph Feichtenhofer, Haoqi '\n",
      "                       'Fan, Kaiming He, Philipp Krahenbuhl, and Ross '\n",
      "                       'Girshick. “Long-\\n'\n",
      "                       'term feature banks for detailed video understanding”. '\n",
      "                       'In: Proceedings of the IEEE/CVF conference on computer '\n",
      "                       'vision\\n'\n",
      "                       'and pattern recognition . 2019, pp. 284–293.\\n'\n",
      "                       '[118] Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, '\n",
      "                       'Jianmin Wang, and Mingsheng Long. “TimesNet: Temporal '\n",
      "                       '2D-\\n'\n",
      "                       'Variation Modeling for General Time Series Analysis”. '\n",
      "                       'In: The Eleventh International Conference on Learning\\n'\n",
      "                       'Representations. 2023. url: '\n",
      "                       'https://openreview.net/forum?id=ju_Uqw384Oq.\\n'\n",
      "                       '[119] Qingyang Wu, Zhenzhong Lan, Kun Qian, Jing Gu, '\n",
      "                       'Alborz Geramifard, and Zhou Yu. “Memformer: A memory-\\n'\n",
      "                       'augmented transformer for sequence modeling”. In: '\n",
      "                       'arXiv preprint arXiv:2010.06891 (2020).\\n'\n",
      "                       '[120] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song '\n",
      "                       'Han, and Mike Lewis. “Efficient Streaming Language '\n",
      "                       'Models\\n'\n",
      "                       'with Attention Sinks”. In: The Twelfth International '\n",
      "                       'Conference on Learning Representations . 2024. url: '\n",
      "                       'https:\\n'\n",
      "                       '//openreview.net/forum?id=NG7sS51zVF.\\n'\n",
      "                       '[121] An Yang, Baosong Yang, Beichen Zhang, Binyuan '\n",
      "                       'Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, '\n",
      "                       'Fei\\n'\n",
      "                       'Huang, Haoran Wei, et al. “Qwen2. 5 Technical Report”. '\n",
      "                       'In:arXiv preprint arXiv:2412.15115 (2024).\\n'\n",
      "                       '[122] Songlin Yang, Jan Kautz, and Ali Hatamizadeh. '\n",
      "                       '“Gated Delta Networks: Improving Mamba2 with Delta '\n",
      "                       'Rule”. In:\\n'\n",
      "                       'arXiv preprint arXiv:2412.06464 (2024).\\n'\n",
      "                       '[123] Songlin Yang, Bailin Wang, Yikang Shen, Rameswar '\n",
      "                       'Panda, and Yoon Kim. “Gated Linear Attention '\n",
      "                       'Transformers\\n'\n",
      "                       'with Hardware-Efficient Training”. In: Forty-first '\n",
      "                       'International Conference on Machine Learning . 2024. '\n",
      "                       'url: https:\\n'\n",
      "                       '//openreview.net/forum?id=ia5XvxFUJT.\\n'\n",
      "                       '[124] Songlin Yang, Bailin Wang, Yu Zhang, Yikang '\n",
      "                       'Shen, and Yoon Kim. “Parallelizing Linear Transformers '\n",
      "                       'with the\\n'\n",
      "                       'Delta Rule over Sequence Length”. In:The Thirty-eighth '\n",
      "                       'Annual Conference on Neural Information Processing '\n",
      "                       'Systems .\\n'\n",
      "                       '2024. url: '\n",
      "                       'https://openreview.net/forum?id=y8Rm4VNRPH.\\n'\n",
      "                       '[125] Luca Zancato, Arjun Seshadri, Yonatan Dukler, '\n",
      "                       'Aditya Golatkar, Yantao Shen, Benjamin Bowman, Matthew '\n",
      "                       'Trager,\\n'\n",
      "                       'Alessandro Achille, and Stefano Soatto. “B’MOJO: '\n",
      "                       'Hybrid State Space Realizations of Foundation Models '\n",
      "                       'with\\n'\n",
      "                       'Eidetic and Fading Memory”. In: The Thirty-eighth '\n",
      "                       'Annual Conference on Neural Information Processing '\n",
      "                       'Systems .\\n'\n",
      "                       '2024. url: '\n",
      "                       'https://openreview.net/forum?id=RnQdRY1h5v.\\n'\n",
      "                       '23\\n'\n",
      "                       '[126] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali '\n",
      "                       'Farhadi, and Yejin Choi. “HellaSwag: Can a Machine '\n",
      "                       'Really Finish\\n'\n",
      "                       'Your Sentence?” In: Proceedings of the 57th Annual '\n",
      "                       'Meeting of the Association for Computational '\n",
      "                       'Linguistics . Ed. by\\n'\n",
      "                       'Anna Korhonen, David Traum, and Lluís Màrquez. '\n",
      "                       'Florence, Italy: Association for Computational '\n",
      "                       'Linguistics, July\\n'\n",
      "                       '2019, pp. 4791–4800. doi: 10.18653/v1/P19-1472. url: '\n",
      "                       'https://aclanthology.org/P19-1472/.\\n'\n",
      "                       '[127] Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu. '\n",
      "                       '“Are transformers effective for time series '\n",
      "                       'forecasting?” In:\\n'\n",
      "                       'Proceedings of the AAAI conference on artificial '\n",
      "                       'intelligence . Vol. 37. 2023, pp. 11121–11128.\\n'\n",
      "                       '[128] Hao Zhang, Alexander C Berg, Michael Maire, and '\n",
      "                       'Jitendra Malik. “SVM-KNN: Discriminative nearest '\n",
      "                       'neighbor\\n'\n",
      "                       'classification for visual category recognition”. In: '\n",
      "                       '2006 IEEE Computer Society Conference on Computer '\n",
      "                       'Vision and\\n'\n",
      "                       'Pattern Recognition (CVPR’06) . Vol. 2. IEEE. 2006, '\n",
      "                       'pp. 2126–2136.\\n'\n",
      "                       '[129] Jianyu Zhang, Niklas Nolte, Ranajoy Sadhukhan, '\n",
      "                       'Beidi Chen, and Léon Bottou. “Memory Mosaics”. '\n",
      "                       'In:arXiv preprint\\n'\n",
      "                       'arXiv:2405.06394 (2024).\\n'\n",
      "                       '[130] Yunhao Zhang and Junchi Yan. “Crossformer: '\n",
      "                       'Transformer utilizing cross-dimension dependency for '\n",
      "                       'multivariate\\n'\n",
      "                       'time series forecasting”. In: The eleventh '\n",
      "                       'international conference on learning representations . '\n",
      "                       '2023.\\n'\n",
      "                       '[131] Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai '\n",
      "                       'Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang. '\n",
      "                       '“Informer:\\n'\n",
      "                       'Beyond efficient transformer for long sequence '\n",
      "                       'time-series forecasting”. In: Proceedings of the AAAI '\n",
      "                       'conference on\\n'\n",
      "                       'artificial intelligence . Vol. 35. 12. 2021, pp. '\n",
      "                       '11106–11115.\\n'\n",
      "                       '[132] Luisa Zintgraf, Kyriacos Shiarli, Vitaly Kurin, '\n",
      "                       'Katja Hofmann, and Shimon Whiteson. “Fast context '\n",
      "                       'adaptation via\\n'\n",
      "                       'meta-learning”. In: International Conference on '\n",
      "                       'Machine Learning . PMLR. 2019, pp. 7693–7702.\\n'\n",
      "                       '24\\n'\n",
      "                       'A Related Work\\n'\n",
      "                       'There are diverse perspectives that can independently '\n",
      "                       'lead to the design of Titans or its components. '\n",
      "                       'Accordingly, to\\n'\n",
      "                       'further situate our work in a broader context, we '\n",
      "                       'review three categories of studies:\\n'\n",
      "                       'A.1 Linear Recurrent Models\\n'\n",
      "                       'Recently, to address the computational cost of '\n",
      "                       'Transformers in both training and inference, linear '\n",
      "                       'recurrent models\\n'\n",
      "                       'have attracted much attention (Tiezzi et al. 2024), '\n",
      "                       'mainly due to their fast inference and training. The '\n",
      "                       'first generation\\n'\n",
      "                       'of models–such as RetNet (Yutao Sun et al. 2023), LRU '\n",
      "                       '(Orvieto et al. 2023), RWKV (Peng, Alcaide, et al. '\n",
      "                       '2023), S5 (J. T.\\n'\n",
      "                       'Smith, Warrington, and Linderman 2023), and S4 (Gu, '\n",
      "                       'Goel, and Re 2022)–uses data-independent transition '\n",
      "                       'matrix/decay\\n'\n",
      "                       'mechanism. The second generation of such models '\n",
      "                       'started to incorporate gating mechanism, a widely used '\n",
      "                       'techniques\\n'\n",
      "                       'in traditional RNNs (Gers, Jürgen Schmidhuber, and '\n",
      "                       'Cummins 2000; Greff et al. 2016; Van Der Westhuizen '\n",
      "                       'and Lasenby\\n'\n",
      "                       '2018), into such linear architectures–e.g., Griffin '\n",
      "                       '(De et al. 2024), SSMs (Behrouz, Santacatterina, and '\n",
      "                       'Zabih 2024; Dao\\n'\n",
      "                       'and Gu 2024; Gu and Dao 2024; Hasani et al. 2023), '\n",
      "                       'RWKV6 (Peng, Goldstein, et al. 2024). The third '\n",
      "                       'generation of linear\\n'\n",
      "                       'recurrent models are based on more complex memory '\n",
      "                       'updating rule based on meta-learning, online learning, '\n",
      "                       'and/or\\n'\n",
      "                       'delta-rule, resulting in more expressive and effective '\n",
      "                       'models such as: Longhorn (B. Liu et al. 2024), Gated '\n",
      "                       'DeltaNet (S. Yang,\\n'\n",
      "                       'Kautz, and Hatamizadeh 2024), TTT (Yu Sun et al. '\n",
      "                       '2024), and DeltaNet (S. Yang, B. Wang, Yu Zhang, et '\n",
      "                       'al. 2024). Our\\n'\n",
      "                       'LMM model can be seen as the next generation of such '\n",
      "                       'models, in which we incorporate the token flow into '\n",
      "                       'the memory\\n'\n",
      "                       'updating mechanism, having more powerful memory '\n",
      "                       'updating process. See Appendix C for a detailed '\n",
      "                       'discussion of\\n'\n",
      "                       'different recurrent models and Titans.\\n'\n",
      "                       'A.2 Transformer-based Architectures\\n'\n",
      "                       'Transformers. Transformers (Vaswani et al. 2017) as '\n",
      "                       'the de facto backbone for many deep learning models '\n",
      "                       'are based on\\n'\n",
      "                       'attention mechanism (Bahdanau 2014). They, however, '\n",
      "                       'suffer from quadratic computational cost, limiting '\n",
      "                       'their ability\\n'\n",
      "                       'to scale to long context window. To improve the memory '\n",
      "                       'consumption and throughput of softmax attention for '\n",
      "                       'longer\\n'\n",
      "                       'sequences, various studies focused on I/O aware '\n",
      "                       'implementations of attention (Dao 2024; Dao, D. Fu, et '\n",
      "                       'al. 2022), designing\\n'\n",
      "                       'more efficient attention mechanisms by sparsifying the '\n",
      "                       'attention matrix (B. Chen et al. 2021; Choromanski et '\n",
      "                       'al. 2021; Dai\\n'\n",
      "                       'et al. 2019; J. Dong et al. 2024; Roy et al. 2021), '\n",
      "                       'approximating the softmax (Arora et al. 2024), or '\n",
      "                       'developing kernel-based\\n'\n",
      "                       '(linear) attentions (Aksenov et al. 2024; Kacham, '\n",
      "                       'Mirrokni, and P. Zhong 2024; Schlag, Irie, and Jürgen '\n",
      "                       'Schmidhuber 2021;\\n'\n",
      "                       'S. Yang, B. Wang, Shen, et al. 2024).\\n'\n",
      "                       'Segment-based Transformers. Another line of research '\n",
      "                       'to improve the efficiency of Transformers is '\n",
      "                       'segment-based or\\n'\n",
      "                       'Chunk Transformers (Dai et al. 2019). The main '\n",
      "                       'drawback of chunk Transformers is that segments are '\n",
      "                       'fully separated and\\n'\n",
      "                       'so the context window is limited to the length of the '\n",
      "                       'chunks. To address this issue, various studies discuss '\n",
      "                       'the importance\\n'\n",
      "                       'of a memory so it can help the model to transfer '\n",
      "                       'information across chunks (Bulatov, Yuri Kuratov, et '\n",
      "                       'al. 2023; Bulatov,\\n'\n",
      "                       'Yury Kuratov, and Burtsev 2022; Feng et al. 2022; '\n",
      "                       'Hutchins et al. 2022; Rodkin et al. 2024; Z. Wang et '\n",
      "                       'al. 2019; Q. Wu\\n'\n",
      "                       'et al. 2020; Zancato et al. 2024). The key differences '\n",
      "                       'of Titans with these models are: (1) The memory in '\n",
      "                       'such models are\\n'\n",
      "                       'simple small size vectors, lacking expressive power to '\n",
      "                       'compress complex information; (2) The memory module '\n",
      "                       'lacks forget\\n'\n",
      "                       'mechanism, leading to a fast memory overflow; (3) only '\n",
      "                       'focus on momentary surprise, missing the information '\n",
      "                       'flow. More\\n'\n",
      "                       'specifically, recalling Recurrent Memory Transformers '\n",
      "                       '(RMT) (Bulatov, Yuri Kuratov, et al. 2023; Bulatov, '\n",
      "                       'Yury Kuratov,\\n'\n",
      "                       'and Burtsev 2022; Rodkin et al. 2024), one can treat '\n",
      "                       'Titans (MAC) as the generalization of RMT, where we '\n",
      "                       'use a neural\\n'\n",
      "                       'memory module instead of a vector-valued small size '\n",
      "                       'memory.\\n'\n",
      "                       'Memory for Large Language Models. Another interesting '\n",
      "                       'research direction has been to incorporate external '\n",
      "                       'memory\\n'\n",
      "                       'modules to LLMs after training (Z. He et al. 2024; '\n",
      "                       'Khandelwal et al. 2020; Y. Wang, Y. Gao, et al. 2024). '\n",
      "                       'Such models\\n'\n",
      "                       'are different from our approach as we incorporate the '\n",
      "                       'memory as a part of initial architecture and so we '\n",
      "                       'train it in\\n'\n",
      "                       'an end-to-end manner. Also, most of these explicit '\n",
      "                       'memory modules suffer from the same limitations as '\n",
      "                       'chunk-based\\n'\n",
      "                       'Transformers (mentioned above). For a detailed '\n",
      "                       'discussion of such models, we refer to the recent '\n",
      "                       'study of Y. Wang, Han,\\n'\n",
      "                       'et al. (2024).\\n'\n",
      "                       '25\\n'\n",
      "                       'A.3 Test Time Training and Fast Weight Programs\\n'\n",
      "                       'Memory Design and Augmentation with Memory. In the '\n",
      "                       'literature, a substantial research effort have been '\n",
      "                       'toward\\n'\n",
      "                       'designing memory modules that are capable of either '\n",
      "                       'memorizing the knowledge abstraction (e.g., persistent '\n",
      "                       'mem-\\n'\n",
      "                       'ory) (Sukhbaatar, Grave, et al. 2019), or memorizing '\n",
      "                       'the data-dependent information (also known as '\n",
      "                       'contextual memory),\\n'\n",
      "                       'through recurrence (Bulatov, Yury Kuratov, and Burtsev '\n",
      "                       '2022; Rodkin et al. 2024; Zancato et al. 2024), '\n",
      "                       'Transformers (Berges\\n'\n",
      "                       'et al. 2024; Cetin et al. 2024; Feng et al. 2022; Le, '\n",
      "                       'Tran, and Venkatesh 2020; Munkhdalai, Faruqui, and '\n",
      "                       'Gopal 2024; J. Zhang\\n'\n",
      "                       'et al. 2024), gradient (Irie, Csordás, and Jürgen '\n",
      "                       'Schmidhuber 2022; Munkhdalai, Sordoni, et al. 2019), '\n",
      "                       'or other learning\\n'\n",
      "                       'paradigms (Sukhbaatar, Weston, Fergus, et al. 2015; '\n",
      "                       'Weston, Chopra, and Bordes 2014). These memory models, '\n",
      "                       'however,\\n'\n",
      "                       'either (1) are based on momentary surprise, missing '\n",
      "                       'the data flow and events, (2) lack forget mechanisms '\n",
      "                       'to remove\\n'\n",
      "                       'the memory, leading to a fast memory overflow (3) are '\n",
      "                       'fixed-size shallow (matrix valued) memory, resulting '\n",
      "                       'in poor\\n'\n",
      "                       'performance in long context, and (4) are based on '\n",
      "                       'fixed parameters at test time, lacking test time '\n",
      "                       'adaption.\\n'\n",
      "                       'Fast Weight Programs. The idea of seeing linear layers '\n",
      "                       'as the key-value (associative) memory system backs to '\n",
      "                       'fast\\n'\n",
      "                       'weight programs, in which dynamic fast programs are '\n",
      "                       'incorporated into recurrent neural networks to serve '\n",
      "                       'as writable\\n'\n",
      "                       'memory (Schlag, Irie, and Jürgen Schmidhuber 2021; JH '\n",
      "                       'Schmidhuber 1992; Jürgen Schmidhuber 1993). The two '\n",
      "                       'learning\\n'\n",
      "                       'rules of Hebbian (Hebb 2005) and delta (Prados and Kak '\n",
      "                       '1989) are the most popular learning rules for fast '\n",
      "                       'weight programs,\\n'\n",
      "                       'which have been extensively explored in various '\n",
      "                       'studies (Irie, Schlag, et al. 2021; Munkhdalai, '\n",
      "                       'Sordoni, et al. 2019;\\n'\n",
      "                       'Munkhdalai and H. Yu 2017; Schlag, Irie, and Jürgen '\n",
      "                       'Schmidhuber 2021; JH Schmidhuber 1992; S. Yang, Kautz, '\n",
      "                       'and\\n'\n",
      "                       'Hatamizadeh 2024; S. Yang, B. Wang, Yu Zhang, et al. '\n",
      "                       '2024). All these models, however, are based on '\n",
      "                       'momentary surprise,\\n'\n",
      "                       'missing the token flow in the sequences (see Section '\n",
      "                       '3.1), and most of them lacks a forgetting gate, '\n",
      "                       'resulting in a poor\\n'\n",
      "                       'memory management.\\n'\n",
      "                       'Test Time Training. The key ideas of learning at test '\n",
      "                       'time or learning to learn (i.e., (Andrychowicz et al. '\n",
      "                       '2016)) backs to\\n'\n",
      "                       'very early studies on local learning Bottou and Vapnik '\n",
      "                       '1992, in which each test data sample is trained on its '\n",
      "                       'neighbors\\n'\n",
      "                       'before making a prediction (Gandelsman et al. 2022; H. '\n",
      "                       'Zhang et al. 2006). This approach further has shown '\n",
      "                       'promising\\n'\n",
      "                       'performance in vision tasks (Jain and Learned-Miller '\n",
      "                       '2011; Mullapudi et al. 2019), mostly due to their '\n",
      "                       'ability to mitigate\\n'\n",
      "                       'out-of-distribution samples. The most similar studies '\n",
      "                       'to ours in this direction are MNM (Munkhdalai, '\n",
      "                       'Sordoni, et al. 2019)\\n'\n",
      "                       'and TTT-layer (Yu Sun et al. 2024), which we discussed '\n",
      "                       'the key differences in Appendix C.\\n'\n",
      "                       'B Language Modeling and Common-sense Reasoning '\n",
      "                       'Datasets\\n'\n",
      "                       'Following recent studies on linear recurrent models '\n",
      "                       '(Dao and Gu 2024; S. Yang, Kautz, and Hatamizadeh '\n",
      "                       '2024; S. Yang,\\n'\n",
      "                       'B. Wang, Yu Zhang, et al. 2024), we use Wikitext '\n",
      "                       '(Merity et al. 2017), LMB (Paperno et al. 2016), PIQA '\n",
      "                       '(Bisk et al. 2020),\\n'\n",
      "                       'HellaSwag (Zellers et al. 2019), WinoGrande (Sakaguchi '\n",
      "                       'et al. 2021), ARC-easy (ARC-e) and ARC-challenge '\n",
      "                       '(ARC-c) (P.\\n'\n",
      "                       'Clark et al. 2018), SIQA (Sap et al. 2019), and BoolQ '\n",
      "                       '(C. Clark et al. 2019). Also, the baselines results '\n",
      "                       'for 400M models are\\n'\n",
      "                       'from the reported results by S. Yang, Kautz, and '\n",
      "                       'Hatamizadeh (2024).\\n'\n",
      "                       'C Long-term Memory Module (LMM) as a Sequence Model\\n'\n",
      "                       'In this section, we discuss how LMM as a sequence '\n",
      "                       'model is connected to modern linear recurrent models. '\n",
      "                       'For the sake\\n'\n",
      "                       'of simplicity, we start with a linear memory, where M𝑡 '\n",
      "                       '= 𝑊𝑡 ∈R𝑑in ×𝑑in . In this case, our objective function '\n",
      "                       'becomes\\n'\n",
      "                       'ℓ(M; 𝑥𝑡)= 1\\n'\n",
      "                       '2 ∥M𝑡k𝑡 −v𝑡∥2\\n'\n",
      "                       '2, in which we use gradient descent with momentum and '\n",
      "                       'weight decay for the optimization.\\n'\n",
      "                       'Accordingly, revisiting the recurrent formula in '\n",
      "                       'Equation 13:\\n'\n",
      "                       'M𝑡 = diag (1 −𝛼𝑡)M𝑡 +𝑆𝑡 (32)\\n'\n",
      "                       '𝑆𝑡 = diag (𝜂𝑡)𝑆𝑡−1 −diag (𝜃𝑡)\\x00M𝑡−1k⊤\\n'\n",
      "                       '𝑡 k𝑡 −v⊤\\n'\n",
      "                       '𝑡 k𝑡\\n'\n",
      "                       '\\x01 . (33)\\n'\n",
      "                       'LMM is Generalized Gated DeltaNet. As discussed by S. '\n",
      "                       'Yang, Kautz, and Hatamizadeh (2024), DeltaNet (S. '\n",
      "                       'Yang, B. Wang,\\n'\n",
      "                       'Yu Zhang, et al. 2024) can alternatively be '\n",
      "                       'interpreted as an online learning problem that '\n",
      "                       'optimizes the L= 1\\n'\n",
      "                       '2 ∥S𝑡k𝑡 −v𝑡∥2\\n'\n",
      "                       '2,\\n'\n",
      "                       'resulting in:\\n'\n",
      "                       'S𝑡+1 = S𝑡 −𝜃𝑡∇L= S𝑡\\n'\n",
      "                       '\\x00I −𝜃𝑡k𝑡k⊤\\n'\n",
      "                       '𝑡\\n'\n",
      "                       '\\x01 +𝜃𝑡v𝑡k⊤\\n'\n",
      "                       '𝑡 . (34)\\n'\n",
      "                       '26\\n'\n",
      "                       'In this formulation, Gated DeltaNet is the same as '\n",
      "                       'above but with an additional weight decay term (S. '\n",
      "                       'Yang, Kautz, and\\n'\n",
      "                       'Hatamizadeh 2024). Comparing Equation 32 and Equation '\n",
      "                       '34, we can see that setting 𝜂𝑡 = 0 results in both '\n",
      "                       'formulations to\\n'\n",
      "                       'be equivalent. Accordingly, we can say LMM is '\n",
      "                       'generalizing the very recent study of Gated DeltaNet '\n",
      "                       '(S. Yang, Kautz, and\\n'\n",
      "                       'Hatamizadeh 2024) from three aspects:\\n'\n",
      "                       '• Momentum-based Rule: The Delta Rule is based on '\n",
      "                       'momentary surprise, meaning that the flow of tokens '\n",
      "                       'cannot\\n'\n",
      "                       'affect the memory update rule. LMM, however, is based '\n",
      "                       'on a momentum rule, which consider both past and\\n'\n",
      "                       'momentary surprise.\\n'\n",
      "                       '• Deep Memory: While Gated DeltaNet is limited to a '\n",
      "                       'linear (matrix-valued) memory as it requires finding '\n",
      "                       'the closed\\n'\n",
      "                       'recurrence form, LMM allows using deep memory module '\n",
      "                       'by using a gradient-based formulation, resulting in '\n",
      "                       'higher\\n'\n",
      "                       'expressive power.\\n'\n",
      "                       '• Non-Linear Recurrence: While DeltaNet and Gated '\n",
      "                       'DeltaNet are based on linear recurrence, our LMM is '\n",
      "                       'using\\n'\n",
      "                       'inter-chunk non-linear recurrence and intra-chunk '\n",
      "                       'linear recurrence. This design allows LMM having a '\n",
      "                       'higher\\n'\n",
      "                       'expressive power.\\n'\n",
      "                       'Here, we discussed Gated DeltaNet as a sample of '\n",
      "                       'recent generation of recurrent models. Similar '\n",
      "                       'approaches such\\n'\n",
      "                       'as RWKV-7 (Peng 2021) are also using the same '\n",
      "                       'formulation and loss function, and so LMM is '\n",
      "                       'generalizing all such\\n'\n",
      "                       'models.\\n'\n",
      "                       'LMM is Generalized Longhorn. Similar to DeltaNet, '\n",
      "                       'Longhorn (B. Liu et al. 2024) uses the same loss '\n",
      "                       'function but it\\n'\n",
      "                       'derives the closed form using implicit online '\n",
      "                       'learning:\\n'\n",
      "                       'S𝑡+1 = S𝑡\\n'\n",
      "                       '\\x00I −𝛿𝑡k𝑡k⊤\\n'\n",
      "                       '𝑡\\n'\n",
      "                       '\\x01 +𝛿𝑡v𝑡k⊤\\n'\n",
      "                       '𝑡 , (35)\\n'\n",
      "                       'where 𝛿𝑡 = 𝜃𝑡\\n'\n",
      "                       '1+𝜃𝑡 k𝑡 k⊤\\n'\n",
      "                       '𝑡\\n'\n",
      "                       '. It, however, lacks a forgetting gate, resulting in a '\n",
      "                       'faster memory overflow. Therefore, in addition two\\n'\n",
      "                       'the abovementioned aspects of (1) Momentum-based Rule, '\n",
      "                       '(2) Deep Memory, and (3) Non-Linear Recurrence, LMM '\n",
      "                       'has\\n'\n",
      "                       'the advantage of using an additional (4) Forget Gate, '\n",
      "                       'leading to a better memory management.\\n'\n",
      "                       'LMM is Generalized TTT Layer. To the best of our '\n",
      "                       'knowledge, TTT (Yu Sun et al. 2024), is the only '\n",
      "                       'modern linear\\n'\n",
      "                       'recurrent models with a gradient-based updating rule. '\n",
      "                       'In addition to different architectural designs and '\n",
      "                       'also objective\\n'\n",
      "                       'functions, our LMM has three key differences with '\n",
      "                       'presented TTT layers (Yu Sun et al. 2024):\\n'\n",
      "                       '1. Forgetting Mechanism : TTT layers are updating '\n",
      "                       'memory at each time, without having the chance to '\n",
      "                       'forget the\\n'\n",
      "                       'past data. Accordingly, when fixing the memory size, '\n",
      "                       'the model cannot manage the memory for long sequences. '\n",
      "                       'A\\n'\n",
      "                       'forget mechanism, such as LMM’s, allows clearing the '\n",
      "                       'memory when very past information is not needed '\n",
      "                       'anymore.\\n'\n",
      "                       'We show that in a general case, this forget mechanism '\n",
      "                       'is equivalent to weight decay and provide a fast '\n",
      "                       'method to\\n'\n",
      "                       'incorporate it into the parallel training.\\n'\n",
      "                       '2. Momentum-based Update Rule : TTT layers are based '\n",
      "                       'on momentary surprise, meaning that the flow of '\n",
      "                       'tokens\\n'\n",
      "                       'cannot affect the memory update rule. LMM, however, is '\n",
      "                       'based on a momentum rule, which consider both past '\n",
      "                       'and\\n'\n",
      "                       'momentary surprise. See Section 3.1 for the motivation '\n",
      "                       'of this design.\\n'\n",
      "                       '3. Deep Memory : While TTT-layers allows for deeper '\n",
      "                       'memory, the advantages/disadvantages of such deeper '\n",
      "                       'memory\\n'\n",
      "                       'modules have not been experimentally evaluated.\\n'\n",
      "                       'To the best of our knowledge, our neural long-term '\n",
      "                       'memory module is the first linear recurrent model with '\n",
      "                       'momentum-\\n'\n",
      "                       'based update rule.\\n'\n",
      "                       'Finally, as a key difference with all the above and '\n",
      "                       'other recent linear recurrent studies, note that the '\n",
      "                       'hybrid variants of\\n'\n",
      "                       'modern linear models–such as Griffin (De et al. 2024), '\n",
      "                       'DeltaNet (S. Yang, B. Wang, Yu Zhang, et al. 2024), '\n",
      "                       'Gated DeltaNet (S.\\n'\n",
      "                       'Yang, Kautz, and Hatamizadeh 2024), H3 (D. Y. Fu et '\n",
      "                       'al. 2023), Mamba2 (Dao and Gu 2024), Samba (Ren et al. '\n",
      "                       '2024), etc.–all\\n'\n",
      "                       'are based on sequential layer-wise design. We present '\n",
      "                       'Titans to show how effectively one can incorporate '\n",
      "                       'such memory\\n'\n",
      "                       'modules into an architecture.\\n'\n",
      "                       '27'}}\n",
      "\u001b[36;1m\u001b[1;3m[0:checkpoint]\u001b[0m \u001b[1mState at the end of step 0:\n",
      "\u001b[0m{'process_pdf': {'state': {'error': None,\n",
      "                           'extracted_info': None,\n",
      "                           'pdf_text': 'Titans: Learning to Memorize at Test '\n",
      "                                       'Time\\n'\n",
      "                                       'Ali Behrouz\\n'\n",
      "                                       '†\\n'\n",
      "                                       ', Peilin Zhong\\n'\n",
      "                                       '†\\n'\n",
      "                                       ', and Vahab Mirrokni\\n'\n",
      "                                       '†\\n'\n",
      "                                       '†\\n'\n",
      "                                       'Google Research\\n'\n",
      "                                       '{alibehrouz, peilinz, '\n",
      "                                       'mirrokni}@google.com\\n'\n",
      "                                       'Abstract\\n'\n",
      "                                       'Over more than a decade there has been '\n",
      "                                       'an extensive research effort of how '\n",
      "                                       'effectively utilize recurrent models '\n",
      "                                       'and\\n'\n",
      "                                       'attentions. While recurrent models aim '\n",
      "                                       'to compress the data into a fixed-size '\n",
      "                                       'memory (called hidden state), '\n",
      "                                       'attention allows\\n'\n",
      "                                       'attending to the entire context '\n",
      "                                       'window, capturing the direct '\n",
      "                                       'dependencies of all tokens. This more '\n",
      "                                       'accurate modeling\\n'\n",
      "                                       'of dependencies, however, comes with a '\n",
      "                                       'quadratic cost, limiting the model to '\n",
      "                                       'a fixed-length context. We present a '\n",
      "                                       'new\\n'\n",
      "                                       'neural long-term memory module that '\n",
      "                                       'learns to memorize historical context '\n",
      "                                       'and helps an attention to attend to '\n",
      "                                       'the\\n'\n",
      "                                       'current context while utilizing long '\n",
      "                                       'past information. We show that this '\n",
      "                                       'neural memory has the advantage of a '\n",
      "                                       'fast\\n'\n",
      "                                       'parallelizable training while '\n",
      "                                       'maintaining a fast inference. From a '\n",
      "                                       'memory perspective, we argue that '\n",
      "                                       'attention due to its\\n'\n",
      "                                       'limited context but accurate '\n",
      "                                       'dependency modeling performs as a '\n",
      "                                       'short-term memory, while neural memory '\n",
      "                                       'due to its\\n'\n",
      "                                       'ability to memorize the data, acts as '\n",
      "                                       'a long-term, more persistent, memory. '\n",
      "                                       'Based on these two modules, we '\n",
      "                                       'introduce\\n'\n",
      "                                       'a new family of architectures, called '\n",
      "                                       'Titans, and present three variants to '\n",
      "                                       'address how one can effectively '\n",
      "                                       'incorporate\\n'\n",
      "                                       'memory into this architecture. Our '\n",
      "                                       'experimental results on language '\n",
      "                                       'modeling, common-sense reasoning, '\n",
      "                                       'genomics,\\n'\n",
      "                                       'and time series tasks show that Titans '\n",
      "                                       'are more effective than Transformers '\n",
      "                                       'and recent modern linear recurrent '\n",
      "                                       'models.\\n'\n",
      "                                       'They further can effectively scale to '\n",
      "                                       'larger than 2M context window size '\n",
      "                                       'with higher accuracy in '\n",
      "                                       'needle-in-haystack tasks\\n'\n",
      "                                       'compared to baselines.\\n'\n",
      "                                       '1 Introduction\\n'\n",
      "                                       '“The true art of memory is the art of '\n",
      "                                       'attention!\"\\n'\n",
      "                                       '— Samuel Johnson, 1787\\n'\n",
      "                                       'T\\n'\n",
      "                                       'ransformers, pure attention-based '\n",
      "                                       'architectures (Vaswani et al. 2017), '\n",
      "                                       'have been firmly established as '\n",
      "                                       'state-of-\\n'\n",
      "                                       'the-art models in sequence modeling, '\n",
      "                                       'mainly due to their in-context '\n",
      "                                       'learning and ability to learn at scale '\n",
      "                                       '(Kaplan\\n'\n",
      "                                       'et al. 2020). The primary building '\n",
      "                                       'blocks of Transformers–attention '\n",
      "                                       'modules—function as associative '\n",
      "                                       'memory\\n'\n",
      "                                       'blocks (Bietti et al. 2024), where '\n",
      "                                       'they learn to store key-value '\n",
      "                                       'associations and retrieve them by '\n",
      "                                       'computing pairwise\\n'\n",
      "                                       'similarity between queries (i.e., '\n",
      "                                       'search signals) and keys (i.e., '\n",
      "                                       'contexts). Accordingly, by design, the '\n",
      "                                       'output of a Transformer\\n'\n",
      "                                       'is exclusively conditioned on the '\n",
      "                                       'direct dependencies of tokens in the '\n",
      "                                       'current context window. This accurate '\n",
      "                                       'modeling of\\n'\n",
      "                                       'dependencies, however, comes with '\n",
      "                                       'quadratic time and memory complexity '\n",
      "                                       'in terms of the context length. In '\n",
      "                                       'complex\\n'\n",
      "                                       'real-world tasks (e.g., language '\n",
      "                                       'modeling (N. F. Liu et al. 2024), '\n",
      "                                       'video understanding (C.-Y. Wu et al. '\n",
      "                                       '2019), long-term time\\n'\n",
      "                                       'series forecasting (H. Zhou et al. '\n",
      "                                       '2021)), the context window can become '\n",
      "                                       'extremely large, making the '\n",
      "                                       'applicability of\\n'\n",
      "                                       'Transformers challenging in these '\n",
      "                                       'downstream tasks.\\n'\n",
      "                                       'To overcome the scalability issue of '\n",
      "                                       'Transformers, recent studies aim to '\n",
      "                                       'design different variants of linear '\n",
      "                                       'Transform-\\n'\n",
      "                                       'ers (Kacham, Mirrokni, and P. Zhong '\n",
      "                                       '2024; Katharopoulos et al. 2020; S. '\n",
      "                                       'Yang, B. Wang, Shen, et al. 2024), '\n",
      "                                       'where softmax is\\n'\n",
      "                                       'replaced by a kernel function in the '\n",
      "                                       'attention (see §2.1 for details), '\n",
      "                                       'resulting in a significant drop in '\n",
      "                                       'memory consumption.\\n'\n",
      "                                       'Despite efficiency and the ability to '\n",
      "                                       'scale to longer context, linear '\n",
      "                                       'Transformers do not show competitive '\n",
      "                                       'performance\\n'\n",
      "                                       'compared to Transformers as the kernel '\n",
      "                                       'trick makes the model a linear '\n",
      "                                       'recurrent network, in which the data '\n",
      "                                       'is compressed\\n'\n",
      "                                       'into a matrix-valued states '\n",
      "                                       '(Katharopoulos et al. 2020). This, '\n",
      "                                       'however, brings a contradictory fact '\n",
      "                                       'about linear recurrent (or\\n'\n",
      "                                       'linear Transformers) models: On one '\n",
      "                                       'hand, we use these linear models to '\n",
      "                                       'enhance scalability and efficiency '\n",
      "                                       '(linear vs.\\n'\n",
      "                                       'quadratic complexity), whose '\n",
      "                                       'advantages is appeared for very long '\n",
      "                                       'context; On the other hand, a very '\n",
      "                                       'long context cannot\\n'\n",
      "                                       'be properly compressed in a small '\n",
      "                                       'vector-valued or matrix-valued states '\n",
      "                                       '(S. Wang 2024).\\n'\n",
      "                                       '1\\n'\n",
      "                                       'arXiv:2501.00663v1  [cs.LG]  31 Dec '\n",
      "                                       '2024\\n'\n",
      "                                       'Furthermore, beyond efficiency, most '\n",
      "                                       'existing architectures–ranging from '\n",
      "                                       'Hopfield Networks (Hopfield 1982) to '\n",
      "                                       'LSTMs (Jür-\\n'\n",
      "                                       'gen Schmidhuber and Hochreiter 1997) '\n",
      "                                       'and Transformers (Vaswani et al. '\n",
      "                                       '2017)–face challenges when dealing '\n",
      "                                       'with general-\\n'\n",
      "                                       'ization, length extrapolation, and/or '\n",
      "                                       'reasoning (Anil et al. 2022; Qin, Y. '\n",
      "                                       'Zhong, and Deng 2024), all of which '\n",
      "                                       'are inseparable\\n'\n",
      "                                       'parts of many hard real-world tasks. '\n",
      "                                       'Although these architectures draw '\n",
      "                                       'inspiration from the human brain, each '\n",
      "                                       'of which\\n'\n",
      "                                       'are missing: (1) a crucial component '\n",
      "                                       'for learning process—such as '\n",
      "                                       'short-term memory, long-term memory, '\n",
      "                                       'meta-memory,\\n'\n",
      "                                       'attending to current context, etc. '\n",
      "                                       '(Cowan 2008); (2) how these components '\n",
      "                                       'are interconnected systems that can '\n",
      "                                       'operate\\n'\n",
      "                                       'independently; and/or (3) the ability '\n",
      "                                       'to actively learn from data and '\n",
      "                                       'memorize the abstraction of past '\n",
      "                                       'history. We argue\\n'\n",
      "                                       'that in an effective learning '\n",
      "                                       'paradigm, similar to human brain, '\n",
      "                                       'there aredistinct yet interconnected '\n",
      "                                       'modules, each of which\\n'\n",
      "                                       'is responsible for a component crucial '\n",
      "                                       'to the learning process.\\n'\n",
      "                                       'Memory Perspective\\n'\n",
      "                                       'Memory is a fundamental mental process '\n",
      "                                       'and is an inseparable component of '\n",
      "                                       'human learning (Terry 2017). Without\\n'\n",
      "                                       'a properly functioning memory system, '\n",
      "                                       'humans and animals would be restricted '\n",
      "                                       'to basic reflexes and stereotyped\\n'\n",
      "                                       'behaviors. Accordingly, memory has '\n",
      "                                       'been the inspiration for many seminal '\n",
      "                                       'research in machine learning '\n",
      "                                       'literature; e.g.,\\n'\n",
      "                                       'Hopfield Networks (Hopfield 1982), '\n",
      "                                       'LSTMs (Jürgen Schmidhuber and '\n",
      "                                       'Hochreiter 1997), and Transformers '\n",
      "                                       '(Vaswani et al.\\n'\n",
      "                                       '2017).\\n'\n",
      "                                       'Taking inspiration from the common '\n",
      "                                       'definitions of memory and learning in '\n",
      "                                       'neuropsychology literature (Okano, '\n",
      "                                       'Hirano,\\n'\n",
      "                                       'and Balaban 2000), most existing '\n",
      "                                       'architectures consider memory as a '\n",
      "                                       'neural update caused by an input, and '\n",
      "                                       'define learning\\n'\n",
      "                                       'as a process for acquiring effective '\n",
      "                                       'and useful memory, given an objective. '\n",
      "                                       'In this perspective, Recurrent Neural '\n",
      "                                       'Networks\\n'\n",
      "                                       '(RNNs) (Williams and Zipser 1989) can '\n",
      "                                       'be defined as models with a '\n",
      "                                       'vector-valued memory module M(also '\n",
      "                                       'called hidden\\n'\n",
      "                                       'state) with two main steps: Given a '\n",
      "                                       'new input 𝑥𝑡 at time 𝑡, the model (1) '\n",
      "                                       'updates the memory using a function '\n",
      "                                       '𝑓(M𝑡−1,𝑥𝑡)\\n'\n",
      "                                       '(with compression); and (2) retrieves '\n",
      "                                       'the corresponding memory of input '\n",
      "                                       'using a function 𝑔(M𝑡,𝑥𝑡)(see §2.1 for '\n",
      "                                       'details).\\n'\n",
      "                                       'Similarly, Transformers can be seen as '\n",
      "                                       'architectures with a growing memory '\n",
      "                                       'and two similar steps. That is, the '\n",
      "                                       'pair of key\\n'\n",
      "                                       'and value matrices acts as the model’s '\n",
      "                                       'memory, and the model: (1) updates the '\n",
      "                                       'memory by appending the key and value '\n",
      "                                       'to\\n'\n",
      "                                       'the memory (without compression), and '\n",
      "                                       '(2) retrieves query vectors’ '\n",
      "                                       'corresponding memory by finding the '\n",
      "                                       'similarity of\\n'\n",
      "                                       'query and key vectors, which is then '\n",
      "                                       'used to weight the value vectors for '\n",
      "                                       'the output.\\n'\n",
      "                                       'This perspective, can help us better '\n",
      "                                       'understand existing paradigms, their '\n",
      "                                       'critical differences, and design more '\n",
      "                                       'effective\\n'\n",
      "                                       'architectures. For example, the main '\n",
      "                                       'difference between Transformers '\n",
      "                                       '(Vaswani et al. 2017) and linear '\n",
      "                                       'Transform-\\n'\n",
      "                                       'ers (Katharopoulos et al. 2020) is the '\n",
      "                                       'memory structure as well as the memory '\n",
      "                                       'updating step, in which linear '\n",
      "                                       'Transformers\\n'\n",
      "                                       'compress the historical data into a '\n",
      "                                       'fixed-size matrix-valued memory while '\n",
      "                                       'Transformers keep all historical data '\n",
      "                                       '(within\\n'\n",
      "                                       'the context length) without any '\n",
      "                                       'compression. While both linear '\n",
      "                                       'Transformers and linear RNNs '\n",
      "                                       '(including state space\\n'\n",
      "                                       'models) compress the information in '\n",
      "                                       'memory update step, the critical '\n",
      "                                       'difference lies in the structure of '\n",
      "                                       'the memory,\\n'\n",
      "                                       'where linear RNNs (vs. linear '\n",
      "                                       'Transformers) use a vector-valued '\n",
      "                                       'memory (vs. matrix-valued memory). '\n",
      "                                       'Therefore, this\\n'\n",
      "                                       'perspective motivates us to ask: (Q1) '\n",
      "                                       'What constitute a good structure for '\n",
      "                                       'the memory? (Q2) What is a proper '\n",
      "                                       'memory\\n'\n",
      "                                       'update mechanism? and (Q3) What is a '\n",
      "                                       'good memory retrieval process?\\n'\n",
      "                                       'Revisiting our understanding of human '\n",
      "                                       'memory, it is neither a unitary '\n",
      "                                       'process nor it serves a single '\n",
      "                                       'function (Cowan\\n'\n",
      "                                       '2008). In fact, memory is a '\n",
      "                                       'confederation of systems–e.g., '\n",
      "                                       'short-term, working, and long-term '\n",
      "                                       'memory–each serving a\\n'\n",
      "                                       'different function with different '\n",
      "                                       'neural structures, and each capable of '\n",
      "                                       'operating independently (Willingham '\n",
      "                                       '1997). This\\n'\n",
      "                                       'fact motivates us to ask: (Q4) How to '\n",
      "                                       'design an efficient architecture that '\n",
      "                                       'incorporates different interconnected '\n",
      "                                       'memory\\n'\n",
      "                                       'modules. Finally, storing a memory is '\n",
      "                                       'a neural process that requires to '\n",
      "                                       'encode and store the abstraction of '\n",
      "                                       'the past. It can\\n'\n",
      "                                       'be over-simplification to assume a '\n",
      "                                       'single vector or a matrix, whose '\n",
      "                                       'parameters are encoding the data in a '\n",
      "                                       'linear manner,\\n'\n",
      "                                       'are enough for storing long-term '\n",
      "                                       'history. (Q5) Is a deep memory module '\n",
      "                                       'needed to effectively store/remember '\n",
      "                                       'long\\n'\n",
      "                                       'past?\\n'\n",
      "                                       'Contributions and Roadmap\\n'\n",
      "                                       'In this paper, we aim to answer the '\n",
      "                                       'above five questions by designing a '\n",
      "                                       'long-term neural memory module, that '\n",
      "                                       'can\\n'\n",
      "                                       'efficiently and effectively learn to '\n",
      "                                       'memorize at test time. Building upon '\n",
      "                                       'its design, we discuss how it can be '\n",
      "                                       'incorporated\\n'\n",
      "                                       'into an architecture.\\n'\n",
      "                                       'Neural Memory (§3). We present a '\n",
      "                                       '(deep) neural long-term memory that '\n",
      "                                       '(as a meta in-context model) learns '\n",
      "                                       'how to\\n'\n",
      "                                       'memorize/store the data into its '\n",
      "                                       'parameters at test time. Inspired by '\n",
      "                                       'human long-term memory system (Mandler '\n",
      "                                       '2014),\\n'\n",
      "                                       '2\\n'\n",
      "                                       'we design this memory module so an '\n",
      "                                       'event that violates the expectations '\n",
      "                                       '(being surprising) is more memorable. '\n",
      "                                       'To this\\n'\n",
      "                                       'end, we measure the surprise of an '\n",
      "                                       'input with the gradient of the neural '\n",
      "                                       'network with respect to the input in '\n",
      "                                       'associative\\n'\n",
      "                                       'memory loss (see §3.1 for details). To '\n",
      "                                       'better handle the limited memory, we '\n",
      "                                       'present a decaying mechanism that '\n",
      "                                       'consider the\\n'\n",
      "                                       'proportion of memory size and the '\n",
      "                                       'amount of data surprise, resulting in '\n",
      "                                       'better memory management. We show that '\n",
      "                                       'this\\n'\n",
      "                                       'decay mechanism is in fact the '\n",
      "                                       'generalization of forgetting mechanism '\n",
      "                                       'in modern recurrent models (Dao and Gu '\n",
      "                                       '2024; Gu\\n'\n",
      "                                       'and Dao 2024; S. Yang, Kautz, and '\n",
      "                                       'Hatamizadeh 2024). Interestingly, we '\n",
      "                                       'find that this mechanism is equivalent '\n",
      "                                       'to optimizing\\n'\n",
      "                                       'a meta neural network with mini-batch '\n",
      "                                       'gradient descent, momentum, and weight '\n",
      "                                       'decay. Building upon tensorizing\\n'\n",
      "                                       'mini-batch gradient descent to use '\n",
      "                                       'more matmul operations (Yu Sun et al. '\n",
      "                                       '2024), we present a fast and '\n",
      "                                       'parallelizable\\n'\n",
      "                                       'algorithm to train our deep neural '\n",
      "                                       'long-term memory.\\n'\n",
      "                                       'Titans Architectures (§4). After '\n",
      "                                       'designing the long-term neural memory, '\n",
      "                                       'an important remaining question is how '\n",
      "                                       'to\\n'\n",
      "                                       'effectively and efficiently '\n",
      "                                       'incorporate memory into a deep '\n",
      "                                       'learning architecture. We present '\n",
      "                                       'Titans, a family of deep models\\n'\n",
      "                                       'that consists of three hyper-heads: '\n",
      "                                       '(1) Core: this module consists of the '\n",
      "                                       'short-term memory, and is responsible '\n",
      "                                       'for the main\\n'\n",
      "                                       'flow of processing the data (we use '\n",
      "                                       'attention with limited window size); '\n",
      "                                       '(2) Long-term Memory: this branch is '\n",
      "                                       'our neural\\n'\n",
      "                                       'long-term memory module that is '\n",
      "                                       'responsible to store/remember long '\n",
      "                                       'past; (3) Persistent Memory: this is a '\n",
      "                                       'set of learnable\\n'\n",
      "                                       'but date-independent parameters that '\n",
      "                                       'encodes the knowledge about a task. '\n",
      "                                       'Finally, as a proof of concept, we '\n",
      "                                       'present three\\n'\n",
      "                                       'variants of Titans, in which we '\n",
      "                                       'incorporate memory as: (i) a context, '\n",
      "                                       '(ii) a layer, and (iii) a gated '\n",
      "                                       'branch.\\n'\n",
      "                                       'Experimental Results (§5). We perform '\n",
      "                                       'experimental evaluations on language '\n",
      "                                       'modeling, commonsense reasoning, '\n",
      "                                       'recall-\\n'\n",
      "                                       'intensive, needle in haystack, time '\n",
      "                                       'series forecasting, and DNA modeling '\n",
      "                                       'tasks. We observe that our Titan '\n",
      "                                       'architecture\\n'\n",
      "                                       'outperforms all modern recurrent '\n",
      "                                       'models as well as their hybrid '\n",
      "                                       'variants (combining with '\n",
      "                                       'sliding-window attention) across\\n'\n",
      "                                       'a comprehensive set of benchmarks. '\n",
      "                                       'Furthermore, Titans outperforms '\n",
      "                                       'Transformers with the same context '\n",
      "                                       'window, and\\n'\n",
      "                                       'show competitive performance with '\n",
      "                                       'Transformers that use the entire '\n",
      "                                       'context. This results are achieved '\n",
      "                                       'while, contrary to\\n'\n",
      "                                       'Transformers, Titans scale to larger '\n",
      "                                       'than 2M context window size.\\n'\n",
      "                                       '2 Preliminaries\\n'\n",
      "                                       'I\\n'\n",
      "                                       'n this section, we discuss the '\n",
      "                                       'notation and some background concepts '\n",
      "                                       'that we use though the paper. We let\\n'\n",
      "                                       '𝑥 ∈R𝑁×𝑑in be the input, Mbe a neural '\n",
      "                                       'network (neural memory module), Q,K,V '\n",
      "                                       'be the query, key and value\\n'\n",
      "                                       'of the attention mechanism, and M be '\n",
      "                                       'the attention mask. When segmenting '\n",
      "                                       'the sequence, we use S(𝑖)to refer to\\n'\n",
      "                                       'the 𝑖-th segment. Through the paper, '\n",
      "                                       'we abuse the notation and use '\n",
      "                                       'subscripts to refer to a specific '\n",
      "                                       'element of a matrix,\\n'\n",
      "                                       'vector, or segments. For example, we '\n",
      "                                       'let S(𝑖)\\n'\n",
      "                                       '𝑗 be the 𝑗-th token in the 𝑖-th '\n",
      "                                       'segment. The only exception is '\n",
      "                                       'subscripts with 𝑡,\\n'\n",
      "                                       'which we reserved to index recurrence '\n",
      "                                       'over time, or the state of a neural '\n",
      "                                       'network at time𝑡. Given a neural '\n",
      "                                       'network Nand\\n'\n",
      "                                       'a data sample 𝑥, we use N(𝑥)(resp. '\n",
      "                                       'N∗(𝑥)) to refer to the forward pass '\n",
      "                                       'with (resp. without) weight '\n",
      "                                       'adjustment. Also, we\\n'\n",
      "                                       'abuse the notation and use N(𝑘)to '\n",
      "                                       'refer to the 𝑘-th layer of the neural '\n",
      "                                       'network. In the following, we first, '\n",
      "                                       'discuss the\\n'\n",
      "                                       'backgrounds for attention and its '\n",
      "                                       'efficient variants followed by a '\n",
      "                                       'review of modern linear RNNs. Finally, '\n",
      "                                       'we discuss a\\n'\n",
      "                                       'memory perspective of these '\n",
      "                                       'architectures that motivates us to '\n",
      "                                       'design Titans.\\n'\n",
      "                                       '2.1 Backgrounds\\n'\n",
      "                                       'Attention. Transformers (Vaswani et '\n",
      "                                       'al. 2017) as the de facto backbone for '\n",
      "                                       'many deep learning models are based '\n",
      "                                       'on\\n'\n",
      "                                       'attention mechanism. Given input 𝑥 '\n",
      "                                       '∈R𝑁×𝑑in , causal attention computes '\n",
      "                                       'output y ∈R𝑁×𝑑in based on softmax over '\n",
      "                                       'input\\n'\n",
      "                                       'dependent key, value, and query '\n",
      "                                       'matrices:\\n'\n",
      "                                       'Q = 𝑥WQ, K = 𝑥WK, V = 𝑥WV, (1)\\n'\n",
      "                                       'y𝑖 =\\n'\n",
      "                                       '𝑖∑︁\\n'\n",
      "                                       '𝑗=1\\n'\n",
      "                                       'exp\\n'\n",
      "                                       '\\x10\\n'\n",
      "                                       'Q⊤\\n'\n",
      "                                       '𝑖 K𝑗/√𝑑in\\n'\n",
      "                                       '\\x11\\n'\n",
      "                                       'V𝑗\\n'\n",
      "                                       'Í𝑖\\n'\n",
      "                                       'ℓ=1 exp\\n'\n",
      "                                       '\\x10\\n'\n",
      "                                       'Q⊤\\n'\n",
      "                                       '𝑖 Kℓ/√𝑑in\\n'\n",
      "                                       '\\x11, (2)\\n'\n",
      "                                       'where WQ,WK,and WV ∈R𝑑in ×𝑑in are '\n",
      "                                       'learnable parameters. Despite the '\n",
      "                                       'power and effectiveness in recall, '\n",
      "                                       'transformers\\n'\n",
      "                                       'need at least 𝑁 ×𝑑 operators to '\n",
      "                                       'calculate the output, resulting in '\n",
      "                                       'larger memory consumption and '\n",
      "                                       'lower-throughput for\\n'\n",
      "                                       'longer sequences.\\n'\n",
      "                                       'Efficient Attentions. To improve the '\n",
      "                                       'memory consumption and throughput of '\n",
      "                                       'softmax attention for longer '\n",
      "                                       'sequences,\\n'\n",
      "                                       'various studies focused on I/O aware '\n",
      "                                       'implementations of attention (Dao '\n",
      "                                       '2024; Dao, D. Fu, et al. 2022), '\n",
      "                                       'designing more\\n'\n",
      "                                       '3\\n'\n",
      "                                       'efficient attention mechanisms by '\n",
      "                                       'sparsifying the attention matrix (B. '\n",
      "                                       'Chen et al. 2021; Choromanski et al. '\n",
      "                                       '2021; Dai et al.\\n'\n",
      "                                       '2019), approximating the softmax '\n",
      "                                       '(Arora et al. 2024), or developing '\n",
      "                                       'kernel-based (linear) attentions '\n",
      "                                       '(Aksenov et al. 2024;\\n'\n",
      "                                       'Kacham, Mirrokni, and P. Zhong 2024; '\n",
      "                                       'Schlag, Irie, and Jürgen Schmidhuber '\n",
      "                                       '2021; S. Yang, B. Wang, Shen, et al. '\n",
      "                                       '2024). In\\n'\n",
      "                                       'this part, we focus on the later, '\n",
      "                                       'i.e., linear attentions, where the '\n",
      "                                       'softmax in standard attention is '\n",
      "                                       'replaced with an alternative\\n'\n",
      "                                       'kernel function 𝜙(.,.), such that '\n",
      "                                       '𝜙(𝑥,𝑦)= 𝜙(𝑥)𝜙(𝑦). Accordingly, the '\n",
      "                                       'attention can be written as:\\n'\n",
      "                                       'y𝑖 =\\n'\n",
      "                                       '𝑖∑︁\\n'\n",
      "                                       '𝑗=1\\n'\n",
      "                                       '𝜙(𝑄⊤\\n'\n",
      "                                       '𝑖 𝐾𝑗)\\n'\n",
      "                                       'Í𝑖\\n'\n",
      "                                       'ℓ=1 𝜙(𝑄⊤\\n'\n",
      "                                       '𝑖 𝐾ℓ)\\n'\n",
      "                                       '𝑉𝑗 =\\n'\n",
      "                                       '𝑖∑︁\\n'\n",
      "                                       '𝑗=1\\n'\n",
      "                                       '𝜙(𝑄𝑖)⊤𝜙(𝐾𝑗)\\n'\n",
      "                                       'Í𝑖\\n'\n",
      "                                       'ℓ=1 𝜙(𝑄𝑖)⊤𝜙(𝐾ℓ)\\n'\n",
      "                                       '𝑉𝑗 =\\n'\n",
      "                                       '𝜙(𝑄𝑖)⊤Í𝑖\\n'\n",
      "                                       '𝑗=1 𝜙(𝐾𝑗)𝑉𝑗\\n'\n",
      "                                       '𝜙(𝑄𝑖)⊤Í𝑖\\n'\n",
      "                                       'ℓ=1 𝜙(𝐾ℓ)\\n'\n",
      "                                       ', (3)\\n'\n",
      "                                       'resulting in a higher-throughput as '\n",
      "                                       'terms Í𝑖\\n'\n",
      "                                       '𝑗=1 𝜙(𝐾𝑗)and Í𝑖\\n'\n",
      "                                       'ℓ=1 𝜙(𝐾ℓ)are re-using in each step. '\n",
      "                                       'When choosing the kernel\\n'\n",
      "                                       'as identity matrix (Yutao Sun et al. '\n",
      "                                       '2023), the above formulation can also '\n",
      "                                       'be written in a recurrent format:\\n'\n",
      "                                       'M𝑡 = M𝑡−1 +𝐾⊤\\n'\n",
      "                                       '𝑡 𝑉𝑡 , (4)\\n'\n",
      "                                       'y𝑡 = 𝑄𝑡M𝑡 , (5)\\n'\n",
      "                                       'which allows efficient inference for '\n",
      "                                       'linear attentions.\\n'\n",
      "                                       'Modern Linear Models and Their Memory '\n",
      "                                       'Perspective. As discussed earlier, one '\n",
      "                                       'can define learning as a process for\\n'\n",
      "                                       'acquiring effective and useful memory. '\n",
      "                                       'Building upon this, one can see the '\n",
      "                                       'hidden state of Recurrent Neural '\n",
      "                                       'Networks\\n'\n",
      "                                       '(RNNs) as a memory unit, which the '\n",
      "                                       'model aims to compress the information '\n",
      "                                       'into. Accordingly, in a general form '\n",
      "                                       'of\\n'\n",
      "                                       'recurrent neural network, the hidden '\n",
      "                                       'state can be treated as a memory unit '\n",
      "                                       'and the recurrence process can be '\n",
      "                                       'split into the\\n'\n",
      "                                       'read and write operations in the '\n",
      "                                       'memory unit. That is, we let 𝑥 ∈R𝑁×𝑑in '\n",
      "                                       'be the input, M∈ R𝑑 is the memory '\n",
      "                                       'unit, and\\n'\n",
      "                                       'y ∈R𝑑in is the output, then the '\n",
      "                                       'general form of the recurrent neural '\n",
      "                                       'network is defined as:\\n'\n",
      "                                       'M𝑡 = 𝑓(M𝑡−1,𝑥𝑡), Write Operation (6)\\n'\n",
      "                                       'y𝑡 = 𝑔(M𝑡,𝑥𝑡), Read Operation (7)\\n'\n",
      "                                       'where 𝑓(.,.)is the read and 𝑔(.,.)is '\n",
      "                                       'the write corresponding functions. '\n",
      "                                       'Note that here the subscript of M𝑡 '\n",
      "                                       'shows the state\\n'\n",
      "                                       'of the memory at time 𝑡.\\n'\n",
      "                                       'In this perspective, the recurrence '\n",
      "                                       'formula of linear Transformers (see '\n",
      "                                       'Equation 4) is equivalent to '\n",
      "                                       'additively compress\\n'\n",
      "                                       'and write keys and values, (𝐾𝑡,𝑉𝑡), '\n",
      "                                       'into a matrix-valued memory unit M𝑡. '\n",
      "                                       'Therefore, when dealing with long '\n",
      "                                       'context\\n'\n",
      "                                       'data, this additive nature of the '\n",
      "                                       'process results in memory overflow, '\n",
      "                                       'significantly damaging the performance '\n",
      "                                       'of the model.\\n'\n",
      "                                       'To address this, studies have focused '\n",
      "                                       'on two promising directions: (1) '\n",
      "                                       'Adding forget mechanism: several '\n",
      "                                       'studies have\\n'\n",
      "                                       'presented adaptive (data-dependent) '\n",
      "                                       'forgetting gate mechanisms for linear '\n",
      "                                       'models, where it can erase the memory '\n",
      "                                       'when it\\n'\n",
      "                                       'is needed. As examples of such models, '\n",
      "                                       'we refer to GLA (S. Yang, B. Wang, '\n",
      "                                       'Shen, et al. 2024), LRU (Orvieto et '\n",
      "                                       'al. 2023),\\n'\n",
      "                                       'Griffin (De et al. 2024), xLSTM (Beck '\n",
      "                                       'et al. 2024), and Mamba2 (Dao and Gu '\n",
      "                                       '2024), which the later is also '\n",
      "                                       'connected to the\\n'\n",
      "                                       'discretized version of traditional '\n",
      "                                       'state space models (Gu and Dao '\n",
      "                                       '2024).(2) Improving the write '\n",
      "                                       'operation: To overcome the\\n'\n",
      "                                       'additive nature of memory write '\n",
      "                                       'operation in traditional recurrent '\n",
      "                                       'models, Widrow and Hoff (1988) '\n",
      "                                       'presented Delta Rule,\\n'\n",
      "                                       'in which before adding a memory (i.e., '\n",
      "                                       'a pair of key and value), the model '\n",
      "                                       'first removes its past value. To '\n",
      "                                       'enhance the\\n'\n",
      "                                       'parallelizable training and scaling, '\n",
      "                                       'S. Yang, B. Wang, Yu Zhang, et al. '\n",
      "                                       '(2024) present a fast paralellizable '\n",
      "                                       'algorithm. Finally,\\n'\n",
      "                                       'very recently, S. Yang, Kautz, and '\n",
      "                                       'Hatamizadeh (2024) improved the '\n",
      "                                       'DeltaNets by adding a forget gate.\\n'\n",
      "                                       'Memory Modules. Memory has always been '\n",
      "                                       'one of the core parts of the neural '\n",
      "                                       'network designs (Graves, Wayne,\\n'\n",
      "                                       'and Danihelka 2014; JH Schmidhuber '\n",
      "                                       '1992; Jürgen Schmidhuber and '\n",
      "                                       'Hochreiter 1997; J. Zhang et al. '\n",
      "                                       '2024). The idea of\\n'\n",
      "                                       'seeing linear layers as the key-value '\n",
      "                                       '(associative) memory system backs to '\n",
      "                                       'fast weight programs, in which dynamic '\n",
      "                                       'fast\\n'\n",
      "                                       'programs are incorporated into '\n",
      "                                       'recurrent neural networks to serve as '\n",
      "                                       'writable memory (JH Schmidhuber 1992). '\n",
      "                                       'The two\\n'\n",
      "                                       'learning rules of Hebbian (Hebb 2005) '\n",
      "                                       'and delta (Prados and Kak 1989) are '\n",
      "                                       'the most popular learning rules for '\n",
      "                                       'fast weight\\n'\n",
      "                                       'programs, which have been extensively '\n",
      "                                       'explored in various studies (Irie, '\n",
      "                                       'Schlag, et al. 2021; Munkhdalai, '\n",
      "                                       'Sordoni, et al.\\n'\n",
      "                                       '2019; Munkhdalai and H. Yu 2017; '\n",
      "                                       'Schlag, Irie, and Jürgen Schmidhuber '\n",
      "                                       '2021; JH Schmidhuber 1992; S. Yang, '\n",
      "                                       'Kautz, and\\n'\n",
      "                                       'Hatamizadeh 2024; S. Yang, B. Wang, Yu '\n",
      "                                       'Zhang, et al. 2024). All these models, '\n",
      "                                       'however, are based on momentary '\n",
      "                                       'surprise,\\n'\n",
      "                                       'missing the token flow in the '\n",
      "                                       'sequences (see Section 3.1), and most '\n",
      "                                       'of them lacks a forgetting gate, '\n",
      "                                       'resulting in a poor\\n'\n",
      "                                       'memory management.\\n'\n",
      "                                       'We further discuss the connection of '\n",
      "                                       'our architectures with recent models '\n",
      "                                       'in Appendix C. Additional related work '\n",
      "                                       'are\\n'\n",
      "                                       'discussed in Appendix A.\\n'\n",
      "                                       '4\\n'\n",
      "                                       '3 Learning to Memorize at Test Time\\n'\n",
      "                                       'T\\n'\n",
      "                                       'o overcome the lack of long-term '\n",
      "                                       'memory and to enable the model to '\n",
      "                                       'learn, forget, and retrieve '\n",
      "                                       'information, in\\n'\n",
      "                                       'this section, we present a neural '\n",
      "                                       'long-term memory module, which is a '\n",
      "                                       'meta models that learns to memorize '\n",
      "                                       'at\\n'\n",
      "                                       'test time. In Section 3.1, we first '\n",
      "                                       'discuss the motivation and the design '\n",
      "                                       'of the neural memory. In Section 3.2, '\n",
      "                                       'we\\n'\n",
      "                                       'discuss how our architecture design '\n",
      "                                       'can benefit from a fast and '\n",
      "                                       'parallelizable training. Finally, in '\n",
      "                                       'Section 3.3, we augment\\n'\n",
      "                                       'our architecture using persistent '\n",
      "                                       'memory module, in which we use '\n",
      "                                       'learnable but data-independent '\n",
      "                                       'parameters to learn\\n'\n",
      "                                       'meta information about the task.\\n'\n",
      "                                       '3.1 Long-term Memory\\n'\n",
      "                                       'To design a neural long-term memory '\n",
      "                                       'module, we need a model that can '\n",
      "                                       'encode the abstraction of the past '\n",
      "                                       'history into its\\n'\n",
      "                                       'parameters. An example of this can be '\n",
      "                                       'LLMs that are shown to be memorizing '\n",
      "                                       'their training data (Leybzon and '\n",
      "                                       'Kervadec\\n'\n",
      "                                       '2024; Schwarzschild et al. 2024; Staab '\n",
      "                                       'et al. 2024). Therefore, a simple idea '\n",
      "                                       'is to train a neural network and '\n",
      "                                       'expect it to\\n'\n",
      "                                       'memorize its training data. '\n",
      "                                       'Memorization, however, has almost '\n",
      "                                       'always been known as an undesirable '\n",
      "                                       'phenomena in\\n'\n",
      "                                       'neural networks as it limits the model '\n",
      "                                       'generalization (Bayat et al. 2024), '\n",
      "                                       'causes privacy concerns (Staab et al. '\n",
      "                                       '2024), and\\n'\n",
      "                                       'so results in poor performance at test '\n",
      "                                       'time. Moreover, the memorization of '\n",
      "                                       'the training data might not be helpful '\n",
      "                                       'at test\\n'\n",
      "                                       'time, in which the data might be '\n",
      "                                       'out-of-distribution. We argue that, we '\n",
      "                                       'need an online meta-model that learns '\n",
      "                                       'how to\\n'\n",
      "                                       'memorize/forget the data at test time. '\n",
      "                                       'In this setup, the model is learning a '\n",
      "                                       'function that is capable of '\n",
      "                                       'memorization, but it\\n'\n",
      "                                       'is not overfitting to the training '\n",
      "                                       'data, resulting in a better '\n",
      "                                       'generalization at test time.\\n'\n",
      "                                       'Learning Process and Surprise Metric. '\n",
      "                                       'The key idea to train a long-term '\n",
      "                                       'memory is to treat its training as an '\n",
      "                                       'online\\n'\n",
      "                                       'learning problem, in which we aim to '\n",
      "                                       'compress the past information 𝑥1,...,𝑥 '\n",
      "                                       '𝑡−1 into the parameters of our '\n",
      "                                       'long-term\\n'\n",
      "                                       'neural memory module M𝑡. As discussed '\n",
      "                                       'earlier, an event that violates the '\n",
      "                                       'expectations (i.e., is surprising) is '\n",
      "                                       'more\\n'\n",
      "                                       'memorable for humans (Mandler 2014). '\n",
      "                                       'Inspired by this, a simple definition '\n",
      "                                       'of surprise for a model can be its '\n",
      "                                       'gradient with\\n'\n",
      "                                       'respect to the input. The larger the '\n",
      "                                       'gradient is, the more different the '\n",
      "                                       'input data is from the past data. '\n",
      "                                       'Accordingly, using\\n'\n",
      "                                       'this surprise score, we can update the '\n",
      "                                       'memory as:\\n'\n",
      "                                       'M𝑡 = M𝑡−1 −𝜃𝑡 ∇ℓ(M𝑡−1; 𝑥𝑡)|           '\n",
      "                                       '{z           }\\n'\n",
      "                                       'Surprise\\n'\n",
      "                                       '. (8)\\n'\n",
      "                                       'This surprise metric, however, can '\n",
      "                                       'result in missing important '\n",
      "                                       'information that comes after a big '\n",
      "                                       'surprising moment.\\n'\n",
      "                                       'That is, the gradient can become '\n",
      "                                       'extremely small after several '\n",
      "                                       'surprising steps, leading to stocking '\n",
      "                                       'in a flat area (i.e., local\\n'\n",
      "                                       'minima), and missing information about '\n",
      "                                       'some parts of the sequence. From the '\n",
      "                                       'human memory perspective, an event '\n",
      "                                       'might\\n'\n",
      "                                       'not consistently surprise us through a '\n",
      "                                       'long-period of time although it is '\n",
      "                                       'memorable. The reason is that the '\n",
      "                                       'initial moment\\n'\n",
      "                                       'is surprising enough to get our '\n",
      "                                       'attention through a long time frame, '\n",
      "                                       'leading to memorizing the entire time '\n",
      "                                       'frame. To\\n'\n",
      "                                       'improve the above surprise metric '\n",
      "                                       '(Equation 8), we break the surprise '\n",
      "                                       'metric into (1) past surprise , which '\n",
      "                                       'measures the\\n'\n",
      "                                       'surprise amount of a very recent past; '\n",
      "                                       'and (2) momentary surprise , which '\n",
      "                                       'measures the surprise of incoming '\n",
      "                                       'data:\\n'\n",
      "                                       'M𝑡 = M𝑡−1 +𝑆𝑡, (9)\\n'\n",
      "                                       '𝑆𝑡 = 𝜂𝑡 𝑆𝑡−1\\n'\n",
      "                                       '|{z}\\n'\n",
      "                                       'Past Surprise\\n'\n",
      "                                       '−𝜃𝑡 ∇ℓ(𝑀𝑡−1; 𝑥𝑡)|          {z          '\n",
      "                                       '}\\n'\n",
      "                                       'Momentary Surprise\\n'\n",
      "                                       '. (10)\\n'\n",
      "                                       'Interestingly, this formulation is '\n",
      "                                       'similar to gradient descent with '\n",
      "                                       'momentum, where𝑆𝑡 is the momentum '\n",
      "                                       'element. Therefore,\\n'\n",
      "                                       'the momentum here act as a memory of '\n",
      "                                       'surprise across time (sequence '\n",
      "                                       'length). In this formulation, the term '\n",
      "                                       '𝜂𝑡 is a\\n'\n",
      "                                       'data-dependent surprise decay (a '\n",
      "                                       'function of 𝑥𝑡), controlling how '\n",
      "                                       'surprise decays over time, and the '\n",
      "                                       'term 𝜃𝑡 is controlling\\n'\n",
      "                                       'how much of momentary surprise should '\n",
      "                                       'be incorporated into the final '\n",
      "                                       'surprise metric in a data-dependent '\n",
      "                                       'manner. This\\n'\n",
      "                                       'data-dependency is particularly '\n",
      "                                       'important in this design: While '\n",
      "                                       'surprise of previous tokens might be '\n",
      "                                       'needed to affect\\n'\n",
      "                                       'the surprise of the next token, it is '\n",
      "                                       'mostly valid if all tokens are '\n",
      "                                       'relevant and are in the same context. '\n",
      "                                       'Accordingly, a\\n'\n",
      "                                       'data-dependent 𝜂can control if memory '\n",
      "                                       'needs to: (1) ignore the last surprise '\n",
      "                                       'by setting 𝜂𝑡 →0 (possibly due to the '\n",
      "                                       'change\\n'\n",
      "                                       'of context), or (2) fully incorporate '\n",
      "                                       'the last surprise by setting 𝜂𝑡 →1 '\n",
      "                                       '(possibly as the token is highly '\n",
      "                                       'relevant to its recent\\n'\n",
      "                                       'past tokens).\\n'\n",
      "                                       'Objective. Our above surprise metric '\n",
      "                                       'is based on a loss function ℓ(.; .), '\n",
      "                                       'which is the objective that our memory '\n",
      "                                       'is learning\\n'\n",
      "                                       'to act as it at test time. That is, '\n",
      "                                       'our memory module is a meta model that '\n",
      "                                       'learns a function based on the loss '\n",
      "                                       'function ℓ(.; .).\\n'\n",
      "                                       '5\\n'\n",
      "                                       'In this work, we focus on associative '\n",
      "                                       'memory , in which we aim to store the '\n",
      "                                       'past data as the pairs of keys and '\n",
      "                                       'values. Given\\n'\n",
      "                                       '𝑥𝑡, similar to Transformers (Vaswani '\n",
      "                                       'et al. 2017), we use two linear layers '\n",
      "                                       'to project𝑥𝑡 into a key and value:\\n'\n",
      "                                       'k𝑡 = 𝑥𝑡𝑊𝐾, v𝑡 = 𝑥𝑡𝑊𝑉, (11)\\n'\n",
      "                                       'where 𝑊𝐾 and 𝑊𝑉 ∈R𝑑in ×𝑑in . Next, we '\n",
      "                                       'expect our memory module to learn the '\n",
      "                                       'associations between keys and values. '\n",
      "                                       'To\\n'\n",
      "                                       'this end, we define the loss as '\n",
      "                                       'follows:\\n'\n",
      "                                       'ℓ(M𝑡−1; 𝑥𝑡)= ∥M𝑡−1 (k𝑡)−v𝑡∥2\\n'\n",
      "                                       '2 (12)\\n'\n",
      "                                       'By optimizing the above loss function '\n",
      "                                       'in the inner-loop of our meta model '\n",
      "                                       '(memory), the model learns how to '\n",
      "                                       'memorize\\n'\n",
      "                                       'the mapping between keys and values at '\n",
      "                                       'test time. Note that, similar to '\n",
      "                                       'meta-learning models (Nichol 2018; '\n",
      "                                       'Zintgraf et al.\\n'\n",
      "                                       '2019), training of the memory is in '\n",
      "                                       'the inner-loop, and so parameters 𝑊𝐾 '\n",
      "                                       'and 𝑊𝑉 are hyperparameters in the '\n",
      "                                       'above loss\\n'\n",
      "                                       'function. Accordingly, in the inner '\n",
      "                                       'loop, we optimize M’s weights, while '\n",
      "                                       'in the outer-loop, we optimize other '\n",
      "                                       'parameters\\n'\n",
      "                                       'of the entire architecture.\\n'\n",
      "                                       'Forgetting Mechanism. When dealing '\n",
      "                                       'with very large sequences (e.g., '\n",
      "                                       'millions of tokens), it is crucial to '\n",
      "                                       'manage which\\n'\n",
      "                                       'past information should be '\n",
      "                                       'forgotten–even with a deep or a very '\n",
      "                                       'large matrix-valued memory. To this '\n",
      "                                       'end, we use an\\n'\n",
      "                                       'adaptive forgetting mechanism that '\n",
      "                                       'allows the memory to forget the '\n",
      "                                       'information that is not needed '\n",
      "                                       'anymore, resulting in\\n'\n",
      "                                       'better managing the memory’s limited '\n",
      "                                       'capacity. That is, given the next '\n",
      "                                       'token 𝑥𝑡, we modify the update rule '\n",
      "                                       'as:\\n'\n",
      "                                       'M𝑡 = (1 −𝛼𝑡)M𝑡−1 +𝑆𝑡, (13)\\n'\n",
      "                                       '𝑆𝑡 = 𝜂𝑡𝑆𝑡−1 −𝜃𝑡 ∇ℓ(𝑀𝑡−1; 𝑥𝑡), (14)\\n'\n",
      "                                       'where 𝛼𝑡 ∈[0,1]is the gating mechanism '\n",
      "                                       'that flexibly controls the memory; '\n",
      "                                       'i.e., decides how much information '\n",
      "                                       'should be\\n'\n",
      "                                       'forgotten. For example, it can update '\n",
      "                                       'the memory without affecting the past '\n",
      "                                       'abstraction by letting 𝛼𝑡 →0, and can '\n",
      "                                       'clear\\n'\n",
      "                                       'the entire memory by letting 𝛼𝑡 →1. '\n",
      "                                       'Later in this section, we show that '\n",
      "                                       'this weight decay mechanism is closely '\n",
      "                                       'related to\\n'\n",
      "                                       'the gating mechanism in modern RNNs '\n",
      "                                       '(Dao and Gu 2024; Orvieto et al. '\n",
      "                                       '2023).\\n'\n",
      "                                       'Memory Architecture. In this paper, we '\n",
      "                                       'focus on simple MLPs with 𝐿M ≥1 layers '\n",
      "                                       'as the architecture of our long-term\\n'\n",
      "                                       'memory. The main reason behind this '\n",
      "                                       'choice is that we want to focus on '\n",
      "                                       'better motivating the design of the '\n",
      "                                       'long-term\\n'\n",
      "                                       'memory and ways that it can be '\n",
      "                                       'incorporated into an architecture. '\n",
      "                                       'However, our formulation and '\n",
      "                                       'architectural design\\n'\n",
      "                                       'opens a new research direction to '\n",
      "                                       'design neural architectures that are '\n",
      "                                       'more effective and efficient in '\n",
      "                                       'memorization of data.\\n'\n",
      "                                       'Recently, there has been a promising '\n",
      "                                       'line of work to design such '\n",
      "                                       'architectures (Berges et al. 2024; '\n",
      "                                       'Cetin et al. 2024; J. Zhang\\n'\n",
      "                                       'et al. 2024), which incorporating them '\n",
      "                                       'into our framework (i.e., replacing '\n",
      "                                       'simple MLPs with such architectures) '\n",
      "                                       'can be an\\n'\n",
      "                                       'interesting future work.\\n'\n",
      "                                       'When using vector-valued or '\n",
      "                                       'matrix-valued memory (De et al. 2024; '\n",
      "                                       'Orvieto et al. 2023; S. Yang, B. Wang, '\n",
      "                                       'Shen, et\\n'\n",
      "                                       'al. 2024), the memory module is '\n",
      "                                       'compressing the past data and fit it '\n",
      "                                       'into a line. That is, from the meta '\n",
      "                                       'learning or\\n'\n",
      "                                       'online learning perspective (Yu Sun et '\n",
      "                                       'al. 2024), using a matrix-valued '\n",
      "                                       'memory M = 𝑊 ∈R𝑑in ×𝑑in is equivalent '\n",
      "                                       'to\\n'\n",
      "                                       'optimize ℓ(𝑊𝑡−1; 𝑥𝑡)= ∥𝑊𝑡−1k𝑡 −v𝑡∥2\\n'\n",
      "                                       '2, which is an online linear '\n",
      "                                       'regression objective and so the '\n",
      "                                       'optimal solution assumes\\n'\n",
      "                                       'the underlying dependency of '\n",
      "                                       'historical data is linear. On the '\n",
      "                                       'other hand, we argue that deep memory '\n",
      "                                       'modules (i.e.,\\n'\n",
      "                                       '𝐿M ≥2) . Aligning with the theoretical '\n",
      "                                       'results that MLPs with at least two '\n",
      "                                       'layers are strictly more expressive '\n",
      "                                       'than linear\\n'\n",
      "                                       'models (Hornik, Stinchcombe, and White '\n",
      "                                       '1989), in Section 5.5, we show that '\n",
      "                                       'deep memory modules are more effective '\n",
      "                                       'in\\n'\n",
      "                                       'practice.\\n'\n",
      "                                       'Retrieving a Memory. In the above, we '\n",
      "                                       'discuss how one can design and train a '\n",
      "                                       'long-term memory module that learns '\n",
      "                                       'to\\n'\n",
      "                                       'memorize at test time. A key remaining '\n",
      "                                       'question is: How one can retrieve '\n",
      "                                       'information from the memory? We simply '\n",
      "                                       'use the\\n'\n",
      "                                       'forward pass without weight update '\n",
      "                                       '(i.e., inference) to retrieve a memory '\n",
      "                                       'correspond to a query. Formally, given '\n",
      "                                       'an input\\n'\n",
      "                                       '𝑥𝑡, we use a linear layer 𝑊𝑄 to '\n",
      "                                       'project the input, i.e., q𝑡 = 𝑥𝑡𝑊𝑄 and '\n",
      "                                       'retrieve the corresponding (or useful) '\n",
      "                                       'information\\n'\n",
      "                                       'from the memory 𝑦𝑡 by:\\n'\n",
      "                                       '𝑦𝑡 = M∗(q𝑡). (15)\\n'\n",
      "                                       '6\\n'\n",
      "                                       'Figure 1: The illustration of how the '\n",
      "                                       'training of neural memory can be done '\n",
      "                                       'in parallel and using matmuls.\\n'\n",
      "                                       '3.2 How to Parallelize the Long-term '\n",
      "                                       'Memory Training\\n'\n",
      "                                       'As discussed above, the design of our '\n",
      "                                       'long-term memory module is equivalent '\n",
      "                                       'to training a meta model by '\n",
      "                                       'optimizing\\n'\n",
      "                                       'associative memory loss function '\n",
      "                                       'ℓ(M𝑡−1; 𝑥𝑡)= ∥M𝑡−1 (k𝑡)−v𝑡∥2\\n'\n",
      "                                       '2 using gradient descent with momentum '\n",
      "                                       'and weight\\n'\n",
      "                                       'decay. Therefore, in theory, the '\n",
      "                                       'training of long-term memory module '\n",
      "                                       'requires O(𝑁)FLOPs, where 𝑁 is the '\n",
      "                                       'sequence\\n'\n",
      "                                       'length. However, in practice, we need '\n",
      "                                       'to parallelize the training process '\n",
      "                                       'and to fully take advantage of '\n",
      "                                       'hardware accelerators\\n'\n",
      "                                       '(e.g., TPUs, GPUs), we need to '\n",
      "                                       'tensorize the process and use more '\n",
      "                                       'matmuls.\\n'\n",
      "                                       'Next, we show that calculating the '\n",
      "                                       'weights in the inner loop with '\n",
      "                                       'mini-batch gradient descent, '\n",
      "                                       'data-dependent learning\\n'\n",
      "                                       'rate, and weight decay can be '\n",
      "                                       'reformulated so that it uses only '\n",
      "                                       'matmuls and sum. We build upon the '\n",
      "                                       'work of Yu Sun et al.\\n'\n",
      "                                       '(2024) that shows forward pass of a '\n",
      "                                       'model optimizing with the mini-batch '\n",
      "                                       'gradient descent (with constant '\n",
      "                                       'learning rate)\\n'\n",
      "                                       'can be calculated using matmuls. We '\n",
      "                                       'can split the sequence into chunks of '\n",
      "                                       'size 𝑏 ≥1, and write the mini-batch '\n",
      "                                       'gradient\\n'\n",
      "                                       'descent as:\\n'\n",
      "                                       'M𝑡 = (1 −𝛼𝑡)M𝑡−1 −𝜃𝑡∇ℓ(M𝑡−1; 𝑥𝑡)= 𝛽𝑡M0 '\n",
      "                                       '−\\n'\n",
      "                                       '𝑡∑︁\\n'\n",
      "                                       '𝑖=1\\n'\n",
      "                                       '𝜃𝑖\\n'\n",
      "                                       '𝛽𝑡\\n'\n",
      "                                       '𝛽𝑖\\n'\n",
      "                                       '∇ℓ(M𝑡′; 𝑥𝑖), (16)\\n'\n",
      "                                       'where 𝑡′= 𝑡 −mod(𝑡,𝑏), and 𝛽𝑖 = Î𝑖\\n'\n",
      "                                       '𝑗=1 (1 −𝛼𝑗). For the sake of '\n",
      "                                       'simplicity, we focus on the first '\n",
      "                                       'chunk, i.e., 𝑡 = 𝑏and so\\n'\n",
      "                                       '𝑡′= 0. Also, we explain the process '\n",
      "                                       'for the case that M𝑡 = 𝑊𝑡 is linear. '\n",
      "                                       'The process for MLPs with 𝑁𝑝 ≥2 is '\n",
      "                                       'similar. Using\\n'\n",
      "                                       'our loss function, we have:\\n'\n",
      "                                       '∇ℓ(𝑊0; 𝑥𝑡)= (𝑊0𝑥𝑡 −𝑥𝑡)𝑥⊤\\n'\n",
      "                                       '𝑡 ⇒\\n'\n",
      "                                       '𝑏∑︁\\n'\n",
      "                                       '𝑖=1\\n'\n",
      "                                       '𝜃𝑖\\n'\n",
      "                                       '𝛽𝑏\\n'\n",
      "                                       '𝛽𝑖\\n'\n",
      "                                       '∇ℓ(𝑊0; 𝑥𝑖)= Θ𝑏B𝑏(𝑊0𝑋 −𝑋)𝑋⊤, (17)\\n'\n",
      "                                       'where Θ𝑏 = diag \\x00\\x02𝜃1 𝜃2 ... 𝜃 𝑏\\n'\n",
      "                                       '\\x03\\x01 and B𝑏 is defined analogously '\n",
      "                                       'on 𝛽𝑏\\n'\n",
      "                                       '𝛽𝑖\\n'\n",
      "                                       's. Note that, we do not need to store '\n",
      "                                       'allΘ𝑘𝑏 and\\n'\n",
      "                                       'B𝑘𝑏 for 𝑘 = 1,...,𝑁 /𝑏, instead, we '\n",
      "                                       'store these matrices for each chunk, '\n",
      "                                       'resulting in using less memory. Next, '\n",
      "                                       'we extend\\n'\n",
      "                                       'this representation so we can also '\n",
      "                                       'incorporate the momentum term. In a '\n",
      "                                       'chunk wise gradient descent with '\n",
      "                                       'momentum, if\\n'\n",
      "                                       'we look at the momentum term, we '\n",
      "                                       'have:\\n'\n",
      "                                       '𝑆𝑡 = 𝜂𝑡𝑆𝑡−1 −𝜃𝑡 𝑢𝑡, (18)\\n'\n",
      "                                       'where 𝑢𝑡 = ∇ℓ(𝑀𝑡′; 𝑥𝑡). Note that, we '\n",
      "                                       'can compute all 𝑢𝑡 at the same time, '\n",
      "                                       'and so Equation 18 is a linear '\n",
      "                                       'recurrence\\n'\n",
      "                                       'with 𝑢𝑡 as an input, 𝑆𝑡 as the hidden '\n",
      "                                       'state, and 𝜂𝑡 as input-dependent '\n",
      "                                       'transition value. Accordingly, we can '\n",
      "                                       'use parallel\\n'\n",
      "                                       'associative scan (J. T. Smith, '\n",
      "                                       'Warrington, and Linderman 2023) to '\n",
      "                                       'calculate𝑆𝑡s in this chunk.\\n'\n",
      "                                       'Parameters as the Function of Chunks. '\n",
      "                                       'Instead of making parameters '\n",
      "                                       'like𝛼𝑡,𝜃𝑡, and 𝜂𝑡 input-dependent '\n",
      "                                       '(i.e., a function\\n'\n",
      "                                       'of token 𝑥𝑡), we can make them '\n",
      "                                       'functions of their chunk. Despite '\n",
      "                                       'losing expressive power, this '\n",
      "                                       'formulation can help to\\n'\n",
      "                                       'make the training even faster. In this '\n",
      "                                       'case, we are using the same value for '\n",
      "                                       'each of 𝛼, 𝜃, and 𝜂in each chunk. '\n",
      "                                       'Accordingly,\\n'\n",
      "                                       'in Equation 17, we can store Θ using a '\n",
      "                                       'single scaler. Similarly we can make '\n",
      "                                       'Equation 18 faster. That is, when 𝜂and '\n",
      "                                       '𝜃 are\\n'\n",
      "                                       'learnable but time-invariant inside '\n",
      "                                       'each chunk, this equation becomes a '\n",
      "                                       'linear time-invariant system (LTI), '\n",
      "                                       'which can be\\n'\n",
      "                                       'computed by a global convolution (Gu, '\n",
      "                                       'Goel, and Re 2022). In our '\n",
      "                                       'experiments, we make these parameters '\n",
      "                                       'as the functions\\n'\n",
      "                                       'of tokens. However, such '\n",
      "                                       'simplifications (i.e., as the function '\n",
      "                                       'of chunks) can be the interest of '\n",
      "                                       'future work to training\\n'\n",
      "                                       'larger models in more efficient '\n",
      "                                       'manner.\\n'\n",
      "                                       '7\\n'\n",
      "                                       'Figure 2: Memory as a Context (MAC) '\n",
      "                                       'Architecture. This architecture '\n",
      "                                       'includes three branches of (1) core, '\n",
      "                                       '(2) contextual\\n'\n",
      "                                       '(long-term) memory, and (3) persistent '\n",
      "                                       'memory. The core branch concatenates '\n",
      "                                       'the corresponding long-term and '\n",
      "                                       'persistent\\n'\n",
      "                                       'memories with the input sequence. '\n",
      "                                       'Next, attention performs on the '\n",
      "                                       'sequence and decides what part of the '\n",
      "                                       'information\\n'\n",
      "                                       'should store in the long-term memory. '\n",
      "                                       'At the test time, parameters '\n",
      "                                       'corresponds to contextual memory are '\n",
      "                                       'still learning,\\n'\n",
      "                                       'parameters corresponds to the core '\n",
      "                                       'branch are responsible for in-context '\n",
      "                                       'learning, and parameters of persistent '\n",
      "                                       'memory\\n'\n",
      "                                       'are responsible to store the knowledge '\n",
      "                                       'about tasks and so are fixed.\\n'\n",
      "                                       '3.3 Persistent Memory\\n'\n",
      "                                       'Our long-term memory can also be seen '\n",
      "                                       'as a contextual memory, meaning that '\n",
      "                                       'the output is fully depend on the '\n",
      "                                       'context.\\n'\n",
      "                                       'Therefore, in addition to our '\n",
      "                                       'long-term memory, we also use a set of '\n",
      "                                       'learnable but input-independent '\n",
      "                                       'parameters to act as\\n'\n",
      "                                       'task-related memory. This type of '\n",
      "                                       'memory has been referred to as '\n",
      "                                       'persistent or meta-memory in the '\n",
      "                                       'literature (X. Dong\\n'\n",
      "                                       'et al. 2024; Sukhbaatar, Grave, et al. '\n",
      "                                       '2019). Given 𝑁𝑝 ≥1, we use learnable '\n",
      "                                       'parameters 𝑃 =\\n'\n",
      "                                       '\\x02𝑝1 𝑝2 ... 𝑝 𝑁𝑝\\n'\n",
      "                                       '\\x03\\n'\n",
      "                                       'and\\n'\n",
      "                                       'append it to the start of our '\n",
      "                                       'sequence: i.e., given a context window '\n",
      "                                       'size of 𝑁, we modify the input as:\\n'\n",
      "                                       '𝑥new =\\n'\n",
      "                                       '\\x02𝑝1 𝑝2 ... 𝑝 𝑁𝑝\\n'\n",
      "                                       '\\x03\\n'\n",
      "                                       '|| 𝑥, (19)\\n'\n",
      "                                       'where ||is concatenation. Next, we '\n",
      "                                       'discuss the motivation of persistent '\n",
      "                                       'memory from three perspective:\\n'\n",
      "                                       'Memory Perspective. As discussed '\n",
      "                                       'earlier, our neural long-term memory '\n",
      "                                       'is a contextual memory, in which all '\n",
      "                                       'parameters\\n'\n",
      "                                       'are input-dependent. An effective '\n",
      "                                       'memory system, however, also needs '\n",
      "                                       'input-independent parameters to store '\n",
      "                                       'the\\n'\n",
      "                                       'abstraction of the task knowledge. '\n",
      "                                       'That is, mastering a task requires the '\n",
      "                                       'memorization of the knowledge that how '\n",
      "                                       'the task\\n'\n",
      "                                       'can be done, and these parameters are '\n",
      "                                       'responsible for storing such '\n",
      "                                       'knowledge.\\n'\n",
      "                                       'Feedforward Network Perspective. In '\n",
      "                                       'the Transformer architectures, there '\n",
      "                                       'are fully connected layers after the '\n",
      "                                       'attention\\n'\n",
      "                                       'module, which are shown to be similar '\n",
      "                                       'to attention weights but with '\n",
      "                                       'data-independent parameters. That is, '\n",
      "                                       'Sukhbaatar,\\n'\n",
      "                                       'Grave, et al. (2019) showed that '\n",
      "                                       'replacing the ReLU in fully connected '\n",
      "                                       'layers with Softmax can results in an '\n",
      "                                       'attention-like\\n'\n",
      "                                       'weights, in which weights are '\n",
      "                                       'data-independent:\\n'\n",
      "                                       '𝐹𝐹𝑁 (𝑥)= 𝑊𝑉 Softmax (𝑊𝐾𝑥). (20)\\n'\n",
      "                                       'In fact, 𝑊𝐾 and 𝑊𝑉 are acting similar '\n",
      "                                       'to 𝐾 and 𝑉 matrices in attention '\n",
      "                                       'module when they are '\n",
      "                                       'input-independent. The\\n'\n",
      "                                       'persistent memory weights are expected '\n",
      "                                       'to have the same functionality, '\n",
      "                                       'meaning that using them in the first '\n",
      "                                       'part of the\\n'\n",
      "                                       'sequence leads to having '\n",
      "                                       'input-independent attention weights '\n",
      "                                       '(Sukhbaatar, Grave, et al. 2019).\\n'\n",
      "                                       'Technical Perspective. Attention with '\n",
      "                                       'causal mask has implicit bias toward '\n",
      "                                       'initial tokens in the sequence, and so '\n",
      "                                       'attention\\n'\n",
      "                                       'weights are almost always highly '\n",
      "                                       'active for initial tokens, resulting '\n",
      "                                       'in performance damage. From the '\n",
      "                                       'technical perspective,\\n'\n",
      "                                       'these learnable parameters at the '\n",
      "                                       'start of the sequence can mitigate '\n",
      "                                       'such effect by redistributing the '\n",
      "                                       'attention weights\\n'\n",
      "                                       'more effectively (Han et al. 2024; '\n",
      "                                       'Xiao et al. 2024).\\n'\n",
      "                                       '8\\n'\n",
      "                                       '(a) Memory as a Context (MAC). We '\n",
      "                                       'segment the sequence\\n'\n",
      "                                       'and use full causal attention in each '\n",
      "                                       'window. Again, the first\\n'\n",
      "                                       '𝑁𝑝 tokens are persistent memory and '\n",
      "                                       'the next 𝑁𝑙 are long-term\\n'\n",
      "                                       'memory tokens\\n'\n",
      "                                       '(b) Memory as Gating (MAG). We use '\n",
      "                                       'sliding window attention\\n'\n",
      "                                       '(SWA) as a short-term memory and our '\n",
      "                                       'neural memory module\\n'\n",
      "                                       'as a long-term memory, combining by a '\n",
      "                                       'gating.\\n'\n",
      "                                       'Figure 3: Attention masks for '\n",
      "                                       'different variants of Titans.\\n'\n",
      "                                       '4 How to Incorporate Memory?\\n'\n",
      "                                       'A\\n'\n",
      "                                       'n important question that remained '\n",
      "                                       'unanswered is: How one can effectively '\n",
      "                                       'and efficiently incorporate the\\n'\n",
      "                                       'designed neural memory into a deep '\n",
      "                                       'learning architecture? As discussed '\n",
      "                                       'earlier, from a memory perspective,\\n'\n",
      "                                       'the pair of K and V matrices in '\n",
      "                                       'transformers can be interpreted as an '\n",
      "                                       'associative memory block. Due to '\n",
      "                                       'their\\n'\n",
      "                                       'accurate modeling of dependencies and '\n",
      "                                       'so their limited context window, we '\n",
      "                                       'interpret them as short-term memory '\n",
      "                                       'modules,\\n'\n",
      "                                       'attending to the current context '\n",
      "                                       'window size. On the other hand, our '\n",
      "                                       'neural memory with the ability to '\n",
      "                                       'continuously\\n'\n",
      "                                       'learn from data and store it in its '\n",
      "                                       'weights can play the role of a a '\n",
      "                                       'long-term memory. In this section, we '\n",
      "                                       'aim to answer\\n'\n",
      "                                       'the above question by proposing three '\n",
      "                                       'different variants of Titans. Later in '\n",
      "                                       'our experiments, we show that each of '\n",
      "                                       'these\\n'\n",
      "                                       'variants has its own '\n",
      "                                       'advantages/disadvantages and also can '\n",
      "                                       'show a trade-off between the '\n",
      "                                       'efficiency and effectiveness in\\n'\n",
      "                                       'very long-contexts.\\n'\n",
      "                                       '4.1 Memory as a Context\\n'\n",
      "                                       'In the first architecture design (see '\n",
      "                                       'Figure 2), we treat the memory as a '\n",
      "                                       'context to the current information. '\n",
      "                                       'That is, given\\n'\n",
      "                                       'a long sequence 𝑥 ∈R𝑁×𝑑in , we first '\n",
      "                                       'chunk the sequence into fixed-size '\n",
      "                                       'segments S(𝑖) for 𝑖 = 1,...,𝑁 /𝐶. '\n",
      "                                       'Given the\\n'\n",
      "                                       'incoming segment S(𝑡), we consider it '\n",
      "                                       'as the current context and its past '\n",
      "                                       'segment as the historical information. '\n",
      "                                       'Therefore,\\n'\n",
      "                                       'let M𝑡−1 be the state of long-term '\n",
      "                                       'memory before segment S(𝑡), we use the '\n",
      "                                       'input context as the query to the '\n",
      "                                       'memory\\n'\n",
      "                                       'M𝑡−1 to retrieve the corresponding '\n",
      "                                       'information from the long-term memory. '\n",
      "                                       'That is, we retrieve the past '\n",
      "                                       'information that\\n'\n",
      "                                       'corresponds to S(𝑡)as:\\n'\n",
      "                                       'ℎ𝑡 = M∗\\n'\n",
      "                                       '𝑡−1 (q𝑡), (21)\\n'\n",
      "                                       'where q𝑡 = S(𝑡)𝑊𝑄. Next, we use this '\n",
      "                                       'historical information along with our '\n",
      "                                       'persistent memory parameters as the '\n",
      "                                       'input\\n'\n",
      "                                       'sequence to the attention module:\\n'\n",
      "                                       '˜S\\n'\n",
      "                                       '(𝑡)\\n'\n",
      "                                       '=\\n'\n",
      "                                       '\\x02𝑝1 𝑝2 ... 𝑝 𝑁𝑝\\n'\n",
      "                                       '\\x03\\n'\n",
      "                                       '|| ℎ𝑡 || S(𝑡), (22)\\n'\n",
      "                                       '𝑦𝑡 = Attn\\n'\n",
      "                                       '\\x10\\n'\n",
      "                                       '˜S\\n'\n",
      "                                       '(𝑡)\\x11\\n'\n",
      "                                       '. (23)\\n'\n",
      "                                       'The structure of the attention map '\n",
      "                                       'over the entire sequence is shown in '\n",
      "                                       'Figure 3a. We then use 𝑦𝑡 to update '\n",
      "                                       'the long-term\\n'\n",
      "                                       'memory module for the next segment and '\n",
      "                                       'the final output:\\n'\n",
      "                                       'M𝑡 = M𝑡−1 (𝑦𝑡), (24)\\n'\n",
      "                                       '𝑜𝑡 = 𝑦𝑡 ⊗M∗\\n'\n",
      "                                       '𝑡 (𝑦𝑡). (25)\\n'\n",
      "                                       'Note that, in the above, we are '\n",
      "                                       'updating the weight of M𝑡−1 through '\n",
      "                                       'forward pass.\\n'\n",
      "                                       'This architecture has two key '\n",
      "                                       'advantages: (1) Attention by having '\n",
      "                                       'both historical and current context, '\n",
      "                                       'has the ability to\\n'\n",
      "                                       'decides whether given the current '\n",
      "                                       'data, the long-term memory information '\n",
      "                                       'is needed. (2) The attention module '\n",
      "                                       'helps\\n'\n",
      "                                       '9\\n'\n",
      "                                       'Figure 4: Memory as a Gate (MAG) '\n",
      "                                       'Architecture. This architecture, '\n",
      "                                       'similarly, has the three branches of '\n",
      "                                       '(1) core, (2)\\n'\n",
      "                                       'contextual memory, and (3) persistent '\n",
      "                                       'memory. It, however, incorporates only '\n",
      "                                       'persistent memory into the context '\n",
      "                                       'and\\n'\n",
      "                                       'combine memory with the core branch '\n",
      "                                       'using a gating mechanism. At test '\n",
      "                                       'time, the behavior is the same as '\n",
      "                                       'Figure 2.\\n'\n",
      "                                       'the long-term memory to store only '\n",
      "                                       'useful information from the current '\n",
      "                                       'context. That is, not all tokens in '\n",
      "                                       'each segment\\n'\n",
      "                                       'are useful and memorizing all of them '\n",
      "                                       'can result in memory overflow. '\n",
      "                                       'Therefore, attention is helping the '\n",
      "                                       'memory to\\n'\n",
      "                                       'understand which information is '\n",
      "                                       'useful, better managing the memory '\n",
      "                                       'capacity. (3) At test time: (i) '\n",
      "                                       'persistent memory\\n'\n",
      "                                       'parameters are fixed as they encodes '\n",
      "                                       'the knowledge about the task, which '\n",
      "                                       'should not be changed; (ii) the '\n",
      "                                       'attention module\\n'\n",
      "                                       'weights are in-context learner; and '\n",
      "                                       '(iii) the long-term memory module is '\n",
      "                                       'still learning (memorizing) the '\n",
      "                                       'information at test\\n'\n",
      "                                       'time. That is, we update the weights '\n",
      "                                       'of the neural memory even at test time '\n",
      "                                       'as weights are encoding the '\n",
      "                                       'abstraction of\\n'\n",
      "                                       'long past.\\n'\n",
      "                                       '4.2 Gated Memory\\n'\n",
      "                                       'In the next variant (see Figure 4), in '\n",
      "                                       'one branch, we directly use the input '\n",
      "                                       'data to update the long-term memory, '\n",
      "                                       'and in the\\n'\n",
      "                                       'second branch, we use a sliding window '\n",
      "                                       'attention (SWA):\\n'\n",
      "                                       '˜𝑥 =\\n'\n",
      "                                       '\\x02𝑝1 𝑝2 ... 𝑝 𝑁𝑝\\n'\n",
      "                                       '\\x03\\n'\n",
      "                                       '|| 𝑥, (26)\\n'\n",
      "                                       '𝑦 = SW-Attn∗(˜𝑥), (27)\\n'\n",
      "                                       '𝑜 = 𝑦⊗M( ˜𝑥), (28)\\n'\n",
      "                                       'where SW-Attn∗is sliding window '\n",
      "                                       'attention with prefix (see Figure 3b). '\n",
      "                                       'Note that, contrary to the previous '\n",
      "                                       'design, we are\\n'\n",
      "                                       'not segmenting the input data. Also, '\n",
      "                                       'we abuse the notation and use M(𝑥)to '\n",
      "                                       'refer to the final output of the '\n",
      "                                       'memory after\\n'\n",
      "                                       'all recursion over the tokens of the '\n",
      "                                       'sequence. In the above equation, ⊗can '\n",
      "                                       'be any non-linear gating. In our '\n",
      "                                       'experiments,\\n'\n",
      "                                       'we normalize the outputs 𝑦and '\n",
      "                                       'M(˜𝑥)using learnable vector-valued '\n",
      "                                       'weights, followed by a non-linearity '\n",
      "                                       '𝜎(.).\\n'\n",
      "                                       'The overall attention mask of this '\n",
      "                                       'design is shown in Figure 3b. In this '\n",
      "                                       'design, sliding window attention is '\n",
      "                                       'act as a precise\\n'\n",
      "                                       'short-term memory, while the neural '\n",
      "                                       'memory module is acting as a fading '\n",
      "                                       'memory for the model. This '\n",
      "                                       'architecture design\\n'\n",
      "                                       'can also be seen as a multi-head '\n",
      "                                       'architecture where the structure of '\n",
      "                                       'heads are different (X. Dong et al. '\n",
      "                                       '2024).\\n'\n",
      "                                       '4.3 Memory as a Layer\\n'\n",
      "                                       'The last variant uses the neural '\n",
      "                                       'Memory As a Layer (MAL) of a deep '\n",
      "                                       'neural network (see Figure 5). This '\n",
      "                                       'architecture\\n'\n",
      "                                       'design is more common in the '\n",
      "                                       'literature, where the hybrid models '\n",
      "                                       'stack recurrent models with full or '\n",
      "                                       'sliding window\\n'\n",
      "                                       'attentions. Given input 𝑥, we have:\\n'\n",
      "                                       '˜𝑥 =\\n'\n",
      "                                       '\\x02𝑝1 𝑝2 ... 𝑝 𝑁𝑝\\n'\n",
      "                                       '\\x03\\n'\n",
      "                                       '|| 𝑥, (29)\\n'\n",
      "                                       '𝑦 = M(˜𝑥), (30)\\n'\n",
      "                                       '𝑜 = SW-Attn (𝑦), (31)\\n'\n",
      "                                       '10\\n'\n",
      "                                       'Figure 5: Memory as a Layer (MAL) '\n",
      "                                       'Architecture. In this architecture, '\n",
      "                                       'the memory layer is responsible to '\n",
      "                                       'compress the\\n'\n",
      "                                       'past and current context before the '\n",
      "                                       'attention module.\\n'\n",
      "                                       'where SW-Attn is sliding window '\n",
      "                                       'attention. The main drawback of this '\n",
      "                                       'design is that the power of the model '\n",
      "                                       'is limited by\\n'\n",
      "                                       'each of the layers and so it cannot '\n",
      "                                       'take advantage of the complementary '\n",
      "                                       'data processing of attention and '\n",
      "                                       'neural memory\\n'\n",
      "                                       'module. In our experiments, for '\n",
      "                                       'evaluating memory in this design, we '\n",
      "                                       'use a similar architecture as H3 (D. '\n",
      "                                       'Y. Fu et al. 2023),\\n'\n",
      "                                       'where we replace the the sequence '\n",
      "                                       'model with our neural memory module '\n",
      "                                       '(LMM).\\n'\n",
      "                                       'Memory Without Attention. Although in '\n",
      "                                       'the above, we discussed MAL as the '\n",
      "                                       'combination of LMMs and attention in\\n'\n",
      "                                       'a sequential manner, one simple '\n",
      "                                       'variant of MAL is to treat LMM as a '\n",
      "                                       'sequence model without any attention. '\n",
      "                                       'From the\\n'\n",
      "                                       'memory perspective, as discussed in '\n",
      "                                       'Section 1, we expect each part of the '\n",
      "                                       'memory system to work independently, '\n",
      "                                       'even if\\n'\n",
      "                                       'other components are disturbed. '\n",
      "                                       'Therefore, a long-term memory module '\n",
      "                                       'should still be a powerful model even '\n",
      "                                       'without\\n'\n",
      "                                       'short-term memory (i.e., attention). '\n",
      "                                       'We refer to this variant as LMM or '\n",
      "                                       'Titans (LMM) in our experiments. We '\n",
      "                                       'provide\\n'\n",
      "                                       'additional discussions on the '\n",
      "                                       'connection of Titans and other modern '\n",
      "                                       'recurrent models in Appendix C.\\n'\n",
      "                                       '4.4 Architectural Details\\n'\n",
      "                                       'For the sake of simplicity and '\n",
      "                                       'presentation, we avoid discussing the '\n",
      "                                       'implementation details like using '\n",
      "                                       'residual connection,\\n'\n",
      "                                       'gating with linear layer, and '\n",
      "                                       'normalization. In all blocks, we use '\n",
      "                                       'residual connections. In our '\n",
      "                                       'implementation, we use\\n'\n",
      "                                       'SiLU(.) activation (Elfwing, Uchibe, '\n",
      "                                       'and Doya 2018) as the non-linear '\n",
      "                                       'activation for computing query, key, '\n",
      "                                       'and values and\\n'\n",
      "                                       'normalize queries and keys using '\n",
      "                                       'ℓ2-norm.\\n'\n",
      "                                       'Convolution. Following the recent '\n",
      "                                       'modern linear recurrent models (Gu and '\n",
      "                                       'Dao 2024; S. Yang, Kautz, and '\n",
      "                                       'Hatamizadeh\\n'\n",
      "                                       '2024), we incorporate a 1D '\n",
      "                                       'depthwise-separable convolution layer '\n",
      "                                       'after each of the query, key, and '\n",
      "                                       'value projections.\\n'\n",
      "                                       'While not significantly affect the '\n",
      "                                       'performance, these 1D convolutions '\n",
      "                                       'have shown performance improvement and '\n",
      "                                       'are also\\n'\n",
      "                                       'computationally efficient.\\n'\n",
      "                                       'Gating. We also follow the recent '\n",
      "                                       'architectures that use normalization '\n",
      "                                       'and gating with a linear layer before '\n",
      "                                       'the final\\n'\n",
      "                                       'output projection (Mehta et al. '\n",
      "                                       '2023).\\n'\n",
      "                                       'Theorem 4.1. Contrary to Transformers, '\n",
      "                                       'diagonal linear recurrent models, and '\n",
      "                                       'DeltaNet, all of which are limited to '\n",
      "                                       'TC0 (Merrill,\\n'\n",
      "                                       'Petty, and Sabharwal 2024), Titans are '\n",
      "                                       'capable of solving problems beyond TC '\n",
      "                                       '0, meaning that Titans are '\n",
      "                                       'theoretically more\\n'\n",
      "                                       'expressive than Transformers and most '\n",
      "                                       'modern linear recurrent models in '\n",
      "                                       'state tracking tasks.\\n'\n",
      "                                       '5 Experiments\\n'\n",
      "                                       'N\\n'\n",
      "                                       'ext, we evaluate the performance of '\n",
      "                                       'Titans and its variants in language '\n",
      "                                       'modeling, commonsense reasoning, '\n",
      "                                       'needle\\n'\n",
      "                                       'in haystack, DNA modeling, and time '\n",
      "                                       'series forecasting tasks1. In more '\n",
      "                                       'details, in this section, we answer '\n",
      "                                       'the\\n'\n",
      "                                       'following empirical questions: (1) How '\n",
      "                                       'do Titans perform compared to '\n",
      "                                       'baselines in downstream tasks? (see '\n",
      "                                       '§5.2,\\n'\n",
      "                                       '1In the first version of the work, we '\n",
      "                                       'aim to provide insights/evidences '\n",
      "                                       'about why the learning paradigms of '\n",
      "                                       'Titans are effective. We are working '\n",
      "                                       'on\\n'\n",
      "                                       'finalizing the results of larger '\n",
      "                                       'models and will report them in the '\n",
      "                                       'next version.\\n'\n",
      "                                       '11\\n'\n",
      "                                       '§5.6, and §5.7); (2) What is the '\n",
      "                                       'actual context length of Titans? (see '\n",
      "                                       '§5.3 and §5.4); (3) How do Titans '\n",
      "                                       'scale with respect to\\n'\n",
      "                                       'context length? (see §5.8); (4) How '\n",
      "                                       'the depth of memory can affect both '\n",
      "                                       'performance and efficiency? (see '\n",
      "                                       '§5.5); and (5)\\n'\n",
      "                                       'What is the contribution of each '\n",
      "                                       'Titans’ component in its performance? '\n",
      "                                       '(see §5.9).\\n'\n",
      "                                       '5.1 Experimental Setup\\n'\n",
      "                                       'Models. In our experiments, we focus '\n",
      "                                       'on the three variants of Titans, which '\n",
      "                                       'we refer to as: Titans with (1) Memory '\n",
      "                                       'as a\\n'\n",
      "                                       'Context (MAC), (2) Memory as a Gate '\n",
      "                                       '(MAG), and (3) Memory as a Layer (MAL) '\n",
      "                                       'as well as (4) neural memory module\\n'\n",
      "                                       'alone. The reason behind using our '\n",
      "                                       'long-term memory as a separate module '\n",
      "                                       'is based on our definition of '\n",
      "                                       'learning. As\\n'\n",
      "                                       'discussed in Section 1, we define '\n",
      "                                       'learning a process for acquiring '\n",
      "                                       'effective and useful memory. '\n",
      "                                       'Accordingly, we expect our\\n'\n",
      "                                       'long-term memory to effectively learn '\n",
      "                                       'from data, even without attention. For '\n",
      "                                       'each of these models, we consider four '\n",
      "                                       'scales\\n'\n",
      "                                       'with: (i) 170M, (ii) 340M, (iii) 400M, '\n",
      "                                       'and (iv) 760M parameters. While the '\n",
      "                                       'first three are trained on 15B tokens '\n",
      "                                       'sampled\\n'\n",
      "                                       'from FineWeb-Edu dataset (Penedo et '\n",
      "                                       'al. 2024), the last one is trained on '\n",
      "                                       '30B tokens from the same dataset.\\n'\n",
      "                                       'Baselines. We compare our models with '\n",
      "                                       'the state-of-the-art linear recurrent '\n",
      "                                       'models, Transformers, and hybrid '\n",
      "                                       'models\\n'\n",
      "                                       '(recurrent + attention). More '\n",
      "                                       'specifically in language tasks, we '\n",
      "                                       'compare with Transformer++ (Touvron et '\n",
      "                                       'al. 2023),\\n'\n",
      "                                       'RetNet (Yutao Sun et al. 2023), Gated '\n",
      "                                       'Linear Attention (GLA) (S. Yang, B. '\n",
      "                                       'Wang, Shen, et al. 2024), Mamba (Gu '\n",
      "                                       'and Dao\\n'\n",
      "                                       '2024), Mamba2 (Dao and Gu 2024), '\n",
      "                                       'DeltaNet (S. Yang, B. Wang, Yu Zhang, '\n",
      "                                       'et al. 2024), TTT (Yu Sun et al. '\n",
      "                                       '2024), and Gated\\n'\n",
      "                                       'DeltaNet (S. Yang, Kautz, and '\n",
      "                                       'Hatamizadeh 2024). In needle in '\n",
      "                                       'haystack tasks, we also compare with '\n",
      "                                       'GPT4 (Achiam et al.\\n'\n",
      "                                       '2023), Llama3 with RAG (Touvron et al. '\n",
      "                                       '2023), RecurrentGemma2-9B (Botev et '\n",
      "                                       'al. 2024), and Mistral (Jiang et al. '\n",
      "                                       '2023)\\n'\n",
      "                                       'models, all of which are provided in '\n",
      "                                       'the benchmark (Yuri Kuratov et al. '\n",
      "                                       '2024). In time series tasks, we '\n",
      "                                       'compare with\\n'\n",
      "                                       'Mamba-based (Behrouz, Santacatterina, '\n",
      "                                       'and Zabih 2024), Transformer-based (Y. '\n",
      "                                       'Liu et al. 2023; Nie et al. 2022; '\n",
      "                                       'Yunhao\\n'\n",
      "                                       'Zhang and Yan 2023), and linear models '\n",
      "                                       '(Das et al. 2023; Z. Li et al. 2023; '\n",
      "                                       'H. Wu et al. 2023; Zeng et al. 2023).\\n'\n",
      "                                       'Training. In the training, we follow '\n",
      "                                       'the training procedure of S. Yang, '\n",
      "                                       'Kautz, and Hatamizadeh (2024), and use '\n",
      "                                       'LLama 2\\n'\n",
      "                                       'tokenizer with a vocabulary size of '\n",
      "                                       '32K and use training length of 4K '\n",
      "                                       'tokens. We employ AdamW optimizer with '\n",
      "                                       'learning\\n'\n",
      "                                       'rate of 4𝑒-4 with cosine annealing '\n",
      "                                       'schedule with batch size of 0.5M '\n",
      "                                       'tokens, and weight decay of 0.1.\\n'\n",
      "                                       '5.2 Language Modeling\\n'\n",
      "                                       'We first focus on the perplexity in '\n",
      "                                       'language modeling and also commonsense '\n",
      "                                       'reasoning tasks. The results for '\n",
      "                                       'Titans’\\n'\n",
      "                                       'variants and also baselines with three '\n",
      "                                       'different sizes of 340M, 400M, and '\n",
      "                                       '760M are reported in Table 1. Among '\n",
      "                                       'non-hybrid\\n'\n",
      "                                       'models, including Transformer++, our '\n",
      "                                       'neural memory module achieves the best '\n",
      "                                       'performance in both perplexity and\\n'\n",
      "                                       'accuracy measures. Comparing our '\n",
      "                                       'neural memory module and TTT, which is '\n",
      "                                       'also a gradient-based recurrent model '\n",
      "                                       'can\\n'\n",
      "                                       'show us the importance of our weight '\n",
      "                                       'decay as well as the momentum. As '\n",
      "                                       'discussed earlier, the weight decay '\n",
      "                                       'can be\\n'\n",
      "                                       'interpreted as a gating mechanism to '\n",
      "                                       'forget the past data, when it is '\n",
      "                                       'needed. Also, momentum can help us '\n",
      "                                       'better manage\\n'\n",
      "                                       'the memory by providing additional '\n",
      "                                       'memory for the surprise metric. While '\n",
      "                                       'some baselines also take advantage of '\n",
      "                                       'gating\\n'\n",
      "                                       'mechanism, e.g., Mamba, Mamba2, and '\n",
      "                                       'Gated DeltaNet, the superior '\n",
      "                                       'performance of our neural memory '\n",
      "                                       'module shows\\n'\n",
      "                                       'the importance of both our surprise '\n",
      "                                       'mechanism and having deep and '\n",
      "                                       'non-linear memory. We further discuss '\n",
      "                                       'the later in\\n'\n",
      "                                       'Section 5.5.\\n'\n",
      "                                       'Comparing the hybrid models, we found '\n",
      "                                       'that all three variants of Titans '\n",
      "                                       '(MAC, MAG, and MAL) outperform both '\n",
      "                                       'Samba\\n'\n",
      "                                       '(Mamba + attention) and Gated '\n",
      "                                       'DeltaNet-H2 (Gated DeltaNet + '\n",
      "                                       'atttention). We attribute the superior '\n",
      "                                       'performance of Titans\\n'\n",
      "                                       '(MAL) to the power of neural memory '\n",
      "                                       'module as the architecture design and '\n",
      "                                       'used attention are all the same. '\n",
      "                                       'Comparing\\n'\n",
      "                                       'Titans (MAG) and (MAC), we find that '\n",
      "                                       'while their performance are close, MAC '\n",
      "                                       'performs better when dealing with '\n",
      "                                       'longer\\n'\n",
      "                                       'dependencies in the data. '\n",
      "                                       'Interestingly, both MAG and MAC '\n",
      "                                       'outperform MAL variant, which due to '\n",
      "                                       'using the same\\n'\n",
      "                                       'modules, we attribute this to the '\n",
      "                                       'architecture design of these models. '\n",
      "                                       'This finding is particularly important '\n",
      "                                       'as the current\\n'\n",
      "                                       'hybrid models (except Hymba (X. Dong '\n",
      "                                       'et al. 2024)) in the literature are '\n",
      "                                       'using MAL-style combination of '\n",
      "                                       'recurrent models\\n'\n",
      "                                       'and attention.\\n'\n",
      "                                       '5.3 Needle in a Haystack\\n'\n",
      "                                       'Scaling a model to longer context '\n",
      "                                       'window is not always equivalent to '\n",
      "                                       'being effective for very long '\n",
      "                                       'sequences (Hsieh\\n'\n",
      "                                       'et al. 2024). The needle-in-a-haystack '\n",
      "                                       '(NIAH) task is designed to measure the '\n",
      "                                       'actual effective context length of '\n",
      "                                       'models.\\n'\n",
      "                                       'In this task, we evaluate the model on '\n",
      "                                       'retrieving a piece of information '\n",
      "                                       '(i.e., the “needle”) from long '\n",
      "                                       'distractor texts (i.e.,\\n'\n",
      "                                       '12\\n'\n",
      "                                       'Table 1: Performance of Titans and '\n",
      "                                       'recurrent- and Transformer-based '\n",
      "                                       'baselines on language modeling and '\n",
      "                                       'common-sense\\n'\n",
      "                                       'reasoning tasks. Hybrid models are '\n",
      "                                       'marked with ∗. The best results among '\n",
      "                                       'simple and hybrid models are '\n",
      "                                       'highlighted.\\n'\n",
      "                                       'Model Wiki. LMB. LMB. PIQA Hella. '\n",
      "                                       'Wino. ARC-e ARC-c SIQA BoolQ Avg.\\n'\n",
      "                                       'ppl↓ ppl↓ acc↑ acc↑ acc_n↑ acc↑ acc↑ '\n",
      "                                       'acc_n↑ acc↑ acc↑ ↑\\n'\n",
      "                                       '340M params / 15B tokens\\n'\n",
      "                                       'Transformer++ 31.52 41.08 30.76 62.98 '\n",
      "                                       '34.76 50.53 45.21 24.05 36.81 58.24 '\n",
      "                                       '42.92\\n'\n",
      "                                       'RetNet 32.50 49.73 28.24 62.61 34.15 '\n",
      "                                       '50.91 44.27 23.62 36.79 59.72 42.54\\n'\n",
      "                                       'GLA 28.51 43.02 28.73 64.05 35.96 '\n",
      "                                       '50.00 54.19 24.29 37.13 58.39 44.09\\n'\n",
      "                                       'Mamba 30.83 40.21 29.94 63.79 35.88 '\n",
      "                                       '49.82 49.24 24.56 35.41 60.07 43.59\\n'\n",
      "                                       'DeltaNet 28.65 47.30 28.43 63.52 35.95 '\n",
      "                                       '49.63 52.68 25.37 37.96 58.79 44.04\\n'\n",
      "                                       'TTT 27.44 34.19 30.06 63.97 35.71 '\n",
      "                                       '50.08 53.01 26.11 37.32 59.83 44.51\\n'\n",
      "                                       'Gated DeltaNet 27.01 30.94 34.11 63.08 '\n",
      "                                       '38.12 51.60 55.28 26.77 34.89 59.54 '\n",
      "                                       '45.42\\n'\n",
      "                                       'Titans (LMM) 26.18 29.97 34.98 64.73 '\n",
      "                                       '39.61 51.85 55.60 28.14 34.52 59.99 '\n",
      "                                       '46.17\\n'\n",
      "                                       'Titans (MAC)∗ 25.43 28.13 36.00 65.32 '\n",
      "                                       '40.35 51.21 58.17 29.00 38.63 60.18 '\n",
      "                                       '47.36\\n'\n",
      "                                       'Titans (MAG)∗ 25.07 28.72 36.71 64.88 '\n",
      "                                       '40.56 52.49 57.72 28.16 39.75 60.01 '\n",
      "                                       '47.54\\n'\n",
      "                                       'Titans (MAL)∗ 24.69 28.80 35.74 64.97 '\n",
      "                                       '39.44 51.97 56.58 28.21 38.14 57.32 '\n",
      "                                       '46.55\\n'\n",
      "                                       '400M params / 15B tokens\\n'\n",
      "                                       'Transformer++ 30.63 37.37 29.64 64.27 '\n",
      "                                       '37.72 51.53 54.95 27.36 38.07 61.59 '\n",
      "                                       '45.64\\n'\n",
      "                                       'RetNet 29.92 46.83 29.16 65.23 36.97 '\n",
      "                                       '51.85 56.01 27.55 37.30 59.66 45.47\\n'\n",
      "                                       'HGRN2 32.33 47.14 26.12 64.52 35.45 '\n",
      "                                       '52.24 55.97 25.51 37.35 59.02 44.52\\n'\n",
      "                                       'GLA 27.96 36.66 27.86 65.94 37.41 '\n",
      "                                       '49.56 56.01 26.36 38.94 59.84 45.24\\n'\n",
      "                                       'Mamba 29.22 39.88 29.82 65.72 37.93 '\n",
      "                                       '50.11 58.37 26.70 37.76 61.13 45.94\\n'\n",
      "                                       'Mamba2 26.34 33.19 32.03 65.77 39.73 '\n",
      "                                       '52.48 59.00 27.64 37.92 60.72 46.91\\n'\n",
      "                                       'DeltaNet 27.69 44.04 29.96 64.52 37.03 '\n",
      "                                       '50.82 56.77 27.13 38.22 60.09 45.57\\n'\n",
      "                                       'TTT 26.11 31.52 33.25 65.70 39.11 '\n",
      "                                       '51.68 58.04 28.99 38.26 59.87 46.86\\n'\n",
      "                                       'Gated DeltaNet 25.47 29.24 34.40 65.94 '\n",
      "                                       '40.46 51.46 59.80 28.58 37.43 60.03 '\n",
      "                                       '47.26\\n'\n",
      "                                       'Samba∗ 25.32 29.47 36.86 66.09 39.24 '\n",
      "                                       '51.45 60.12 27.20 38.68 58.22 47.23\\n'\n",
      "                                       'Gated DeltaNet-H2∗ 24.19 28.09 36.77 '\n",
      "                                       '66.43 40.79 52.17 59.55 29.09 39.04 '\n",
      "                                       '58.56 47.69\\n'\n",
      "                                       'Titans (LMM) 25.03 28.99 35.21 65.85 '\n",
      "                                       '40.91 52.19 59.97 29.20 38.74 60.85 '\n",
      "                                       '47.83\\n'\n",
      "                                       'Titans (MAC)∗ 25.61 27.73 36.92 66.39 '\n",
      "                                       '41.18 52.80 60.24 29.69 40.07 61.93 '\n",
      "                                       '48.65\\n'\n",
      "                                       'Titans (MAG)∗ 23.59 27.81 37.24 66.80 '\n",
      "                                       '40.92 53.21 60.01 29.45 39.91 61.28 '\n",
      "                                       '48.60\\n'\n",
      "                                       'Titans (MAL)∗ 23.93 27.89 36.84 66.29 '\n",
      "                                       '40.74 52.26 59.85 29.71 38.92 58.40 '\n",
      "                                       '47.87\\n'\n",
      "                                       '760M params / 30B tokens\\n'\n",
      "                                       'Transformer++ 25.21 27.64 35.78 66.92 '\n",
      "                                       '42.19 51.95 60.38 32.46 39.51 60.37 '\n",
      "                                       '48.69\\n'\n",
      "                                       'RetNet 26.08 24.45 34.51 67.19 41.63 '\n",
      "                                       '52.09 63.17 32.78 38.36 57.92 48.46\\n'\n",
      "                                       'Mamba 28.12 23.96 32.80 66.04 39.15 '\n",
      "                                       '52.38 61.49 30.34 37.96 57.62 47.22\\n'\n",
      "                                       'Mamba2 22.94 28.37 33.54 67.90 42.71 '\n",
      "                                       '49.77 63.48 31.09 40.06 58.15 48.34\\n'\n",
      "                                       'DeltaNet 24.37 24.60 37.06 66.93 41.98 '\n",
      "                                       '50.65 64.87 31.39 39.88 59.02 48.97\\n'\n",
      "                                       'TTT 24.17 23.51 34.74 67.25 43.92 '\n",
      "                                       '50.99 64.53 33.81 40.16 59.58 47.32\\n'\n",
      "                                       'Gated DeltaNet 21.18 22.09 35.54 68.01 '\n",
      "                                       '44.95 50.73 66.87 33.09 39.21 59.14 '\n",
      "                                       '49.69\\n'\n",
      "                                       'Samba∗ 20.63 22.71 39.72 69.19 47.35 '\n",
      "                                       '52.01 66.92 33.20 38.98 61.24 51.08\\n'\n",
      "                                       'Gated DeltaNet-H2∗ 19.88 20.83 39.18 '\n",
      "                                       '68.95 48.22 52.57 67.01 35.49 39.39 '\n",
      "                                       '61.11 51.49\\n'\n",
      "                                       'Titans (LMM) 20.04 21.96 37.40 69.28 '\n",
      "                                       '48.46 52.27 66.31 35.84 40.13 62.76 '\n",
      "                                       '51.56\\n'\n",
      "                                       'Titans (MAC) 19.93 20.12 39.62 70.46 '\n",
      "                                       '49.01 53.18 67.86 36.01 41.87 62.05 '\n",
      "                                       '52.51\\n'\n",
      "                                       'Titans (MAG) 18.61 19.86 40.98 70.25 '\n",
      "                                       '48.94 52.89 68.23 36.19 40.38 62.11 '\n",
      "                                       '52.50\\n'\n",
      "                                       'Titans (MAL) 19.07 20.33 40.05 69.99 '\n",
      "                                       '48.82 53.02 67.54 35.65 30.98 61.72 '\n",
      "                                       '50.97\\n'\n",
      "                                       'the “haystack”). In this part, we use '\n",
      "                                       'Single NIAH (S-NIAH) task from RULER '\n",
      "                                       'benchmark (Hsieh et al. 2024) and '\n",
      "                                       'evaluate\\n'\n",
      "                                       'Titans and baselines on sequences with '\n",
      "                                       'length 2K, 4K, 8K, and 16K. The '\n",
      "                                       'results are reported in Table 2. '\n",
      "                                       'Neural Memory\\n'\n",
      "                                       'module achieves the best results '\n",
      "                                       'compare to baselines in all three '\n",
      "                                       'tasks. We attribute this superior '\n",
      "                                       'performance to three\\n'\n",
      "                                       'key differences of Titans with '\n",
      "                                       'existing sequence models: (1) Compared '\n",
      "                                       'to TTT, our Neural Memory can better '\n",
      "                                       'handle the\\n'\n",
      "                                       'memory capacity by using momentum and '\n",
      "                                       'also the forgetting mechanism (i.e., '\n",
      "                                       'weight decay). Therefore, with '\n",
      "                                       'increasing\\n'\n",
      "                                       'the sequence length, the performance '\n",
      "                                       'of Neural Memory does not drop and '\n",
      "                                       'show a consistent trend; (2) Compared '\n",
      "                                       'to\\n'\n",
      "                                       'Mamba2, which has the gating '\n",
      "                                       '(forgetting) mechanism, Titans have '\n",
      "                                       'deep non-linear memory, resulting in '\n",
      "                                       'better memory\\n'\n",
      "                                       'management. Also, contrary to our '\n",
      "                                       'neural memory and DeltaNet, Mamba2 is '\n",
      "                                       'not capable of removing a memory and '\n",
      "                                       'so\\n'\n",
      "                                       '13\\n'\n",
      "                                       'Table 2: Performance of Titans and '\n",
      "                                       'baselines on S-NIAH task from RULER '\n",
      "                                       'benchmark. The best results among '\n",
      "                                       'simple\\n'\n",
      "                                       'and hybrid models are highlighted.\\n'\n",
      "                                       'Model S-NIAH-PK S-NIAH-N S-NIAH-W\\n'\n",
      "                                       '2K 4K 8K 16K 2K 4K 8K 16K 2K 4K 8K '\n",
      "                                       '16K\\n'\n",
      "                                       'TTT 98.4 98.8 98.0 88.4 60.2 36.6 10.2 '\n",
      "                                       '4.4 78.8 28.0 4.4 0.0\\n'\n",
      "                                       'Mamba2 98.6 61.4 31.0 5.4 98.4 55.8 '\n",
      "                                       '14.2 0.0 42.2 4.2 0.0 0.0\\n'\n",
      "                                       'DeltaNet 96.8 98.8 98.6 71.4 47.2 15.4 '\n",
      "                                       '12.8 5.4 46.2 20.0 1.6 0.0\\n'\n",
      "                                       'Titans (LMM)99.8 98.4 98.2 96.2 100.0 '\n",
      "                                       '99.8 93.4 80.2 90.4 89.4 85.8 80.6\\n'\n",
      "                                       'Titans (MAC) 99.2 98.8 99.0 98.4 99.6 '\n",
      "                                       '98.2 97.6 97.4 98.2 98.2 95.6 95.2\\n'\n",
      "                                       'Titans (MAG)99.4 98.0 97.4 97.4 99.2 '\n",
      "                                       '98.8 97.2 98.6 98.0 98.0 90.2 88.2\\n'\n",
      "                                       'Titans (MAL) 98.8 98.6 98.8 97.8 99.8 '\n",
      "                                       '98.1 96.8 96.4 98.0 97.4 92.0 90.4\\n'\n",
      "                                       '(a) Few-shot Setup\\n'\n",
      "                                       ' (b) Fine-Tuning Setup\\n'\n",
      "                                       'Figure 6: Performance of Titans and '\n",
      "                                       'baselines on BABILong benchmark. '\n",
      "                                       'Titans (MAC) outperforms all '\n",
      "                                       'baselines, including\\n'\n",
      "                                       'extremely large models, e.g., GPT4.\\n'\n",
      "                                       'we can see a significant drop in '\n",
      "                                       'performance when increasing the '\n",
      "                                       'sequence length; (3) Compared to '\n",
      "                                       'DeltaNet, although it\\n'\n",
      "                                       'is capable of removing memory using '\n",
      "                                       'delta rule, it cannot erase the '\n",
      "                                       'memory, lacking forgetting mechanism. '\n",
      "                                       'Finally, As\\n'\n",
      "                                       'expected we can see on par or better '\n",
      "                                       'results when using Titans variants, '\n",
      "                                       'where the best results correspond to '\n",
      "                                       'MAC.\\n'\n",
      "                                       '5.4 BABILong Benchmark\\n'\n",
      "                                       'In the previous section we discussed '\n",
      "                                       'the results on a simple NIAH tasks '\n",
      "                                       'where a single needle needs to be '\n",
      "                                       'retrieved.\\n'\n",
      "                                       'Although Titans showed better '\n",
      "                                       'performance compared to baselines, '\n",
      "                                       'their true advantage over very long '\n",
      "                                       'sequences is still\\n'\n",
      "                                       'hidden. To this end, in this section, '\n",
      "                                       'we use a harder task from BABILong '\n",
      "                                       'benchmark (Yuri Kuratov et al. 2024), '\n",
      "                                       'in which\\n'\n",
      "                                       'the model needs to reason across facts '\n",
      "                                       'distributed in extremely long '\n",
      "                                       'documents. We follow the original '\n",
      "                                       'experimental setup\\n'\n",
      "                                       'and training process in the benchmark. '\n",
      "                                       'There are two settings: (1) Few-shot '\n",
      "                                       'setting, in which we use large '\n",
      "                                       'pre-trained\\n'\n",
      "                                       'models, and (2) fine-tuning setting, '\n",
      "                                       'where we fine-tune the MAC variant of '\n",
      "                                       'Titans to compare it with other '\n",
      "                                       'fine-tuned\\n'\n",
      "                                       'baselines. The results for few-shot '\n",
      "                                       'setting are reported in Figure 6a. In '\n",
      "                                       'this setup, we can see Titans '\n",
      "                                       'outperform all\\n'\n",
      "                                       'baselines–i.e., Mamba2.8B (Gu and Dao '\n",
      "                                       '2024), RWKV-6-7B (Peng, Goldstein, et '\n",
      "                                       'al. 2024), RecurrentGemma-9B (Botev et '\n",
      "                                       'al.\\n'\n",
      "                                       '2024), Gemma-9B (Team et al. 2024), '\n",
      "                                       'Llama3.1-8B (Touvron et al. 2023), '\n",
      "                                       'GPT-4, and GPT4o-mini (Achiam et al. '\n",
      "                                       '2023). These\\n'\n",
      "                                       'results are achieved while Titans '\n",
      "                                       '(MAC) is having much less number of '\n",
      "                                       'parameters than baselines.\\n'\n",
      "                                       'In the fine-tuning setup, we compare '\n",
      "                                       'the small fine-tuned version of Titans '\n",
      "                                       '(MAC) with: (i) the fine-tuned version '\n",
      "                                       'of small\\n'\n",
      "                                       'models (almost the same number of '\n",
      "                                       'parameters as Titans) such as Mamba '\n",
      "                                       '(Gu and Dao 2024), RMT (Bulatov, Yury '\n",
      "                                       'Kuratov,\\n'\n",
      "                                       'and Burtsev 2022), (ii) large models '\n",
      "                                       'with Retrieval-Augmented Generation '\n",
      "                                       '(RAG) (P. Lewis et al. 2020) such as '\n",
      "                                       'Llama3.1-\\n'\n",
      "                                       '8B (Touvron et al. 2023), and (iii) '\n",
      "                                       'extremely large models such as GPT-4 '\n",
      "                                       '(Achiam et al. 2023), GPT4o-mini, '\n",
      "                                       'Qwen2.5-72B (A.\\n'\n",
      "                                       'Yang et al. 2024), and Llama3.1-70B '\n",
      "                                       '(Touvron et al. 2023). Baseline '\n",
      "                                       'results are reported by (Yuri Kuratov '\n",
      "                                       'et al. 2024). The\\n'\n",
      "                                       'results of Titans and baselines are '\n",
      "                                       'reported in Figure 6b. Titans '\n",
      "                                       'outperform all models even extremely '\n",
      "                                       'large models like\\n'\n",
      "                                       'GPT4. Also, compared to '\n",
      "                                       'Transformer-based with memory models '\n",
      "                                       'like RMT, Titans show better '\n",
      "                                       'performance mainly due\\n'\n",
      "                                       'to their powerful memory. That is, RMT '\n",
      "                                       'compress the historical data into 16 '\n",
      "                                       'size vector-valued memory, while '\n",
      "                                       'Titans with\\n'\n",
      "                                       'in-context online memory learner are '\n",
      "                                       'capable of encoding the past into the '\n",
      "                                       'parameters of the model. '\n",
      "                                       'Interestingly, even\\n'\n",
      "                                       '14\\n'\n",
      "                                       '(a) 170M Parameters\\n'\n",
      "                                       ' (b) 360M Parameters\\n'\n",
      "                                       ' (c) 760M Parameters\\n'\n",
      "                                       'Figure 7: The effect of memory depth '\n",
      "                                       'on the perplexity. Deeper long-term '\n",
      "                                       'memory results in better scaling in '\n",
      "                                       'longer\\n'\n",
      "                                       'sequences.\\n'\n",
      "                                       'Table 3: Performance on long-term '\n",
      "                                       'forecasting. The best results are '\n",
      "                                       'highlighted .\\n'\n",
      "                                       'Neural MemorySimba iTransformerRLinear '\n",
      "                                       'PatchTSTCrossformerTiDE TimesNet '\n",
      "                                       'DLinear\\n'\n",
      "                                       'MSE MAE MSE MAE MSE MAE MSE MAE MSE '\n",
      "                                       'MAE MSE MAE MSE MAE MSE MAE MSE MAE\\n'\n",
      "                                       'ETTm1 0.358 0.387 0.3830.3960.407 '\n",
      "                                       '0.410 0.4140.4070.3870.4000.5130.496 '\n",
      "                                       '0.4190.4190.4000.4060.4030.407\\n'\n",
      "                                       'ETTm2 0.261 0.309 0.2710.3270.288 '\n",
      "                                       '0.332 0.2860.3270.2810.3260.7570.610 '\n",
      "                                       '0.3580.4040.2910.3330.3500.401\\n'\n",
      "                                       'ETTh1 0.420 0.421 0.4410.4320.454 '\n",
      "                                       '0.447 0.4460.4340.4690.4540.5290.522 '\n",
      "                                       '0.5410.5070.4580.4500.4560.452\\n'\n",
      "                                       'ETTh2 0.336 0.382 0.3610.3910.383 '\n",
      "                                       '0.407 0.3740.3980.3870.4070.9420.684 '\n",
      "                                       '0.6110.5500.4140.4270.5590.515\\n'\n",
      "                                       'ECL 0.162 0.261 0.1690.2740.178 0.270 '\n",
      "                                       '0.2190.2980.2050.2900.2440.334 '\n",
      "                                       '0.2510.3440.1920.2950.2120.300\\n'\n",
      "                                       'Traffic 0.415 0.289 0.4930.2910.428 '\n",
      "                                       '0.282 0.6260.3780.4810.3040.5500.304 '\n",
      "                                       '0.7600.4730.6200.3360.6250.383\\n'\n",
      "                                       'Weather0.231 0.265 0.2550.2800.258 '\n",
      "                                       '0.278 0.2720.2910.2590.2810.2590.315 '\n",
      "                                       '0.2710.3200.2590.2870.2650.317\\n'\n",
      "                                       'augmenting Llama3.1-8B model with RAG '\n",
      "                                       'performs worse than Titans with about '\n",
      "                                       '×70 less parameters.\\n'\n",
      "                                       '5.5 The Effect of Deep Memory\\n'\n",
      "                                       'In this section, we evaluate the '\n",
      "                                       'effect of deep memory in both '\n",
      "                                       'wall-clock training time and model '\n",
      "                                       'performance2. To this\\n'\n",
      "                                       'end, we focus on different variants of '\n",
      "                                       'our neural memory module, where 𝐿M= '\n",
      "                                       '1,2,3,4. We also use Mamba as a '\n",
      "                                       'baseline\\n'\n",
      "                                       'for the model performance. For a fair '\n",
      "                                       'comparison, we use the same training '\n",
      "                                       'process for all models and train them '\n",
      "                                       'on a\\n'\n",
      "                                       'subset of the Pile dataset (L. Gao et '\n",
      "                                       'al. 2020).\\n'\n",
      "                                       'We report the perplexity of our models '\n",
      "                                       'and baselines as the function of the '\n",
      "                                       'sequence length in Figure 7. '\n",
      "                                       'Interestingly, with\\n'\n",
      "                                       'the increase of memory depth, 𝐿M, the '\n",
      "                                       'model can achieve better perplexity '\n",
      "                                       'over all sequence length. Also, deeper '\n",
      "                                       'memory\\n'\n",
      "                                       'modules are more robust to the '\n",
      "                                       'sequence length when the model has '\n",
      "                                       'less number of parameters. With the '\n",
      "                                       'increase of the\\n'\n",
      "                                       'number of parameters, all models show '\n",
      "                                       'better performance on longer '\n",
      "                                       'sequences.\\n'\n",
      "                                       'Figure 8: The effect of memory depth '\n",
      "                                       'on\\n'\n",
      "                                       'training throughput\\n'\n",
      "                                       'We also evaluate the effect of memory '\n",
      "                                       'depth ( 𝐿M = 1,2,3,4) on the training\\n'\n",
      "                                       'throughput. We report the training '\n",
      "                                       'throughput (the number of tokens per\\n'\n",
      "                                       'second) as the function of sequence '\n",
      "                                       'length in Figure 8. All models scale '\n",
      "                                       'linearly\\n'\n",
      "                                       'with respect to the context length '\n",
      "                                       '(i.e., constant trend in the number of '\n",
      "                                       'tokens\\n'\n",
      "                                       'per second with respect to sequence '\n",
      "                                       'length). Also, by increasing the '\n",
      "                                       'memory\\n'\n",
      "                                       'depth, as expected, we can see a '\n",
      "                                       'linear trend that a deeper memory '\n",
      "                                       'results in\\n'\n",
      "                                       'a slower training. Therefore, it is '\n",
      "                                       'not always efficient to use deeper '\n",
      "                                       'memory\\n'\n",
      "                                       'modules, showing a trade-off between '\n",
      "                                       'effectiveness and efficiency.\\n'\n",
      "                                       '5.6 Time Series Forecasting\\n'\n",
      "                                       'To show the effectiveness of our '\n",
      "                                       'memory module in a broader tasks, we '\n",
      "                                       'also evaluate its performance in time '\n",
      "                                       'series\\n'\n",
      "                                       'forecasting tasks. To this end, we use '\n",
      "                                       'Simba framework (Patro and Agneeswaran '\n",
      "                                       '2024) for time series forecasting, '\n",
      "                                       'and\\n'\n",
      "                                       '2Note that, in this experiment, we '\n",
      "                                       'only focus on the neural memory module '\n",
      "                                       'to evaluate the effect of memory depth '\n",
      "                                       'in the memorization process.\\n'\n",
      "                                       'Combining neural memory with attention '\n",
      "                                       'as we do in Titans variants, can '\n",
      "                                       'additionally enhance the performance '\n",
      "                                       'of the model over long sequences.\\n'\n",
      "                                       '15\\n'\n",
      "                                       'Table 4: Downstream evaluation of '\n",
      "                                       'pre-trained DNA models on '\n",
      "                                       'GenomicsBenchmarks (Grešová et al. '\n",
      "                                       '2023). We report\\n'\n",
      "                                       'top-1 classification accuracy (%).\\n'\n",
      "                                       'Model Enhancer Cohn Enhancer Ens Human '\n",
      "                                       'Reg. Non-TATA Promoters Human OCR '\n",
      "                                       'Ens.\\n'\n",
      "                                       'CNN 69.5 68.9 93.3 84.6 68.0\\n'\n",
      "                                       'DNABERT 74.0 85.7 88.1 85.6 75.1\\n'\n",
      "                                       'GPT 70.5 83.5 91.5 87.7 73.0\\n'\n",
      "                                       'HyenaDNA 74.2 89.2 93.8 96.6 80.9\\n'\n",
      "                                       'Transformer++ 73.4 89.5 89.9 94.4 '\n",
      "                                       '79.5\\n'\n",
      "                                       'Mamba 73.0 - - 96.6 -\\n'\n",
      "                                       'Based 74.6 89.5 89.5 96.8 79.0\\n'\n",
      "                                       'Neural Memory Module 75.2 89.6 89.3 '\n",
      "                                       '96.6 79.9\\n'\n",
      "                                       'replace its Mamba module with our '\n",
      "                                       'neural memory. We report the results '\n",
      "                                       'on common time series forecasting '\n",
      "                                       'benchmark\\n'\n",
      "                                       'datasets–ETT, ECL, Traffic, and '\n",
      "                                       'Weather (H. Zhou et al. 2021). The '\n",
      "                                       'results are reported in Table 3. Our '\n",
      "                                       'neural memory\\n'\n",
      "                                       'module is outperforming all baselines, '\n",
      "                                       'including Mamba-based, linear-based, '\n",
      "                                       'and Transformer-based architectures.\\n'\n",
      "                                       '5.7 DNA Modeling\\n'\n",
      "                                       'In order to understand the capability '\n",
      "                                       'of Titans beyond natural language, we '\n",
      "                                       'further evaluate the performance of '\n",
      "                                       'our\\n'\n",
      "                                       'neural memory module on DNA modeling '\n",
      "                                       'tasks. To this end, we evaluate '\n",
      "                                       'pre-trained models on the downstream '\n",
      "                                       'tasks\\n'\n",
      "                                       'in GenomicsBenchmarks (Grešová et al. '\n",
      "                                       '2023). We follow the same experimental '\n",
      "                                       'setups from Nguyen et al. (2024), and\\n'\n",
      "                                       're-use the reported results of '\n",
      "                                       'baselines by Arora et al. (2024). The '\n",
      "                                       'performance of Titans (LMM) and '\n",
      "                                       'baselines are reported\\n'\n",
      "                                       'in Table 4. We find that LMM is '\n",
      "                                       'competitive with state-of-the-art '\n",
      "                                       'architectures across different '\n",
      "                                       'downstream genomics\\n'\n",
      "                                       'tasks.\\n'\n",
      "                                       '5.8 Efficiency\\n'\n",
      "                                       'Figure 9: Training throughput '\n",
      "                                       'compari-\\n'\n",
      "                                       'son of Titans and baselines.\\n'\n",
      "                                       'In this part, we compare the '\n",
      "                                       'efficiency of our neural memory as '\n",
      "                                       'well as Titans\\n'\n",
      "                                       'with state-of-the-art sequence models. '\n",
      "                                       'The training throughput of models for\\n'\n",
      "                                       'different sequence length ×batch size '\n",
      "                                       'are reported in Figure 9. Comparing\\n'\n",
      "                                       'recurrent models, including our neural '\n",
      "                                       'memory module, we can see our memory\\n'\n",
      "                                       'module is slightly slower than Mamba2 '\n",
      "                                       'and Gated DeltaNet, mainly due to: '\n",
      "                                       '(1)\\n'\n",
      "                                       'having deep memory and more expressive '\n",
      "                                       'transition process (memory update),\\n'\n",
      "                                       'and (2) highly optimized kernel in the '\n",
      "                                       'implementation of Mamba2. '\n",
      "                                       'Interestingly,\\n'\n",
      "                                       'Titans (MAL) are faster than baselines '\n",
      "                                       'as well as the memory module. The\\n'\n",
      "                                       'main reason for this better throughput '\n",
      "                                       'is the highly optimized kernel of '\n",
      "                                       'Flash-\\n'\n",
      "                                       'Attention (Dao 2024), which is used '\n",
      "                                       'for implementing SWA and full '\n",
      "                                       'attention\\n'\n",
      "                                       'module in Titans.\\n'\n",
      "                                       '5.9 Ablation Study\\n'\n",
      "                                       'Finally, we perform ablation studies '\n",
      "                                       'on the different architectural choices '\n",
      "                                       'in Titans. We consider our neural '\n",
      "                                       'memory\\n'\n",
      "                                       'module as a base model and then '\n",
      "                                       'changing one component at a time: (1) '\n",
      "                                       'replacing deep memory with linear '\n",
      "                                       'memory,\\n'\n",
      "                                       'removing (2) convolution, (3) momentum '\n",
      "                                       'in the surprise measure, (4) weight '\n",
      "                                       'decay (or forgot mechanism), and (5) '\n",
      "                                       'persistent\\n'\n",
      "                                       'memory. The results are reported in '\n",
      "                                       'Table 5. All components of neural '\n",
      "                                       'memory design are positively '\n",
      "                                       'contributing to its\\n'\n",
      "                                       'performance, where the greatest '\n",
      "                                       'contribution comes from weight decay, '\n",
      "                                       'momentum, convolution, and persistent '\n",
      "                                       'memory,\\n'\n",
      "                                       'respectively.\\n'\n",
      "                                       'The Effect of Architectural Design. To '\n",
      "                                       'evaluate the effect of architecture '\n",
      "                                       'design, we compare the performance of '\n",
      "                                       'three\\n'\n",
      "                                       'represented variants of Titans in '\n",
      "                                       'three aspects of (i) language '\n",
      "                                       'modeling, (ii) commen-sense reasoning, '\n",
      "                                       'and (iii) long context\\n'\n",
      "                                       'NIAH (BABILong) tasks. The results are '\n",
      "                                       'reported in Table 5. We find that MAC '\n",
      "                                       'and MAG have close performance in\\n'\n",
      "                                       'language modeling and common-sense '\n",
      "                                       'reasoning tasks, while MAC achieve '\n",
      "                                       'significantly better performance in '\n",
      "                                       'long-context\\n'\n",
      "                                       'NIAH. Both of these models achieve '\n",
      "                                       'better performance than MAL. These '\n",
      "                                       'results along with Figure 9, show a '\n",
      "                                       'trade-off\\n'\n",
      "                                       'between fast training and more '\n",
      "                                       'expressive design.\\n'\n",
      "                                       '16\\n'\n",
      "                                       'Table 5: Ablation Study on Titans. All '\n",
      "                                       'components of Titans are positively '\n",
      "                                       'contributing to its performance.\\n'\n",
      "                                       'Model Language Modeling Reasoning Long '\n",
      "                                       'Context\\n'\n",
      "                                       'ppl↓ acc↑ acc↑\\n'\n",
      "                                       'LMM 27.01 47.83 92.68\\n'\n",
      "                                       '+Attn(MAC) 26.67 48.65 97.95\\n'\n",
      "                                       '+Attn(MAG) 25.70 48.60 96.70\\n'\n",
      "                                       '+Attn(MAL) 25.91 47.87 96.91\\n'\n",
      "                                       'Linear Memory 28.49 46.97 85.34\\n'\n",
      "                                       'w/o Convolution 28.73 45.82 90.28\\n'\n",
      "                                       'w/o Momentum 28.98 45.49 87.12\\n'\n",
      "                                       'w/o Weight Decay 29.04 45.11 85.60\\n'\n",
      "                                       'w/o Persistent Memory 27.63 46.35 '\n",
      "                                       '92.49\\n'\n",
      "                                       '6 Conclusion\\n'\n",
      "                                       'In this paper, we present a neural '\n",
      "                                       'long-term memory that, as a meta '\n",
      "                                       'in-context learner, learns to memorize '\n",
      "                                       'at test time.\\n'\n",
      "                                       'The neural memory module is a '\n",
      "                                       'recurrent model in nature, and is '\n",
      "                                       'adaptively memorizing tokens that are '\n",
      "                                       'more surprising\\n'\n",
      "                                       'or are close to surprising tokens. '\n",
      "                                       'Comparing to modern recurrent models, '\n",
      "                                       'it has more expressive memory update '\n",
      "                                       'and\\n'\n",
      "                                       'storing mechanism. Using this memory, '\n",
      "                                       'we present Titans architectures, and '\n",
      "                                       'its three variants, in which we '\n",
      "                                       'suggest to\\n'\n",
      "                                       'incorporate the memory module as (1) a '\n",
      "                                       'context, (2) gating, and (3) a layer. '\n",
      "                                       'Our experimental evaluation on diverse '\n",
      "                                       'tasks\\n'\n",
      "                                       'tasks validate that Titans are more '\n",
      "                                       'effective than Transformers and recent '\n",
      "                                       'modern linear recurrent models, '\n",
      "                                       'specifically for\\n'\n",
      "                                       'long context. That is, Titans can '\n",
      "                                       'scale to larger than 2M context window '\n",
      "                                       'size with better accuracy than '\n",
      "                                       'baselines.\\n'\n",
      "                                       'Titans are implemented in Pytorch and '\n",
      "                                       'JAX and we intend to make the code we '\n",
      "                                       'used to train and evaluate our models\\n'\n",
      "                                       'available soon.\\n'\n",
      "                                       '17\\n'\n",
      "                                       'References\\n'\n",
      "                                       '[1] Josh Achiam, Steven Adler, '\n",
      "                                       'Sandhini Agarwal, Lama Ahmad, Ilge '\n",
      "                                       'Akkaya, Florencia Leoni Aleman, Diogo\\n'\n",
      "                                       'Almeida, Janko Altenschmidt, Sam '\n",
      "                                       'Altman, Shyamal Anadkat, et al. “Gpt-4 '\n",
      "                                       'technical report”. In: arXiv preprint\\n'\n",
      "                                       'arXiv:2303.08774 (2023).\\n'\n",
      "                                       '[2] Yaroslav Aksenov, Nikita '\n",
      "                                       'Balagansky, Sofia Maria Lo Cicero '\n",
      "                                       'Vaina, Boris Shaposhnikov, Alexey '\n",
      "                                       'Gorbatovski, and\\n'\n",
      "                                       'Daniil Gavrilov. “Linear Transformers '\n",
      "                                       'with Learnable Kernel Functions are '\n",
      "                                       'Better In-Context Models”. In: arXiv\\n'\n",
      "                                       'preprint arXiv:2402.10644 (2024).\\n'\n",
      "                                       '[3] Marcin Andrychowicz, Misha Denil, '\n",
      "                                       'Sergio Gomez, Matthew W Hoffman, David '\n",
      "                                       'Pfau, Tom Schaul, Brendan\\n'\n",
      "                                       'Shillingford, and Nando De Freitas. '\n",
      "                                       '“Learning to learn by gradient descent '\n",
      "                                       'by gradient descent”. In: Advances in\\n'\n",
      "                                       'neural information processing systems '\n",
      "                                       '29 (2016).\\n'\n",
      "                                       '[4] Cem Anil, Yuhuai Wu, Anders '\n",
      "                                       'Andreassen, Aitor Lewkowycz, Vedant '\n",
      "                                       'Misra, Vinay Ramasesh, Ambrose Slone,\\n'\n",
      "                                       'Guy Gur-Ari, Ethan Dyer, and Behnam '\n",
      "                                       'Neyshabur. “Exploring length '\n",
      "                                       'generalization in large language '\n",
      "                                       'models”. In:\\n'\n",
      "                                       'Advances in Neural Information '\n",
      "                                       'Processing Systems 35 (2022), pp. '\n",
      "                                       '38546–38556.\\n'\n",
      "                                       '[5] Simran Arora, Sabri Eyuboglu, '\n",
      "                                       'Michael Zhang, Aman Timalsina, Silas '\n",
      "                                       'Alberti, James Zou, Atri Rudra, and '\n",
      "                                       'Christo-\\n'\n",
      "                                       'pher Re. “Simple linear attention '\n",
      "                                       'language models balance the '\n",
      "                                       'recall-throughput tradeoff”. '\n",
      "                                       'In:Forty-first International\\n'\n",
      "                                       'Conference on Machine Learning . 2024. '\n",
      "                                       'url: '\n",
      "                                       'https://openreview.net/forum?id=e93ffDcpH3.\\n'\n",
      "                                       '[6] Dzmitry Bahdanau. “Neural machine '\n",
      "                                       'translation by jointly learning to '\n",
      "                                       'align and translate”. In: arXiv '\n",
      "                                       'preprint\\n'\n",
      "                                       'arXiv:1409.0473 (2014).\\n'\n",
      "                                       '[7] Reza Bayat, Mohammad Pezeshki, '\n",
      "                                       'Elvis Dohmatob, David Lopez-Paz, and '\n",
      "                                       'Pascal Vincent. “The Pitfalls of '\n",
      "                                       'Memo-\\n'\n",
      "                                       'rization: When Memorization Hurts '\n",
      "                                       'Generalization”. In: arXiv preprint '\n",
      "                                       'arXiv:2412.07684 (2024).\\n'\n",
      "                                       '[8] Maximilian Beck, Korbinian Pöppel, '\n",
      "                                       'Markus Spanring, Andreas Auer, '\n",
      "                                       'Oleksandra Prudnikova, Michael Kopp,\\n'\n",
      "                                       'Günter Klambauer, Johannes '\n",
      "                                       'Brandstetter, and Sepp Hochreiter. '\n",
      "                                       '“xLSTM: Extended Long Short-Term '\n",
      "                                       'Memory”. In:\\n'\n",
      "                                       'arXiv preprint arXiv:2405.04517 '\n",
      "                                       '(2024).\\n'\n",
      "                                       '[9] Ali Behrouz, Michele '\n",
      "                                       'Santacatterina, and Ramin Zabih. '\n",
      "                                       '“Mambamixer: Efficient selective state '\n",
      "                                       'space models with\\n'\n",
      "                                       'dual token and channel selection”. In: '\n",
      "                                       'arXiv preprint arXiv:2403.19888 '\n",
      "                                       '(2024).\\n'\n",
      "                                       '[10] Vincent-Pierre Berges, Barlas '\n",
      "                                       'Oğuz, Daniel Haziza, Wen-tau Yih, Luke '\n",
      "                                       'Zettlemoyer, and Gargi Gosh. “Memory\\n'\n",
      "                                       'Layers at Scale”. In: arXiv preprint '\n",
      "                                       'arXiv:2412.09764 (2024).\\n'\n",
      "                                       '[11] Alberto Bietti, Vivien Cabannes, '\n",
      "                                       'Diane Bouchacourt, Herve Jegou, and '\n",
      "                                       'Leon Bottou. “Birth of a transformer: '\n",
      "                                       'A\\n'\n",
      "                                       'memory viewpoint”. In: Advances in '\n",
      "                                       'Neural Information Processing Systems '\n",
      "                                       '36 (2024).\\n'\n",
      "                                       '[12] Yonatan Bisk, Rowan Zellers, '\n",
      "                                       'Jianfeng Gao, Yejin Choi, et al. '\n",
      "                                       '“Piqa: Reasoning about physical '\n",
      "                                       'commonsense in\\n'\n",
      "                                       'natural language”. In: Proceedings of '\n",
      "                                       'the AAAI conference on artificial '\n",
      "                                       'intelligence . Vol. 34. 05. 2020, pp. '\n",
      "                                       '7432–7439.\\n'\n",
      "                                       '[13] Aleksandar Botev, Soham De, '\n",
      "                                       'Samuel L Smith, Anushan Fernando, '\n",
      "                                       'George-Cristian Muraru, Ruba Haroun, '\n",
      "                                       'Leonard\\n'\n",
      "                                       'Berrada, Razvan Pascanu, Pier Giuseppe '\n",
      "                                       'Sessa, Robert Dadashi, et al. '\n",
      "                                       '“RecurrentGemma: Moving Past '\n",
      "                                       'Transformers\\n'\n",
      "                                       'for Efficient Open Language Models”. '\n",
      "                                       'In: arXiv preprint arXiv:2404.07839 '\n",
      "                                       '(2024).\\n'\n",
      "                                       '[14] Léon Bottou and Vladimir Vapnik. '\n",
      "                                       '“Local learning algorithms”. In: '\n",
      "                                       'Neural computation 4.6 (1992), pp. '\n",
      "                                       '888–900.\\n'\n",
      "                                       '[15] Aydar Bulatov, Yuri Kuratov, '\n",
      "                                       'Yermek Kapushev, and Mikhail S '\n",
      "                                       'Burtsev. “Scaling transformer to 1m '\n",
      "                                       'tokens and\\n'\n",
      "                                       'beyond with rmt”. In: arXiv preprint '\n",
      "                                       'arXiv:2304.11062 (2023).\\n'\n",
      "                                       '[16] Aydar Bulatov, Yury Kuratov, and '\n",
      "                                       'Mikhail Burtsev. “Recurrent memory '\n",
      "                                       'transformer”. In: Advances in Neural\\n'\n",
      "                                       'Information Processing Systems 35 '\n",
      "                                       '(2022), pp. 11079–11091.\\n'\n",
      "                                       '[17] Edoardo Cetin, Qi Sun, Tianyu '\n",
      "                                       'Zhao, and Yujin Tang. “An Evolved '\n",
      "                                       'Universal Transformer Memory”. In: '\n",
      "                                       'arXiv\\n'\n",
      "                                       'preprint arXiv:2410.13166 (2024).\\n'\n",
      "                                       '[18] Beidi Chen, Tri Dao, Eric Winsor, '\n",
      "                                       'Zhao Song, Atri Rudra, and Christopher '\n",
      "                                       'Ré. “Scatterbrain: Unifying sparse '\n",
      "                                       'and\\n'\n",
      "                                       'low-rank attention”. In: Advances in '\n",
      "                                       'Neural Information Processing Systems '\n",
      "                                       '34 (2021), pp. 17413–17426.\\n'\n",
      "                                       '[19] Krzysztof Marcin Choromanski, '\n",
      "                                       'Valerii Likhosherstov, David Dohan, '\n",
      "                                       'Xingyou Song, Andreea Gane, Tamas '\n",
      "                                       'Sarlos,\\n'\n",
      "                                       'Peter Hawkins, Jared Quincy Davis, '\n",
      "                                       'Afroz Mohiuddin, Lukasz Kaiser, David '\n",
      "                                       'Benjamin Belanger, Lucy J Colwell, '\n",
      "                                       'and\\n'\n",
      "                                       'Adrian Weller. “Rethinking Attention '\n",
      "                                       'with Performers”. In: International '\n",
      "                                       'Conference on Learning Representations '\n",
      "                                       '.\\n'\n",
      "                                       '2021. url: '\n",
      "                                       'https://openreview.net/forum?id=Ua6zuk0WRH.\\n'\n",
      "                                       '[20] Christopher Clark, Kenton Lee, '\n",
      "                                       'Ming-Wei Chang, Tom Kwiatkowski, '\n",
      "                                       'Michael Collins, and Kristina '\n",
      "                                       'Toutanova.\\n'\n",
      "                                       '“BoolQ: Exploring the Surprising '\n",
      "                                       'Difficulty of Natural Yes/No '\n",
      "                                       'Questions”. In: Proceedings of the '\n",
      "                                       '2019 Conference\\n'\n",
      "                                       'of the North American Chapter of the '\n",
      "                                       'Association for Computational '\n",
      "                                       'Linguistics: Human Language '\n",
      "                                       'Technologies,\\n'\n",
      "                                       'Volume 1 (Long and Short Papers) . Ed. '\n",
      "                                       'by Jill Burstein, Christy Doran, and '\n",
      "                                       'Thamar Solorio. Minneapolis, '\n",
      "                                       'Minnesota:\\n'\n",
      "                                       'Association for Computational '\n",
      "                                       'Linguistics, June 2019, pp. 2924–2936. '\n",
      "                                       'doi: 10.18653/v1/N19-1300. url: '\n",
      "                                       'https:\\n'\n",
      "                                       '//aclanthology.org/N19-1300/.\\n'\n",
      "                                       '18\\n'\n",
      "                                       '[21] Peter Clark, Isaac Cowhey, Oren '\n",
      "                                       'Etzioni, Tushar Khot, Ashish '\n",
      "                                       'Sabharwal, Carissa Schoenick, and '\n",
      "                                       'Oyvind Tafjord.\\n'\n",
      "                                       '“Think you have solved question '\n",
      "                                       'answering? try arc, the ai2 reasoning '\n",
      "                                       'challenge”. In:arXiv preprint '\n",
      "                                       'arXiv:1803.05457\\n'\n",
      "                                       '(2018).\\n'\n",
      "                                       '[22] Nelson Cowan. “What are the '\n",
      "                                       'differences between long-term, '\n",
      "                                       'short-term, and working memory?” In: '\n",
      "                                       'Progress in\\n'\n",
      "                                       'brain research 169 (2008), pp. '\n",
      "                                       '323–338.\\n'\n",
      "                                       '[23] Zihang Dai, Zhilin Yang, Yiming '\n",
      "                                       'Yang, Jaime G. Carbonell, Quoc Viet '\n",
      "                                       'Le, and Ruslan Salakhutdinov. '\n",
      "                                       '“Transformer-\\n'\n",
      "                                       'XL: Attentive Language Models beyond a '\n",
      "                                       'Fixed-Length Context”. In: ACL (1). '\n",
      "                                       'Ed. by Anna Korhonen, David R.\\n'\n",
      "                                       'Traum, and Lluís Màrquez. Association '\n",
      "                                       'for Computational Linguistics, 2019, '\n",
      "                                       'pp. 2978–2988.isbn: '\n",
      "                                       '978-1-950737-48-2.\\n'\n",
      "                                       '[24] Tri Dao. “FlashAttention-2: '\n",
      "                                       'Faster Attention with Better '\n",
      "                                       'Parallelism and Work Partitioning”. '\n",
      "                                       'In: The Twelfth Inter-\\n'\n",
      "                                       'national Conference on Learning '\n",
      "                                       'Representations . 2024. url: '\n",
      "                                       'https://openreview.net/forum?id=mZn2Xyh9Ec.\\n'\n",
      "                                       '[25] Tri Dao, Dan Fu, Stefano Ermon, '\n",
      "                                       'Atri Rudra, and Christopher Ré. '\n",
      "                                       '“FlashAttention: Fast and '\n",
      "                                       'Memory-Efficient\\n'\n",
      "                                       'Exact Attention with IO-Awareness”. '\n",
      "                                       'In:Advances in Neural Information '\n",
      "                                       'Processing Systems . Ed. by S. Koyejo, '\n",
      "                                       'S.\\n'\n",
      "                                       'Mohamed, A. Agarwal, D. Belgrave, K. '\n",
      "                                       'Cho, and A. Oh. Vol. 35. Curran '\n",
      "                                       'Associates, Inc., 2022, pp. '\n",
      "                                       '16344–16359. url:\\n'\n",
      "                                       'https://proceedings.neurips.cc/paper_files/paper/2022/file/67d57c32e20fd0a7a302cb81d36e40d5-\\n'\n",
      "                                       'Paper-Conference.pdf.\\n'\n",
      "                                       '[26] Tri Dao and Albert Gu. '\n",
      "                                       '“Transformers are SSMs: Generalized '\n",
      "                                       'models and efficient algorithms '\n",
      "                                       'through structured\\n'\n",
      "                                       'state space duality”. In: arXiv '\n",
      "                                       'preprint arXiv:2405.21060 (2024).\\n'\n",
      "                                       '[27] Abhimanyu Das, Weihao Kong, '\n",
      "                                       'Andrew Leach, Shaan K Mathur, Rajat '\n",
      "                                       'Sen, and Rose Yu. “Long-term '\n",
      "                                       'Forecasting\\n'\n",
      "                                       'with TiDE: Time-series Dense Encoder”. '\n",
      "                                       'In: Transactions on Machine Learning '\n",
      "                                       'Research (2023). issn: 2835-8856. '\n",
      "                                       'url:\\n'\n",
      "                                       'https://openreview.net/forum?id=pCbC3aQB5W.\\n'\n",
      "                                       '[28] Soham De, Samuel L Smith, Anushan '\n",
      "                                       'Fernando, Aleksandar Botev, George '\n",
      "                                       'Cristian-Muraru, Albert Gu, Ruba\\n'\n",
      "                                       'Haroun, Leonard Berrada, Yutian Chen, '\n",
      "                                       'Srivatsan Srinivasan, et al. “Griffin: '\n",
      "                                       'Mixing gated linear recurrences with\\n'\n",
      "                                       'local attention for efficient language '\n",
      "                                       'models”. In: arXiv preprint '\n",
      "                                       'arXiv:2402.19427 (2024).\\n'\n",
      "                                       '[29] Juechu Dong, Boyuan Feng, Driss '\n",
      "                                       'Guessous, Yanbo Liang, and Horace He. '\n",
      "                                       '“Flex Attention: A Programming Model\\n'\n",
      "                                       'for Generating Optimized Attention '\n",
      "                                       'Kernels”. In: arXiv preprint '\n",
      "                                       'arXiv:2412.05496 (2024).\\n'\n",
      "                                       '[30] Xin Dong, Yonggan Fu, Shizhe '\n",
      "                                       'Diao, Wonmin Byeon, Zijia Chen, Ameya '\n",
      "                                       'Sunil Mahabaleshwarkar, Shih-Yang '\n",
      "                                       'Liu,\\n'\n",
      "                                       'Matthijs Van Keirsbilck, Min-Hung '\n",
      "                                       'Chen, Yoshi Suhara, et al. “Hymba: A '\n",
      "                                       'Hybrid-head Architecture for Small\\n'\n",
      "                                       'Language Models”. In: arXiv preprint '\n",
      "                                       'arXiv:2411.13676 (2024).\\n'\n",
      "                                       '[31] Stefan Elfwing, Eiji Uchibe, and '\n",
      "                                       'Kenji Doya. “Sigmoid-weighted linear '\n",
      "                                       'units for neural network function '\n",
      "                                       'approxi-\\n'\n",
      "                                       'mation in reinforcement learning”. In: '\n",
      "                                       'Neural networks 107 (2018), pp. 3–11.\\n'\n",
      "                                       '[32] Yukun Feng, Feng Li, Ziang Song, '\n",
      "                                       'Boyuan Zheng, and Philipp Koehn. '\n",
      "                                       '“Learn to remember: Transformer with\\n'\n",
      "                                       'recurrent memory for document-level '\n",
      "                                       'machine translation”. In: arXiv '\n",
      "                                       'preprint arXiv:2205.01546 (2022).\\n'\n",
      "                                       '[33] Daniel Y Fu, Tri Dao, Khaled '\n",
      "                                       'Kamal Saab, Armin W Thomas, Atri '\n",
      "                                       'Rudra, and Christopher Re. “Hungry '\n",
      "                                       'Hungry\\n'\n",
      "                                       'Hippos: Towards Language Modeling with '\n",
      "                                       'State Space Models”. In:The Eleventh '\n",
      "                                       'International Conference on Learning\\n'\n",
      "                                       'Representations. 2023. url: '\n",
      "                                       'https://openreview.net/forum?id=COZDy0WYGg.\\n'\n",
      "                                       '[34] Yossi Gandelsman, Yu Sun, Xinlei '\n",
      "                                       'Chen, and Alexei Efros. “Test-time '\n",
      "                                       'training with masked autoencoders”. '\n",
      "                                       'In:\\n'\n",
      "                                       'Advances in Neural Information '\n",
      "                                       'Processing Systems 35 (2022), pp. '\n",
      "                                       '29374–29385.\\n'\n",
      "                                       '[35] Leo Gao, Stella Biderman, Sid '\n",
      "                                       'Black, Laurence Golding, Travis Hoppe, '\n",
      "                                       'Charles Foster, Jason Phang, Horace '\n",
      "                                       'He,\\n'\n",
      "                                       'Anish Thite, Noa Nabeshima, et al. '\n",
      "                                       '“The pile: An 800gb dataset of diverse '\n",
      "                                       'text for language modeling”. In: '\n",
      "                                       'arXiv\\n'\n",
      "                                       'preprint arXiv:2101.00027 (2020).\\n'\n",
      "                                       '[36] Felix A Gers, Jürgen Schmidhuber, '\n",
      "                                       'and Fred Cummins. “Learning to forget: '\n",
      "                                       'Continual prediction with LSTM”. In:\\n'\n",
      "                                       'Neural computation 12.10 (2000), pp. '\n",
      "                                       '2451–2471.\\n'\n",
      "                                       '[37] Alex Graves, Greg Wayne, and Ivo '\n",
      "                                       'Danihelka. Neural Turing Machines . '\n",
      "                                       '2014. arXiv: 1410.5401 [cs.NE]. url:\\n'\n",
      "                                       'https://arxiv.org/abs/1410.5401.\\n'\n",
      "                                       '[38] Klaus Greff, Rupesh K Srivastava, '\n",
      "                                       'Jan Koutník, Bas R Steunebrink, and '\n",
      "                                       'Jürgen Schmidhuber. “LSTM: A search '\n",
      "                                       'space\\n'\n",
      "                                       'odyssey”. In: IEEE transactions on '\n",
      "                                       'neural networks and learning systems '\n",
      "                                       '28.10 (2016), pp. 2222–2232.\\n'\n",
      "                                       '[39] Katarína Grešová, Vlastimil '\n",
      "                                       'Martinek, David Čechák, Petr Šimeček, '\n",
      "                                       'and Panagiotis Alexiou. “Genomic '\n",
      "                                       'benchmarks:\\n'\n",
      "                                       'a collection of datasets for genomic '\n",
      "                                       'sequence classification”. In: BMC '\n",
      "                                       'Genomic Data 24.1 (2023), p. 25.\\n'\n",
      "                                       '[40] Albert Gu and Tri Dao. “Mamba: '\n",
      "                                       'Linear-Time Sequence Modeling with '\n",
      "                                       'Selective State Spaces”. In: First '\n",
      "                                       'Conference\\n'\n",
      "                                       'on Language Modeling . 2024. url: '\n",
      "                                       'https://openreview.net/forum?id=tEYskw1VY2.\\n'\n",
      "                                       '[41] Albert Gu, Karan Goel, and '\n",
      "                                       'Christopher Re. “Efficiently Modeling '\n",
      "                                       'Long Sequences with Structured State '\n",
      "                                       'Spaces”.\\n'\n",
      "                                       'In: International Conference on '\n",
      "                                       'Learning Representations . 2022. url: '\n",
      "                                       'https : / / openreview . net / forum ? '\n",
      "                                       'id =\\n'\n",
      "                                       'uYLFoz1vlAC.\\n'\n",
      "                                       '19\\n'\n",
      "                                       '[42] Chi Han, Qifan Wang, Hao Peng, '\n",
      "                                       'Wenhan Xiong, Yu Chen, Heng Ji, and '\n",
      "                                       'Sinong Wang. “LM-Infinite: Zero-Shot\\n'\n",
      "                                       'Extreme Length Generalization for '\n",
      "                                       'Large Language Models”. In: '\n",
      "                                       'Proceedings of the 2024 Conference of '\n",
      "                                       'the North\\n'\n",
      "                                       'American Chapter of the Association '\n",
      "                                       'for Computational Linguistics: Human '\n",
      "                                       'Language Technologies (Volume 1: Long\\n'\n",
      "                                       'Papers). Ed. by Kevin Duh, Helena '\n",
      "                                       'Gomez, and Steven Bethard. Mexico '\n",
      "                                       'City, Mexico: Association for '\n",
      "                                       'Computational\\n'\n",
      "                                       'Linguistics, June 2024, pp. 3991–4008. '\n",
      "                                       'doi: 10.18653/v1/2024.naacl-long.222. '\n",
      "                                       'url: https://aclanthology.\\n'\n",
      "                                       'org/2024.naacl-long.222.\\n'\n",
      "                                       '[43] Ramin Hasani, Mathias Lechner, '\n",
      "                                       'Tsun-Hsuan Wang, Makram Chahine, '\n",
      "                                       'Alexander Amini, and Daniela Rus. '\n",
      "                                       '“Liquid\\n'\n",
      "                                       'Structural State-Space Models”. In: '\n",
      "                                       'The Eleventh International Conference '\n",
      "                                       'on Learning Representations . 2023. '\n",
      "                                       'url:\\n'\n",
      "                                       'https://openreview.net/forum?id=g4OTKRKfS7R.\\n'\n",
      "                                       '[44] Zexue He, Leonid Karlinsky, '\n",
      "                                       'Donghyun Kim, Julian McAuley, Dmitry '\n",
      "                                       'Krotov, and Rogerio Feris. “CAMELoT:\\n'\n",
      "                                       'Towards Large Language Models with '\n",
      "                                       'Training-Free Consolidated Associative '\n",
      "                                       'Memory”. In: arXiv preprint\\n'\n",
      "                                       'arXiv:2402.13449 (2024).\\n'\n",
      "                                       '[45] Donald Olding Hebb. The '\n",
      "                                       'organization of behavior: A '\n",
      "                                       'neuropsychological theory . Psychology '\n",
      "                                       'press, 2005.\\n'\n",
      "                                       '[46] John J Hopfield. “Neural networks '\n",
      "                                       'and physical systems with emergent '\n",
      "                                       'collective computational abilities.” '\n",
      "                                       'In:\\n'\n",
      "                                       'Proceedings of the national academy of '\n",
      "                                       'sciences 79.8 (1982), pp. 2554–2558.\\n'\n",
      "                                       '[47] Kurt Hornik, Maxwell Stinchcombe, '\n",
      "                                       'and Halbert White. “Multilayer '\n",
      "                                       'feedforward networks are universal '\n",
      "                                       'approxi-\\n'\n",
      "                                       'mators”. In: Neural networks 2.5 '\n",
      "                                       '(1989), pp. 359–366.\\n'\n",
      "                                       '[48] Cheng-Ping Hsieh, Simeng Sun, '\n",
      "                                       'Samuel Kriman, Shantanu Acharya, Dima '\n",
      "                                       'Rekesh, Fei Jia, and Boris Ginsburg.\\n'\n",
      "                                       '“RULER: What’s the Real Context Size '\n",
      "                                       'of Your Long-Context Language Models?” '\n",
      "                                       'In: First Conference on Language\\n'\n",
      "                                       'Modeling. 2024. url: '\n",
      "                                       'https://openreview.net/forum?id=kIoBbc76Sy.\\n'\n",
      "                                       '[49] DeLesley Hutchins, Imanol Schlag, '\n",
      "                                       'Yuhuai Wu, Ethan Dyer, and Behnam '\n",
      "                                       'Neyshabur. “Block-recurrent '\n",
      "                                       'transformers”.\\n'\n",
      "                                       'In: Advances in neural information '\n",
      "                                       'processing systems 35 (2022), pp. '\n",
      "                                       '33248–33261.\\n'\n",
      "                                       '[50] Kazuki Irie, Róbert Csordás, and '\n",
      "                                       'Jürgen Schmidhuber. “The dual form of '\n",
      "                                       'neural networks revisited: Connecting '\n",
      "                                       'test\\n'\n",
      "                                       'time predictions to training patterns '\n",
      "                                       'via spotlights of attention”. In: '\n",
      "                                       'International Conference on Machine '\n",
      "                                       'Learning .\\n'\n",
      "                                       'PMLR. 2022, pp. 9639–9659.\\n'\n",
      "                                       '[51] Kazuki Irie, Imanol Schlag, '\n",
      "                                       'Róbert Csordás, and Jürgen '\n",
      "                                       'Schmidhuber. “Going beyond linear '\n",
      "                                       'transformers with\\n'\n",
      "                                       'recurrent fast weight programmers”. '\n",
      "                                       'In: Advances in neural information '\n",
      "                                       'processing systems 34 (2021), pp. '\n",
      "                                       '7703–7717.\\n'\n",
      "                                       '[52] Vidit Jain and Erik '\n",
      "                                       'Learned-Miller. “Online domain '\n",
      "                                       'adaptation of a pre-trained cascade of '\n",
      "                                       'classifiers”. In: CVPR\\n'\n",
      "                                       '2011. IEEE. 2011, pp. 577–584.\\n'\n",
      "                                       '[53] Albert Q Jiang, Alexandre '\n",
      "                                       'Sablayrolles, Arthur Mensch, Chris '\n",
      "                                       'Bamford, Devendra Singh Chaplot, Diego '\n",
      "                                       'de las\\n'\n",
      "                                       'Casas, Florian Bressand, Gianna '\n",
      "                                       'Lengyel, Guillaume Lample, Lucile '\n",
      "                                       'Saulnier, et al. “Mistral 7B”. In: '\n",
      "                                       'arXiv preprint\\n'\n",
      "                                       'arXiv:2310.06825 (2023).\\n'\n",
      "                                       '[54] Praneeth Kacham, Vahab Mirrokni, '\n",
      "                                       'and Peilin Zhong. “PolySketchFormer: '\n",
      "                                       'Fast Transformers via Sketching '\n",
      "                                       'Polyno-\\n'\n",
      "                                       'mial Kernels”. In: Forty-first '\n",
      "                                       'International Conference on Machine '\n",
      "                                       'Learning . 2024. url: '\n",
      "                                       'https://openreview.net/\\n'\n",
      "                                       'forum?id=ghYrfdJfjK.\\n'\n",
      "                                       '[55] Jared Kaplan, Sam McCandlish, Tom '\n",
      "                                       'Henighan, Tom B Brown, Benjamin Chess, '\n",
      "                                       'Rewon Child, Scott Gray, Alec\\n'\n",
      "                                       'Radford, Jeffrey Wu, and Dario Amodei. '\n",
      "                                       '“Scaling laws for neural language '\n",
      "                                       'models”. In:arXiv preprint '\n",
      "                                       'arXiv:2001.08361\\n'\n",
      "                                       '(2020).\\n'\n",
      "                                       '[56] Angelos Katharopoulos, Apoorv '\n",
      "                                       'Vyas, Nikolaos Pappas, and François '\n",
      "                                       'Fleuret. “Transformers are rnns: Fast '\n",
      "                                       'au-\\n'\n",
      "                                       'toregressive transformers with linear '\n",
      "                                       'attention”. In: International '\n",
      "                                       'conference on machine learning . PMLR. '\n",
      "                                       '2020,\\n'\n",
      "                                       'pp. 5156–5165.\\n'\n",
      "                                       '[57] Urvashi Khandelwal, Omer Levy, '\n",
      "                                       'Dan Jurafsky, Luke Zettlemoyer, and '\n",
      "                                       'Mike Lewis. “Generalization through\\n'\n",
      "                                       'Memorization: Nearest Neighbor '\n",
      "                                       'Language Models”. In: International '\n",
      "                                       'Conference on Learning Representations '\n",
      "                                       '. 2020.\\n'\n",
      "                                       'url: '\n",
      "                                       'https://openreview.net/forum?id=HklBjCEKvH.\\n'\n",
      "                                       '[58] Yuri Kuratov, Aydar Bulatov, Petr '\n",
      "                                       'Anokhin, Ivan Rodkin, Dmitry Igorevich '\n",
      "                                       'Sorokin, Artyom Sorokin, and Mikhail\\n'\n",
      "                                       'Burtsev. “BABILong: Testing the Limits '\n",
      "                                       'of LLMs with Long Context '\n",
      "                                       'Reasoning-in-a-Haystack”. In: The '\n",
      "                                       'Thirty-\\n'\n",
      "                                       'eight Conference on Neural Information '\n",
      "                                       'Processing Systems Datasets and '\n",
      "                                       'Benchmarks Track . 2024. url: https:\\n'\n",
      "                                       '//openreview.net/forum?id=u7m2CG84BQ.\\n'\n",
      "                                       '[59] Hung Le, Truyen Tran, and Svetha '\n",
      "                                       'Venkatesh. “Self-attentive associative '\n",
      "                                       'memory”. In:International conference '\n",
      "                                       'on\\n'\n",
      "                                       'machine learning . PMLR. 2020, pp. '\n",
      "                                       '5682–5691.\\n'\n",
      "                                       '[60] Patrick Lewis, Ethan Perez, '\n",
      "                                       'Aleksandra Piktus, Fabio Petroni, '\n",
      "                                       'Vladimir Karpukhin, Naman Goyal, '\n",
      "                                       'Heinrich Küttler,\\n'\n",
      "                                       'Mike Lewis, Wen-tau Yih, Tim '\n",
      "                                       'Rocktäschel, et al. '\n",
      "                                       '“Retrieval-augmented generation for '\n",
      "                                       'knowledge-intensive nlp\\n'\n",
      "                                       'tasks”. In: Advances in Neural '\n",
      "                                       'Information Processing Systems 33 '\n",
      "                                       '(2020), pp. 9459–9474.\\n'\n",
      "                                       '20\\n'\n",
      "                                       '[61] Danny Leybzon and Corentin '\n",
      "                                       'Kervadec. “Learning, Forgetting, '\n",
      "                                       'Remembering: Insights From Tracking '\n",
      "                                       'LLM Mem-\\n'\n",
      "                                       'orization During Training”. In: '\n",
      "                                       'Proceedings of the 7th BlackboxNLP '\n",
      "                                       'Workshop: Analyzing and Interpreting '\n",
      "                                       'Neural\\n'\n",
      "                                       'Networks for NLP . 2024, pp. 43–57.\\n'\n",
      "                                       '[62] Zhe Li, Shiyi Qi, Yiduo Li, and '\n",
      "                                       'Zenglin Xu. “Revisiting long-term time '\n",
      "                                       'series forecasting: An investigation '\n",
      "                                       'on linear\\n'\n",
      "                                       'mapping”. In: arXiv preprint '\n",
      "                                       'arXiv:2305.10721 (2023).\\n'\n",
      "                                       '[63] Bo Liu, Rui Wang, Lemeng Wu, '\n",
      "                                       'Yihao Feng, Peter Stone, and Qiang '\n",
      "                                       'Liu. “Longhorn: State space models are '\n",
      "                                       'amortized\\n'\n",
      "                                       'online learners”. In: arXiv preprint '\n",
      "                                       'arXiv:2407.14207 (2024).\\n'\n",
      "                                       '[64] Nelson F Liu, Kevin Lin, John '\n",
      "                                       'Hewitt, Ashwin Paranjape, Michele '\n",
      "                                       'Bevilacqua, Fabio Petroni, and Percy '\n",
      "                                       'Liang.\\n'\n",
      "                                       '“Lost in the middle: How language '\n",
      "                                       'models use long contexts”. In: '\n",
      "                                       'Transactions of the Association for '\n",
      "                                       'Computational\\n'\n",
      "                                       'Linguistics 12 (2024), pp. 157–173.\\n'\n",
      "                                       '[65] Yong Liu, Tengge Hu, Haoran '\n",
      "                                       'Zhang, Haixu Wu, Shiyu Wang, Lintao '\n",
      "                                       'Ma, and Mingsheng Long. '\n",
      "                                       '“itransformer:\\n'\n",
      "                                       'Inverted transformers are effective '\n",
      "                                       'for time series forecasting”. In: '\n",
      "                                       'arXiv preprint arXiv:2310.06625 '\n",
      "                                       '(2023).\\n'\n",
      "                                       '[66] George Mandler. “The structure of '\n",
      "                                       'value: Accounting for taste”. In: '\n",
      "                                       'Affect and cognition . Psychology '\n",
      "                                       'Press, 2014,\\n'\n",
      "                                       'pp. 3–36.\\n'\n",
      "                                       '[67] Harsh Mehta, Ankit Gupta, Ashok '\n",
      "                                       'Cutkosky, and Behnam Neyshabur. “Long '\n",
      "                                       'Range Language Modeling via\\n'\n",
      "                                       'Gated State Spaces”. In: The Eleventh '\n",
      "                                       'International Conference on Learning '\n",
      "                                       'Representations . 2023. url: https:\\n'\n",
      "                                       '//openreview.net/forum?id=5MkYIYCbva.\\n'\n",
      "                                       '[68] Stephen Merity, Caiming Xiong, '\n",
      "                                       'James Bradbury, and Richard Socher. '\n",
      "                                       '“Pointer Sentinel Mixture Models”. '\n",
      "                                       'In:\\n'\n",
      "                                       'International Conference on Learning '\n",
      "                                       'Representations . 2017. url: '\n",
      "                                       'https://openreview.net/forum?id=Byj72udxe.\\n'\n",
      "                                       '[69] William Merrill, Jackson Petty, '\n",
      "                                       'and Ashish Sabharwal. “The Illusion of '\n",
      "                                       'State in State-Space Models”. In: '\n",
      "                                       'Forty-first\\n'\n",
      "                                       'International Conference on Machine '\n",
      "                                       'Learning . 2024. url: '\n",
      "                                       'https://openreview.net/forum?id=QZgo9JZpLq.\\n'\n",
      "                                       '[70] Ravi Teja Mullapudi, Steven Chen, '\n",
      "                                       'Keyi Zhang, Deva Ramanan, and Kayvon '\n",
      "                                       'Fatahalian. “Online model '\n",
      "                                       'distillation\\n'\n",
      "                                       'for efficient video inference”. In: '\n",
      "                                       'Proceedings of the IEEE/CVF '\n",
      "                                       'International conference on computer '\n",
      "                                       'vision . 2019,\\n'\n",
      "                                       'pp. 3573–3582.\\n'\n",
      "                                       '[71] Tsendsuren Munkhdalai, Manaal '\n",
      "                                       'Faruqui, and Siddharth Gopal. “Leave '\n",
      "                                       'no context behind: Efficient infinite '\n",
      "                                       'context\\n'\n",
      "                                       'transformers with infini-attention”. '\n",
      "                                       'In: arXiv preprint arXiv:2404.07143 '\n",
      "                                       '(2024).\\n'\n",
      "                                       '[72] Tsendsuren Munkhdalai, Alessandro '\n",
      "                                       'Sordoni, Tong Wang, and Adam '\n",
      "                                       'Trischler. “Metalearned neural '\n",
      "                                       'memory”. In:\\n'\n",
      "                                       'Advances in Neural Information '\n",
      "                                       'Processing Systems 32 (2019).\\n'\n",
      "                                       '[73] Tsendsuren Munkhdalai and Hong '\n",
      "                                       'Yu. “Neural semantic encoders”. In: '\n",
      "                                       'Proceedings of the conference. '\n",
      "                                       'Association for\\n'\n",
      "                                       'Computational Linguistics. Meeting . '\n",
      "                                       'Vol. 1. NIH Public Access. 2017, p. '\n",
      "                                       '397.\\n'\n",
      "                                       '[74] Eric Nguyen, Michael Poli, Marjan '\n",
      "                                       'Faizi, Armin Thomas, Michael Wornow, '\n",
      "                                       'Callum Birch-Sykes, Stefano '\n",
      "                                       'Massaroli,\\n'\n",
      "                                       'Aman Patel, Clayton Rabideau, Yoshua '\n",
      "                                       'Bengio, et al. “Hyenadna: Long-range '\n",
      "                                       'genomic sequence modeling at single\\n'\n",
      "                                       'nucleotide resolution”. In: Advances '\n",
      "                                       'in neural information processing '\n",
      "                                       'systems 36 (2024).\\n'\n",
      "                                       '[75] A Nichol. “On first-order '\n",
      "                                       'meta-learning algorithms”. In: arXiv '\n",
      "                                       'preprint arXiv:1803.02999 (2018).\\n'\n",
      "                                       '[76] Yuqi Nie, Nam H Nguyen, Phanwadee '\n",
      "                                       'Sinthong, and Jayant Kalagnanam. “A '\n",
      "                                       'time series is worth 64 words:\\n'\n",
      "                                       'Long-term forecasting with '\n",
      "                                       'transformers”. In: arXiv preprint '\n",
      "                                       'arXiv:2211.14730 (2022).\\n'\n",
      "                                       '[77] Hideyuki Okano, Tomoo Hirano, and '\n",
      "                                       'Evan Balaban. “Learning and memory”. '\n",
      "                                       'In:Proceedings of the National '\n",
      "                                       'Academy\\n'\n",
      "                                       'of Sciences 97.23 (2000), pp. '\n",
      "                                       '12403–12404.\\n'\n",
      "                                       '[78] Antonio Orvieto, Samuel L Smith, '\n",
      "                                       'Albert Gu, Anushan Fernando, Caglar '\n",
      "                                       'Gulcehre, Razvan Pascanu, and Soham '\n",
      "                                       'De.\\n'\n",
      "                                       '“Resurrecting recurrent neural '\n",
      "                                       'networks for long sequences”. In: '\n",
      "                                       'International Conference on Machine '\n",
      "                                       'Learning .\\n'\n",
      "                                       'PMLR. 2023, pp. 26670–26698.\\n'\n",
      "                                       '[79] Denis Paperno, Germán Kruszewski, '\n",
      "                                       'Angeliki Lazaridou, Ngoc Quan Pham, '\n",
      "                                       'Raffaella Bernardi, Sandro Pezzelle,\\n'\n",
      "                                       'Marco Baroni, Gemma Boleda, and Raquel '\n",
      "                                       'Fernández. “The LAMBADA dataset: Word '\n",
      "                                       'prediction requiring a broad\\n'\n",
      "                                       'discourse context”. In:Proceedings of '\n",
      "                                       'the 54th Annual Meeting of the '\n",
      "                                       'Association for Computational '\n",
      "                                       'Linguistics (Volume\\n'\n",
      "                                       '1: Long Papers) . Ed. by Katrin Erk '\n",
      "                                       'and Noah A. Smith. Berlin, Germany: '\n",
      "                                       'Association for Computational '\n",
      "                                       'Linguistics,\\n'\n",
      "                                       'Aug. 2016, pp. 1525–1534. doi: '\n",
      "                                       '10.18653/v1/P16-1144. url: '\n",
      "                                       'https://aclanthology.org/P16-1144/.\\n'\n",
      "                                       '[80] Badri N. Patro and Vijay S. '\n",
      "                                       'Agneeswaran. SiMBA: Simplified '\n",
      "                                       'Mamba-Based Architecture for Vision '\n",
      "                                       'and Multivariate\\n'\n",
      "                                       'Time series . 2024. arXiv: 2403.15360 '\n",
      "                                       '[cs.CV].\\n'\n",
      "                                       '[81] Guilherme Penedo, Hynek Kydlíček, '\n",
      "                                       'Loubna Ben allal, Anton Lozhkov, '\n",
      "                                       'Margaret Mitchell, Colin Raffel, '\n",
      "                                       'Leandro\\n'\n",
      "                                       'Von Werra, and Thomas Wolf. “The '\n",
      "                                       'FineWeb Datasets: Decanting the Web '\n",
      "                                       'for the Finest Text Data at Scale”. '\n",
      "                                       'In:\\n'\n",
      "                                       'The Thirty-eight Conference on Neural '\n",
      "                                       'Information Processing Systems '\n",
      "                                       'Datasets and Benchmarks Track . 2024. '\n",
      "                                       'url:\\n'\n",
      "                                       'https://openreview.net/forum?id=n6SCkn2QaG.\\n'\n",
      "                                       '[82] Bo Peng. RWKV-LM. Version 1.0.0. '\n",
      "                                       'Aug. 2021. doi: 10.5281/zenodo.5196577 '\n",
      "                                       '. url: https://github.com/\\n'\n",
      "                                       'BlinkDL/RWKV-LM.\\n'\n",
      "                                       '21\\n'\n",
      "                                       '[83] Bo Peng, Eric Alcaide, Quentin '\n",
      "                                       'Gregory Anthony, Alon Albalak, Samuel '\n",
      "                                       'Arcadinho, Stella Biderman, Huanqi '\n",
      "                                       'Cao,\\n'\n",
      "                                       'Xin Cheng, Michael Nguyen Chung, Leon '\n",
      "                                       'Derczynski, Xingjian Du, Matteo '\n",
      "                                       'Grella, Kranthi Kiran GV, Xuzheng He,\\n'\n",
      "                                       'Haowen Hou, Przemyslaw Kazienko, Jan '\n",
      "                                       'Kocon, Jiaming Kong, Bartłomiej '\n",
      "                                       'Koptyra, Hayden Lau, Jiaju Lin, '\n",
      "                                       'Krishna\\n'\n",
      "                                       'Sri Ipsit Mantri, Ferdinand Mom, '\n",
      "                                       'Atsushi Saito, Guangyu Song, Xiangru '\n",
      "                                       'Tang, Johan S. Wind, Stanisław '\n",
      "                                       'Woźniak,\\n'\n",
      "                                       'Zhenyuan Zhang, Qinghua Zhou, Jian '\n",
      "                                       'Zhu, and Rui-Jie Zhu. “RWKV: '\n",
      "                                       'Reinventing RNNs for the Transformer '\n",
      "                                       'Era”.\\n'\n",
      "                                       'In: The 2023 Conference on Empirical '\n",
      "                                       'Methods in Natural Language Processing '\n",
      "                                       '. 2023. url: https://openreview.\\n'\n",
      "                                       'net/forum?id=7SaXczaBpG.\\n'\n",
      "                                       '[84] Bo Peng, Daniel Goldstein, '\n",
      "                                       'Quentin Anthony, Alon Albalak, Eric '\n",
      "                                       'Alcaide, Stella Biderman, Eugene '\n",
      "                                       'Cheah, Xingjian\\n'\n",
      "                                       'Du, Teddy Ferdinan, Haowen Hou, et al. '\n",
      "                                       '“Eagle and finch: Rwkv with '\n",
      "                                       'matrix-valued states and dynamic '\n",
      "                                       'recurrence”.\\n'\n",
      "                                       'In: arXiv preprint arXiv:2404.05892 '\n",
      "                                       '(2024).\\n'\n",
      "                                       '[85] DL Prados and SC Kak. “Neural '\n",
      "                                       'network capacity using delta rule”. '\n",
      "                                       'In: Electronics Letters 25.3 (1989), '\n",
      "                                       'pp. 197–199.\\n'\n",
      "                                       '[86] Zhen Qin, Yiran Zhong, and Hui '\n",
      "                                       'Deng. “Exploring Transformer '\n",
      "                                       'Extrapolation”. In: Proceedings of the '\n",
      "                                       'AAAI\\n'\n",
      "                                       'Conference on Artificial Intelligence '\n",
      "                                       '. Vol. 38. 17. 2024, pp. 18897–18905.\\n'\n",
      "                                       '[87] Liliang Ren, Yang Liu, Yadong Lu, '\n",
      "                                       'Yelong Shen, Chen Liang, and Weizhu '\n",
      "                                       'Chen. “Samba: Simple Hybrid State '\n",
      "                                       'Space\\n'\n",
      "                                       'Models for Efficient Unlimited Context '\n",
      "                                       'Language Modeling”. In: arXiv preprint '\n",
      "                                       'arXiv:2406.07522 (2024).\\n'\n",
      "                                       '[88] Ivan Rodkin, Yuri Kuratov, Aydar '\n",
      "                                       'Bulatov, and Mikhail Burtsev. '\n",
      "                                       '“Associative recurrent memory '\n",
      "                                       'transformer”. In:\\n'\n",
      "                                       'arXiv preprint arXiv:2407.04841 '\n",
      "                                       '(2024).\\n'\n",
      "                                       '[89] Aurko Roy, Mohammad Saffar, '\n",
      "                                       'Ashish Vaswani, and David Grangier. '\n",
      "                                       '“Efficient content-based sparse '\n",
      "                                       'attention with\\n'\n",
      "                                       'routing transformers”. In: '\n",
      "                                       'Transactions of the Association for '\n",
      "                                       'Computational Linguistics 9 (2021), '\n",
      "                                       'pp. 53–68.\\n'\n",
      "                                       '[90] Keisuke Sakaguchi, Ronan Le Bras, '\n",
      "                                       'Chandra Bhagavatula, and Yejin Choi. '\n",
      "                                       '“Winogrande: An adversarial winograd\\n'\n",
      "                                       'schema challenge at scale”. In: '\n",
      "                                       'Communications of the ACM 64.9 (2021), '\n",
      "                                       'pp. 99–106.\\n'\n",
      "                                       '[91] Maarten Sap, Hannah Rashkin, '\n",
      "                                       'Derek Chen, Ronan Le Bras, and Yejin '\n",
      "                                       'Choi. “Social IQa: Commonsense '\n",
      "                                       'Reasoning\\n'\n",
      "                                       'about Social Interactions”. '\n",
      "                                       'In:Proceedings of the 2019 Conference '\n",
      "                                       'on Empirical Methods in Natural '\n",
      "                                       'Language Processing\\n'\n",
      "                                       'and the 9th International Joint '\n",
      "                                       'Conference on Natural Language '\n",
      "                                       'Processing (EMNLP-IJCNLP) . Ed. by '\n",
      "                                       'Kentaro Inui,\\n'\n",
      "                                       'Jing Jiang, Vincent Ng, and Xiaojun '\n",
      "                                       'Wan. Hong Kong, China: Association for '\n",
      "                                       'Computational Linguistics, Nov. 2019,\\n'\n",
      "                                       'pp. 4463–4473. doi: '\n",
      "                                       '10.18653/v1/D19-1454. url: '\n",
      "                                       'https://aclanthology.org/D19-1454/.\\n'\n",
      "                                       '[92] Imanol Schlag, Kazuki Irie, and '\n",
      "                                       'Jürgen Schmidhuber. “Linear '\n",
      "                                       'transformers are secretly fast weight '\n",
      "                                       'programmers”.\\n'\n",
      "                                       'In: International Conference on '\n",
      "                                       'Machine Learning . PMLR. 2021, pp. '\n",
      "                                       '9355–9366.\\n'\n",
      "                                       '[93] JH Schmidhuber. “Learning to '\n",
      "                                       'control fast-weight memories: An '\n",
      "                                       'alternative to recurrent nets. '\n",
      "                                       'Accepted for\\n'\n",
      "                                       'publication in”. In: Neural '\n",
      "                                       'Computation (1992).\\n'\n",
      "                                       '[94] Jürgen Schmidhuber. “Reducing the '\n",
      "                                       'ratio between learning complexity and '\n",
      "                                       'number of time varying variables\\n'\n",
      "                                       'in fully recurrent nets”. In: '\n",
      "                                       'ICANN’93: Proceedings of the '\n",
      "                                       'International Conference on Artificial '\n",
      "                                       'Neural Networks\\n'\n",
      "                                       'Amsterdam, The Netherlands 13–16 '\n",
      "                                       'September 1993 3 . Springer. 1993, pp. '\n",
      "                                       '460–463.\\n'\n",
      "                                       '[95] Jürgen Schmidhuber and Sepp '\n",
      "                                       'Hochreiter. “Long Short-term Memory”. '\n",
      "                                       'In: Neural Computation MIT-Press '\n",
      "                                       '(1997).\\n'\n",
      "                                       '[96] Avi Schwarzschild, Zhili Feng, '\n",
      "                                       'Pratyush Maini, Zachary C Lipton, and '\n",
      "                                       'J Zico Kolter. “Rethinking llm '\n",
      "                                       'memorization\\n'\n",
      "                                       'through the lens of adversarial '\n",
      "                                       'compression”. In: arXiv preprint '\n",
      "                                       'arXiv:2404.15146 (2024).\\n'\n",
      "                                       '[97] Jimmy T.H. Smith, Andrew '\n",
      "                                       'Warrington, and Scott Linderman. '\n",
      "                                       '“Simplified State Space Layers for '\n",
      "                                       'Sequence Modeling”.\\n'\n",
      "                                       'In: The Eleventh International '\n",
      "                                       'Conference on Learning Representations '\n",
      "                                       '. 2023. url: '\n",
      "                                       'https://openreview.net/forum?\\n'\n",
      "                                       'id=Ai8Hw3AXqks.\\n'\n",
      "                                       '[98] Robin Staab, Mark Vero, Mislav '\n",
      "                                       'Balunovic, and Martin Vechev. “Beyond '\n",
      "                                       'Memorization: Violating Privacy via\\n'\n",
      "                                       'Inference with Large Language Models”. '\n",
      "                                       'In: The Twelfth International '\n",
      "                                       'Conference on Learning Representations '\n",
      "                                       '. 2024.\\n'\n",
      "                                       'url: '\n",
      "                                       'https://openreview.net/forum?id=kmn0BhQk7p.\\n'\n",
      "                                       '[99] Sainbayar Sukhbaatar, Edouard '\n",
      "                                       'Grave, Guillaume Lample, Herve Jegou, '\n",
      "                                       'and Armand Joulin. “Augmenting self-\\n'\n",
      "                                       'attention with persistent memory”. In: '\n",
      "                                       'arXiv preprint arXiv:1907.01470 '\n",
      "                                       '(2019).\\n'\n",
      "                                       '[100] Sainbayar Sukhbaatar, Jason '\n",
      "                                       'Weston, Rob Fergus, et al. “End-to-end '\n",
      "                                       'memory networks”. In: Advances in '\n",
      "                                       'neural\\n'\n",
      "                                       'information processing systems 28 '\n",
      "                                       '(2015).\\n'\n",
      "                                       '[101] Yu Sun, Xinhao Li, Karan Dalal, '\n",
      "                                       'Jiarui Xu, Arjun Vikram, Genghan '\n",
      "                                       'Zhang, Yann Dubois, Xinlei Chen, '\n",
      "                                       'Xiaolong\\n'\n",
      "                                       'Wang, Sanmi Koyejo, et al. “Learning '\n",
      "                                       'to (learn at test time): Rnns with '\n",
      "                                       'expressive hidden states”. In: arXiv '\n",
      "                                       'preprint\\n'\n",
      "                                       'arXiv:2407.04620 (2024).\\n'\n",
      "                                       '[102] Yutao Sun, Li Dong, Shaohan '\n",
      "                                       'Huang, Shuming Ma, Yuqing Xia, Jilong '\n",
      "                                       'Xue, Jianyong Wang, and Furu Wei. '\n",
      "                                       '“Retentive\\n'\n",
      "                                       'network: A successor to transformer '\n",
      "                                       'for large language models”. In: arXiv '\n",
      "                                       'preprint arXiv:2307.08621 (2023).\\n'\n",
      "                                       '[103] Gemma Team, Thomas Mesnard, '\n",
      "                                       'Cassidy Hardin, Robert Dadashi, Surya '\n",
      "                                       'Bhupatiraju, Shreya Pathak, Laurent '\n",
      "                                       'Sifre,\\n'\n",
      "                                       'Morgane Rivière, Mihir Sanjay Kale, '\n",
      "                                       'Juliette Love, et al. “Gemma: Open '\n",
      "                                       'models based on gemini research and\\n'\n",
      "                                       'technology”. In: arXiv preprint '\n",
      "                                       'arXiv:2403.08295 (2024).\\n'\n",
      "                                       '22\\n'\n",
      "                                       '[104] W Scott Terry. Learning and '\n",
      "                                       'memory: Basic principles, processes, '\n",
      "                                       'and procedures . Routledge, 2017.\\n'\n",
      "                                       '[105] Matteo Tiezzi, Michele Casoni, '\n",
      "                                       'Alessandro Betti, Tommaso Guidi, Marco '\n",
      "                                       'Gori, and Stefano Melacci. “On the\\n'\n",
      "                                       'resurgence of recurrent models for '\n",
      "                                       'long sequences: Survey and research '\n",
      "                                       'opportunities in the transformer era”. '\n",
      "                                       'In:\\n'\n",
      "                                       'arXiv preprint arXiv:2402.08132 '\n",
      "                                       '(2024).\\n'\n",
      "                                       '[106] Hugo Touvron, Thibaut Lavril, '\n",
      "                                       'Gautier Izacard, Xavier Martinet, '\n",
      "                                       'Marie-Anne Lachaux, Timothée Lacroix, '\n",
      "                                       'Baptiste\\n'\n",
      "                                       'Rozière, Naman Goyal, Eric Hambro, '\n",
      "                                       'Faisal Azhar, et al. “Llama: Open and '\n",
      "                                       'efficient foundation language '\n",
      "                                       'models”.\\n'\n",
      "                                       'In: arXiv preprint arXiv:2302.13971 '\n",
      "                                       '(2023).\\n'\n",
      "                                       '[107] Jos Van Der Westhuizen and Joan '\n",
      "                                       'Lasenby. “The unreasonable '\n",
      "                                       'effectiveness of the forget gate”. '\n",
      "                                       'In:arXiv preprint\\n'\n",
      "                                       'arXiv:1804.04849 (2018).\\n'\n",
      "                                       '[108] Ashish Vaswani, Noam Shazeer, '\n",
      "                                       'Niki Parmar, Jakob Uszkoreit, Llion '\n",
      "                                       'Jones, Aidan N Gomez, Łukasz Kaiser,\\n'\n",
      "                                       'and Illia Polosukhin. “Attention is '\n",
      "                                       'All you Need”. In: Advances in Neural '\n",
      "                                       'Information Processing Systems . Ed.\\n'\n",
      "                                       'by I. Guyon, U. Von Luxburg, S. '\n",
      "                                       'Bengio, H. Wallach, R. Fergus, S. '\n",
      "                                       'Vishwanathan, and R. Garnett. Vol. 30. '\n",
      "                                       'Cur-\\n'\n",
      "                                       'ran Associates, Inc., 2017. url: https '\n",
      "                                       ': / / proceedings . neurips . cc / '\n",
      "                                       'paper _ files / paper / 2017 / file /\\n'\n",
      "                                       '3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.\\n'\n",
      "                                       '[109] Shida Wang. “LongSSM: On the '\n",
      "                                       'Length Extension of State-space Models '\n",
      "                                       'in Language Modelling”. In: arXiv '\n",
      "                                       'preprint\\n'\n",
      "                                       'arXiv:2406.02080 (2024).\\n'\n",
      "                                       '[110] Yu Wang, Yifan Gao, Xiusi Chen, '\n",
      "                                       'Haoming Jiang, Shiyang Li, Jingfeng '\n",
      "                                       'Yang, Qingyu Yin, Zheng Li, Xian Li, '\n",
      "                                       'Bing Yin,\\n'\n",
      "                                       'Jingbo Shang, and Julian McAuley. '\n",
      "                                       '“MEMORYLLM: Towards Self-Updatable '\n",
      "                                       'Large Language Models”. '\n",
      "                                       'In:Forty-first\\n'\n",
      "                                       'International Conference on Machine '\n",
      "                                       'Learning . 2024. url: '\n",
      "                                       'https://openreview.net/forum?id=p0lKWzdikQ.\\n'\n",
      "                                       '[111] Yu Wang, Chi Han, Tongtong Wu, '\n",
      "                                       'Xiaoxin He, Wangchunshu Zhou, Nafis '\n",
      "                                       'Sadeq, Xiusi Chen, Zexue He, Wei '\n",
      "                                       'Wang,\\n'\n",
      "                                       'Gholamreza Haffari, et al. “Towards '\n",
      "                                       'LifeSpan Cognitive Systems”. In: arXiv '\n",
      "                                       'preprint arXiv:2409.13265 (2024).\\n'\n",
      "                                       '[112] Zhiwei Wang, Yao Ma, Zitao Liu, '\n",
      "                                       'and Jiliang Tang. “R-transformer: '\n",
      "                                       'Recurrent neural network enhanced '\n",
      "                                       'transformer”.\\n'\n",
      "                                       'In: arXiv preprint arXiv:1907.05572 '\n",
      "                                       '(2019).\\n'\n",
      "                                       '[113] Jason Weston, Sumit Chopra, and '\n",
      "                                       'Antoine Bordes. “Memory networks”. In: '\n",
      "                                       'arXiv preprint arXiv:1410.3916 '\n",
      "                                       '(2014).\\n'\n",
      "                                       '[114] Bernard Widrow and Marcian E '\n",
      "                                       'Hoff. “Adaptive switching circuits”. '\n",
      "                                       'In: Neurocomputing: foundations of '\n",
      "                                       'research .\\n'\n",
      "                                       '1988, pp. 123–134.\\n'\n",
      "                                       '[115] Ronald J Williams and David '\n",
      "                                       'Zipser. “A learning algorithm for '\n",
      "                                       'continually running fully recurrent '\n",
      "                                       'neural networks”.\\n'\n",
      "                                       'In: Neural computation 1.2 (1989), pp. '\n",
      "                                       '270–280.\\n'\n",
      "                                       '[116] Daniel B Willingham. “Systems of '\n",
      "                                       'memory in the human brain”. In: Neuron '\n",
      "                                       '18.1 (1997), pp. 5–8.\\n'\n",
      "                                       '[117] Chao-Yuan Wu, Christoph '\n",
      "                                       'Feichtenhofer, Haoqi Fan, Kaiming He, '\n",
      "                                       'Philipp Krahenbuhl, and Ross Girshick. '\n",
      "                                       '“Long-\\n'\n",
      "                                       'term feature banks for detailed video '\n",
      "                                       'understanding”. In: Proceedings of the '\n",
      "                                       'IEEE/CVF conference on computer '\n",
      "                                       'vision\\n'\n",
      "                                       'and pattern recognition . 2019, pp. '\n",
      "                                       '284–293.\\n'\n",
      "                                       '[118] Haixu Wu, Tengge Hu, Yong Liu, '\n",
      "                                       'Hang Zhou, Jianmin Wang, and Mingsheng '\n",
      "                                       'Long. “TimesNet: Temporal 2D-\\n'\n",
      "                                       'Variation Modeling for General Time '\n",
      "                                       'Series Analysis”. In: The Eleventh '\n",
      "                                       'International Conference on Learning\\n'\n",
      "                                       'Representations. 2023. url: '\n",
      "                                       'https://openreview.net/forum?id=ju_Uqw384Oq.\\n'\n",
      "                                       '[119] Qingyang Wu, Zhenzhong Lan, Kun '\n",
      "                                       'Qian, Jing Gu, Alborz Geramifard, and '\n",
      "                                       'Zhou Yu. “Memformer: A memory-\\n'\n",
      "                                       'augmented transformer for sequence '\n",
      "                                       'modeling”. In: arXiv preprint '\n",
      "                                       'arXiv:2010.06891 (2020).\\n'\n",
      "                                       '[120] Guangxuan Xiao, Yuandong Tian, '\n",
      "                                       'Beidi Chen, Song Han, and Mike Lewis. '\n",
      "                                       '“Efficient Streaming Language Models\\n'\n",
      "                                       'with Attention Sinks”. In: The Twelfth '\n",
      "                                       'International Conference on Learning '\n",
      "                                       'Representations . 2024. url: https:\\n'\n",
      "                                       '//openreview.net/forum?id=NG7sS51zVF.\\n'\n",
      "                                       '[121] An Yang, Baosong Yang, Beichen '\n",
      "                                       'Zhang, Binyuan Hui, Bo Zheng, Bowen '\n",
      "                                       'Yu, Chengyuan Li, Dayiheng Liu, Fei\\n'\n",
      "                                       'Huang, Haoran Wei, et al. “Qwen2. 5 '\n",
      "                                       'Technical Report”. In:arXiv preprint '\n",
      "                                       'arXiv:2412.15115 (2024).\\n'\n",
      "                                       '[122] Songlin Yang, Jan Kautz, and Ali '\n",
      "                                       'Hatamizadeh. “Gated Delta Networks: '\n",
      "                                       'Improving Mamba2 with Delta Rule”. '\n",
      "                                       'In:\\n'\n",
      "                                       'arXiv preprint arXiv:2412.06464 '\n",
      "                                       '(2024).\\n'\n",
      "                                       '[123] Songlin Yang, Bailin Wang, '\n",
      "                                       'Yikang Shen, Rameswar Panda, and Yoon '\n",
      "                                       'Kim. “Gated Linear Attention '\n",
      "                                       'Transformers\\n'\n",
      "                                       'with Hardware-Efficient Training”. In: '\n",
      "                                       'Forty-first International Conference '\n",
      "                                       'on Machine Learning . 2024. url: '\n",
      "                                       'https:\\n'\n",
      "                                       '//openreview.net/forum?id=ia5XvxFUJT.\\n'\n",
      "                                       '[124] Songlin Yang, Bailin Wang, Yu '\n",
      "                                       'Zhang, Yikang Shen, and Yoon Kim. '\n",
      "                                       '“Parallelizing Linear Transformers '\n",
      "                                       'with the\\n'\n",
      "                                       'Delta Rule over Sequence Length”. '\n",
      "                                       'In:The Thirty-eighth Annual Conference '\n",
      "                                       'on Neural Information Processing '\n",
      "                                       'Systems .\\n'\n",
      "                                       '2024. url: '\n",
      "                                       'https://openreview.net/forum?id=y8Rm4VNRPH.\\n'\n",
      "                                       '[125] Luca Zancato, Arjun Seshadri, '\n",
      "                                       'Yonatan Dukler, Aditya Golatkar, '\n",
      "                                       'Yantao Shen, Benjamin Bowman, Matthew '\n",
      "                                       'Trager,\\n'\n",
      "                                       'Alessandro Achille, and Stefano '\n",
      "                                       'Soatto. “B’MOJO: Hybrid State Space '\n",
      "                                       'Realizations of Foundation Models '\n",
      "                                       'with\\n'\n",
      "                                       'Eidetic and Fading Memory”. In: The '\n",
      "                                       'Thirty-eighth Annual Conference on '\n",
      "                                       'Neural Information Processing Systems '\n",
      "                                       '.\\n'\n",
      "                                       '2024. url: '\n",
      "                                       'https://openreview.net/forum?id=RnQdRY1h5v.\\n'\n",
      "                                       '23\\n'\n",
      "                                       '[126] Rowan Zellers, Ari Holtzman, '\n",
      "                                       'Yonatan Bisk, Ali Farhadi, and Yejin '\n",
      "                                       'Choi. “HellaSwag: Can a Machine Really '\n",
      "                                       'Finish\\n'\n",
      "                                       'Your Sentence?” In: Proceedings of the '\n",
      "                                       '57th Annual Meeting of the Association '\n",
      "                                       'for Computational Linguistics . Ed. '\n",
      "                                       'by\\n'\n",
      "                                       'Anna Korhonen, David Traum, and Lluís '\n",
      "                                       'Màrquez. Florence, Italy: Association '\n",
      "                                       'for Computational Linguistics, July\\n'\n",
      "                                       '2019, pp. 4791–4800. doi: '\n",
      "                                       '10.18653/v1/P19-1472. url: '\n",
      "                                       'https://aclanthology.org/P19-1472/.\\n'\n",
      "                                       '[127] Ailing Zeng, Muxi Chen, Lei '\n",
      "                                       'Zhang, and Qiang Xu. “Are transformers '\n",
      "                                       'effective for time series '\n",
      "                                       'forecasting?” In:\\n'\n",
      "                                       'Proceedings of the AAAI conference on '\n",
      "                                       'artificial intelligence . Vol. 37. '\n",
      "                                       '2023, pp. 11121–11128.\\n'\n",
      "                                       '[128] Hao Zhang, Alexander C Berg, '\n",
      "                                       'Michael Maire, and Jitendra Malik. '\n",
      "                                       '“SVM-KNN: Discriminative nearest '\n",
      "                                       'neighbor\\n'\n",
      "                                       'classification for visual category '\n",
      "                                       'recognition”. In: 2006 IEEE Computer '\n",
      "                                       'Society Conference on Computer Vision '\n",
      "                                       'and\\n'\n",
      "                                       'Pattern Recognition (CVPR’06) . Vol. '\n",
      "                                       '2. IEEE. 2006, pp. 2126–2136.\\n'\n",
      "                                       '[129] Jianyu Zhang, Niklas Nolte, '\n",
      "                                       'Ranajoy Sadhukhan, Beidi Chen, and '\n",
      "                                       'Léon Bottou. “Memory Mosaics”. '\n",
      "                                       'In:arXiv preprint\\n'\n",
      "                                       'arXiv:2405.06394 (2024).\\n'\n",
      "                                       '[130] Yunhao Zhang and Junchi Yan. '\n",
      "                                       '“Crossformer: Transformer utilizing '\n",
      "                                       'cross-dimension dependency for '\n",
      "                                       'multivariate\\n'\n",
      "                                       'time series forecasting”. In: The '\n",
      "                                       'eleventh international conference on '\n",
      "                                       'learning representations . 2023.\\n'\n",
      "                                       '[131] Haoyi Zhou, Shanghang Zhang, '\n",
      "                                       'Jieqi Peng, Shuai Zhang, Jianxin Li, '\n",
      "                                       'Hui Xiong, and Wancai Zhang. '\n",
      "                                       '“Informer:\\n'\n",
      "                                       'Beyond efficient transformer for long '\n",
      "                                       'sequence time-series forecasting”. In: '\n",
      "                                       'Proceedings of the AAAI conference on\\n'\n",
      "                                       'artificial intelligence . Vol. 35. 12. '\n",
      "                                       '2021, pp. 11106–11115.\\n'\n",
      "                                       '[132] Luisa Zintgraf, Kyriacos '\n",
      "                                       'Shiarli, Vitaly Kurin, Katja Hofmann, '\n",
      "                                       'and Shimon Whiteson. “Fast context '\n",
      "                                       'adaptation via\\n'\n",
      "                                       'meta-learning”. In: International '\n",
      "                                       'Conference on Machine Learning . PMLR. '\n",
      "                                       '2019, pp. 7693–7702.\\n'\n",
      "                                       '24\\n'\n",
      "                                       'A Related Work\\n'\n",
      "                                       'There are diverse perspectives that '\n",
      "                                       'can independently lead to the design '\n",
      "                                       'of Titans or its components. '\n",
      "                                       'Accordingly, to\\n'\n",
      "                                       'further situate our work in a broader '\n",
      "                                       'context, we review three categories of '\n",
      "                                       'studies:\\n'\n",
      "                                       'A.1 Linear Recurrent Models\\n'\n",
      "                                       'Recently, to address the computational '\n",
      "                                       'cost of Transformers in both training '\n",
      "                                       'and inference, linear recurrent '\n",
      "                                       'models\\n'\n",
      "                                       'have attracted much attention (Tiezzi '\n",
      "                                       'et al. 2024), mainly due to their fast '\n",
      "                                       'inference and training. The first '\n",
      "                                       'generation\\n'\n",
      "                                       'of models–such as RetNet (Yutao Sun et '\n",
      "                                       'al. 2023), LRU (Orvieto et al. 2023), '\n",
      "                                       'RWKV (Peng, Alcaide, et al. 2023), S5 '\n",
      "                                       '(J. T.\\n'\n",
      "                                       'Smith, Warrington, and Linderman '\n",
      "                                       '2023), and S4 (Gu, Goel, and Re '\n",
      "                                       '2022)–uses data-independent transition '\n",
      "                                       'matrix/decay\\n'\n",
      "                                       'mechanism. The second generation of '\n",
      "                                       'such models started to incorporate '\n",
      "                                       'gating mechanism, a widely used '\n",
      "                                       'techniques\\n'\n",
      "                                       'in traditional RNNs (Gers, Jürgen '\n",
      "                                       'Schmidhuber, and Cummins 2000; Greff '\n",
      "                                       'et al. 2016; Van Der Westhuizen and '\n",
      "                                       'Lasenby\\n'\n",
      "                                       '2018), into such linear '\n",
      "                                       'architectures–e.g., Griffin (De et al. '\n",
      "                                       '2024), SSMs (Behrouz, Santacatterina, '\n",
      "                                       'and Zabih 2024; Dao\\n'\n",
      "                                       'and Gu 2024; Gu and Dao 2024; Hasani '\n",
      "                                       'et al. 2023), RWKV6 (Peng, Goldstein, '\n",
      "                                       'et al. 2024). The third generation of '\n",
      "                                       'linear\\n'\n",
      "                                       'recurrent models are based on more '\n",
      "                                       'complex memory updating rule based on '\n",
      "                                       'meta-learning, online learning, '\n",
      "                                       'and/or\\n'\n",
      "                                       'delta-rule, resulting in more '\n",
      "                                       'expressive and effective models such '\n",
      "                                       'as: Longhorn (B. Liu et al. 2024), '\n",
      "                                       'Gated DeltaNet (S. Yang,\\n'\n",
      "                                       'Kautz, and Hatamizadeh 2024), TTT (Yu '\n",
      "                                       'Sun et al. 2024), and DeltaNet (S. '\n",
      "                                       'Yang, B. Wang, Yu Zhang, et al. 2024). '\n",
      "                                       'Our\\n'\n",
      "                                       'LMM model can be seen as the next '\n",
      "                                       'generation of such models, in which we '\n",
      "                                       'incorporate the token flow into the '\n",
      "                                       'memory\\n'\n",
      "                                       'updating mechanism, having more '\n",
      "                                       'powerful memory updating process. See '\n",
      "                                       'Appendix C for a detailed discussion '\n",
      "                                       'of\\n'\n",
      "                                       'different recurrent models and '\n",
      "                                       'Titans.\\n'\n",
      "                                       'A.2 Transformer-based Architectures\\n'\n",
      "                                       'Transformers. Transformers (Vaswani et '\n",
      "                                       'al. 2017) as the de facto backbone for '\n",
      "                                       'many deep learning models are based '\n",
      "                                       'on\\n'\n",
      "                                       'attention mechanism (Bahdanau 2014). '\n",
      "                                       'They, however, suffer from quadratic '\n",
      "                                       'computational cost, limiting their '\n",
      "                                       'ability\\n'\n",
      "                                       'to scale to long context window. To '\n",
      "                                       'improve the memory consumption and '\n",
      "                                       'throughput of softmax attention for '\n",
      "                                       'longer\\n'\n",
      "                                       'sequences, various studies focused on '\n",
      "                                       'I/O aware implementations of attention '\n",
      "                                       '(Dao 2024; Dao, D. Fu, et al. 2022), '\n",
      "                                       'designing\\n'\n",
      "                                       'more efficient attention mechanisms by '\n",
      "                                       'sparsifying the attention matrix (B. '\n",
      "                                       'Chen et al. 2021; Choromanski et al. '\n",
      "                                       '2021; Dai\\n'\n",
      "                                       'et al. 2019; J. Dong et al. 2024; Roy '\n",
      "                                       'et al. 2021), approximating the '\n",
      "                                       'softmax (Arora et al. 2024), or '\n",
      "                                       'developing kernel-based\\n'\n",
      "                                       '(linear) attentions (Aksenov et al. '\n",
      "                                       '2024; Kacham, Mirrokni, and P. Zhong '\n",
      "                                       '2024; Schlag, Irie, and Jürgen '\n",
      "                                       'Schmidhuber 2021;\\n'\n",
      "                                       'S. Yang, B. Wang, Shen, et al. 2024).\\n'\n",
      "                                       'Segment-based Transformers. Another '\n",
      "                                       'line of research to improve the '\n",
      "                                       'efficiency of Transformers is '\n",
      "                                       'segment-based or\\n'\n",
      "                                       'Chunk Transformers (Dai et al. 2019). '\n",
      "                                       'The main drawback of chunk '\n",
      "                                       'Transformers is that segments are '\n",
      "                                       'fully separated and\\n'\n",
      "                                       'so the context window is limited to '\n",
      "                                       'the length of the chunks. To address '\n",
      "                                       'this issue, various studies discuss '\n",
      "                                       'the importance\\n'\n",
      "                                       'of a memory so it can help the model '\n",
      "                                       'to transfer information across chunks '\n",
      "                                       '(Bulatov, Yuri Kuratov, et al. 2023; '\n",
      "                                       'Bulatov,\\n'\n",
      "                                       'Yury Kuratov, and Burtsev 2022; Feng '\n",
      "                                       'et al. 2022; Hutchins et al. 2022; '\n",
      "                                       'Rodkin et al. 2024; Z. Wang et al. '\n",
      "                                       '2019; Q. Wu\\n'\n",
      "                                       'et al. 2020; Zancato et al. 2024). The '\n",
      "                                       'key differences of Titans with these '\n",
      "                                       'models are: (1) The memory in such '\n",
      "                                       'models are\\n'\n",
      "                                       'simple small size vectors, lacking '\n",
      "                                       'expressive power to compress complex '\n",
      "                                       'information; (2) The memory module '\n",
      "                                       'lacks forget\\n'\n",
      "                                       'mechanism, leading to a fast memory '\n",
      "                                       'overflow; (3) only focus on momentary '\n",
      "                                       'surprise, missing the information '\n",
      "                                       'flow. More\\n'\n",
      "                                       'specifically, recalling Recurrent '\n",
      "                                       'Memory Transformers (RMT) (Bulatov, '\n",
      "                                       'Yuri Kuratov, et al. 2023; Bulatov, '\n",
      "                                       'Yury Kuratov,\\n'\n",
      "                                       'and Burtsev 2022; Rodkin et al. 2024), '\n",
      "                                       'one can treat Titans (MAC) as the '\n",
      "                                       'generalization of RMT, where we use a '\n",
      "                                       'neural\\n'\n",
      "                                       'memory module instead of a '\n",
      "                                       'vector-valued small size memory.\\n'\n",
      "                                       'Memory for Large Language Models. '\n",
      "                                       'Another interesting research direction '\n",
      "                                       'has been to incorporate external '\n",
      "                                       'memory\\n'\n",
      "                                       'modules to LLMs after training (Z. He '\n",
      "                                       'et al. 2024; Khandelwal et al. 2020; '\n",
      "                                       'Y. Wang, Y. Gao, et al. 2024). Such '\n",
      "                                       'models\\n'\n",
      "                                       'are different from our approach as we '\n",
      "                                       'incorporate the memory as a part of '\n",
      "                                       'initial architecture and so we train '\n",
      "                                       'it in\\n'\n",
      "                                       'an end-to-end manner. Also, most of '\n",
      "                                       'these explicit memory modules suffer '\n",
      "                                       'from the same limitations as '\n",
      "                                       'chunk-based\\n'\n",
      "                                       'Transformers (mentioned above). For a '\n",
      "                                       'detailed discussion of such models, we '\n",
      "                                       'refer to the recent study of Y. Wang, '\n",
      "                                       'Han,\\n'\n",
      "                                       'et al. (2024).\\n'\n",
      "                                       '25\\n'\n",
      "                                       'A.3 Test Time Training and Fast Weight '\n",
      "                                       'Programs\\n'\n",
      "                                       'Memory Design and Augmentation with '\n",
      "                                       'Memory. In the literature, a '\n",
      "                                       'substantial research effort have been '\n",
      "                                       'toward\\n'\n",
      "                                       'designing memory modules that are '\n",
      "                                       'capable of either memorizing the '\n",
      "                                       'knowledge abstraction (e.g., '\n",
      "                                       'persistent mem-\\n'\n",
      "                                       'ory) (Sukhbaatar, Grave, et al. 2019), '\n",
      "                                       'or memorizing the data-dependent '\n",
      "                                       'information (also known as contextual '\n",
      "                                       'memory),\\n'\n",
      "                                       'through recurrence (Bulatov, Yury '\n",
      "                                       'Kuratov, and Burtsev 2022; Rodkin et '\n",
      "                                       'al. 2024; Zancato et al. 2024), '\n",
      "                                       'Transformers (Berges\\n'\n",
      "                                       'et al. 2024; Cetin et al. 2024; Feng '\n",
      "                                       'et al. 2022; Le, Tran, and Venkatesh '\n",
      "                                       '2020; Munkhdalai, Faruqui, and Gopal '\n",
      "                                       '2024; J. Zhang\\n'\n",
      "                                       'et al. 2024), gradient (Irie, Csordás, '\n",
      "                                       'and Jürgen Schmidhuber 2022; '\n",
      "                                       'Munkhdalai, Sordoni, et al. 2019), or '\n",
      "                                       'other learning\\n'\n",
      "                                       'paradigms (Sukhbaatar, Weston, Fergus, '\n",
      "                                       'et al. 2015; Weston, Chopra, and '\n",
      "                                       'Bordes 2014). These memory models, '\n",
      "                                       'however,\\n'\n",
      "                                       'either (1) are based on momentary '\n",
      "                                       'surprise, missing the data flow and '\n",
      "                                       'events, (2) lack forget mechanisms to '\n",
      "                                       'remove\\n'\n",
      "                                       'the memory, leading to a fast memory '\n",
      "                                       'overflow (3) are fixed-size shallow '\n",
      "                                       '(matrix valued) memory, resulting in '\n",
      "                                       'poor\\n'\n",
      "                                       'performance in long context, and (4) '\n",
      "                                       'are based on fixed parameters at test '\n",
      "                                       'time, lacking test time adaption.\\n'\n",
      "                                       'Fast Weight Programs. The idea of '\n",
      "                                       'seeing linear layers as the key-value '\n",
      "                                       '(associative) memory system backs to '\n",
      "                                       'fast\\n'\n",
      "                                       'weight programs, in which dynamic fast '\n",
      "                                       'programs are incorporated into '\n",
      "                                       'recurrent neural networks to serve as '\n",
      "                                       'writable\\n'\n",
      "                                       'memory (Schlag, Irie, and Jürgen '\n",
      "                                       'Schmidhuber 2021; JH Schmidhuber 1992; '\n",
      "                                       'Jürgen Schmidhuber 1993). The two '\n",
      "                                       'learning\\n'\n",
      "                                       'rules of Hebbian (Hebb 2005) and delta '\n",
      "                                       '(Prados and Kak 1989) are the most '\n",
      "                                       'popular learning rules for fast weight '\n",
      "                                       'programs,\\n'\n",
      "                                       'which have been extensively explored '\n",
      "                                       'in various studies (Irie, Schlag, et '\n",
      "                                       'al. 2021; Munkhdalai, Sordoni, et al. '\n",
      "                                       '2019;\\n'\n",
      "                                       'Munkhdalai and H. Yu 2017; Schlag, '\n",
      "                                       'Irie, and Jürgen Schmidhuber 2021; JH '\n",
      "                                       'Schmidhuber 1992; S. Yang, Kautz, and\\n'\n",
      "                                       'Hatamizadeh 2024; S. Yang, B. Wang, Yu '\n",
      "                                       'Zhang, et al. 2024). All these models, '\n",
      "                                       'however, are based on momentary '\n",
      "                                       'surprise,\\n'\n",
      "                                       'missing the token flow in the '\n",
      "                                       'sequences (see Section 3.1), and most '\n",
      "                                       'of them lacks a forgetting gate, '\n",
      "                                       'resulting in a poor\\n'\n",
      "                                       'memory management.\\n'\n",
      "                                       'Test Time Training. The key ideas of '\n",
      "                                       'learning at test time or learning to '\n",
      "                                       'learn (i.e., (Andrychowicz et al. '\n",
      "                                       '2016)) backs to\\n'\n",
      "                                       'very early studies on local learning '\n",
      "                                       'Bottou and Vapnik 1992, in which each '\n",
      "                                       'test data sample is trained on its '\n",
      "                                       'neighbors\\n'\n",
      "                                       'before making a prediction (Gandelsman '\n",
      "                                       'et al. 2022; H. Zhang et al. 2006). '\n",
      "                                       'This approach further has shown '\n",
      "                                       'promising\\n'\n",
      "                                       'performance in vision tasks (Jain and '\n",
      "                                       'Learned-Miller 2011; Mullapudi et al. '\n",
      "                                       '2019), mostly due to their ability to '\n",
      "                                       'mitigate\\n'\n",
      "                                       'out-of-distribution samples. The most '\n",
      "                                       'similar studies to ours in this '\n",
      "                                       'direction are MNM (Munkhdalai, '\n",
      "                                       'Sordoni, et al. 2019)\\n'\n",
      "                                       'and TTT-layer (Yu Sun et al. 2024), '\n",
      "                                       'which we discussed the key differences '\n",
      "                                       'in Appendix C.\\n'\n",
      "                                       'B Language Modeling and Common-sense '\n",
      "                                       'Reasoning Datasets\\n'\n",
      "                                       'Following recent studies on linear '\n",
      "                                       'recurrent models (Dao and Gu 2024; S. '\n",
      "                                       'Yang, Kautz, and Hatamizadeh 2024; S. '\n",
      "                                       'Yang,\\n'\n",
      "                                       'B. Wang, Yu Zhang, et al. 2024), we '\n",
      "                                       'use Wikitext (Merity et al. 2017), LMB '\n",
      "                                       '(Paperno et al. 2016), PIQA (Bisk et '\n",
      "                                       'al. 2020),\\n'\n",
      "                                       'HellaSwag (Zellers et al. 2019), '\n",
      "                                       'WinoGrande (Sakaguchi et al. 2021), '\n",
      "                                       'ARC-easy (ARC-e) and ARC-challenge '\n",
      "                                       '(ARC-c) (P.\\n'\n",
      "                                       'Clark et al. 2018), SIQA (Sap et al. '\n",
      "                                       '2019), and BoolQ (C. Clark et al. '\n",
      "                                       '2019). Also, the baselines results for '\n",
      "                                       '400M models are\\n'\n",
      "                                       'from the reported results by S. Yang, '\n",
      "                                       'Kautz, and Hatamizadeh (2024).\\n'\n",
      "                                       'C Long-term Memory Module (LMM) as a '\n",
      "                                       'Sequence Model\\n'\n",
      "                                       'In this section, we discuss how LMM as '\n",
      "                                       'a sequence model is connected to '\n",
      "                                       'modern linear recurrent models. For '\n",
      "                                       'the sake\\n'\n",
      "                                       'of simplicity, we start with a linear '\n",
      "                                       'memory, where M𝑡 = 𝑊𝑡 ∈R𝑑in ×𝑑in . In '\n",
      "                                       'this case, our objective function '\n",
      "                                       'becomes\\n'\n",
      "                                       'ℓ(M; 𝑥𝑡)= 1\\n'\n",
      "                                       '2 ∥M𝑡k𝑡 −v𝑡∥2\\n'\n",
      "                                       '2, in which we use gradient descent '\n",
      "                                       'with momentum and weight decay for the '\n",
      "                                       'optimization.\\n'\n",
      "                                       'Accordingly, revisiting the recurrent '\n",
      "                                       'formula in Equation 13:\\n'\n",
      "                                       'M𝑡 = diag (1 −𝛼𝑡)M𝑡 +𝑆𝑡 (32)\\n'\n",
      "                                       '𝑆𝑡 = diag (𝜂𝑡)𝑆𝑡−1 −diag '\n",
      "                                       '(𝜃𝑡)\\x00M𝑡−1k⊤\\n'\n",
      "                                       '𝑡 k𝑡 −v⊤\\n'\n",
      "                                       '𝑡 k𝑡\\n'\n",
      "                                       '\\x01 . (33)\\n'\n",
      "                                       'LMM is Generalized Gated DeltaNet. As '\n",
      "                                       'discussed by S. Yang, Kautz, and '\n",
      "                                       'Hatamizadeh (2024), DeltaNet (S. Yang, '\n",
      "                                       'B. Wang,\\n'\n",
      "                                       'Yu Zhang, et al. 2024) can '\n",
      "                                       'alternatively be interpreted as an '\n",
      "                                       'online learning problem that optimizes '\n",
      "                                       'the L= 1\\n'\n",
      "                                       '2 ∥S𝑡k𝑡 −v𝑡∥2\\n'\n",
      "                                       '2,\\n'\n",
      "                                       'resulting in:\\n'\n",
      "                                       'S𝑡+1 = S𝑡 −𝜃𝑡∇L= S𝑡\\n'\n",
      "                                       '\\x00I −𝜃𝑡k𝑡k⊤\\n'\n",
      "                                       '𝑡\\n'\n",
      "                                       '\\x01 +𝜃𝑡v𝑡k⊤\\n'\n",
      "                                       '𝑡 . (34)\\n'\n",
      "                                       '26\\n'\n",
      "                                       'In this formulation, Gated DeltaNet is '\n",
      "                                       'the same as above but with an '\n",
      "                                       'additional weight decay term (S. Yang, '\n",
      "                                       'Kautz, and\\n'\n",
      "                                       'Hatamizadeh 2024). Comparing Equation '\n",
      "                                       '32 and Equation 34, we can see that '\n",
      "                                       'setting 𝜂𝑡 = 0 results in both '\n",
      "                                       'formulations to\\n'\n",
      "                                       'be equivalent. Accordingly, we can say '\n",
      "                                       'LMM is generalizing the very recent '\n",
      "                                       'study of Gated DeltaNet (S. Yang, '\n",
      "                                       'Kautz, and\\n'\n",
      "                                       'Hatamizadeh 2024) from three aspects:\\n'\n",
      "                                       '• Momentum-based Rule: The Delta Rule '\n",
      "                                       'is based on momentary surprise, '\n",
      "                                       'meaning that the flow of tokens '\n",
      "                                       'cannot\\n'\n",
      "                                       'affect the memory update rule. LMM, '\n",
      "                                       'however, is based on a momentum rule, '\n",
      "                                       'which consider both past and\\n'\n",
      "                                       'momentary surprise.\\n'\n",
      "                                       '• Deep Memory: While Gated DeltaNet is '\n",
      "                                       'limited to a linear (matrix-valued) '\n",
      "                                       'memory as it requires finding the '\n",
      "                                       'closed\\n'\n",
      "                                       'recurrence form, LMM allows using deep '\n",
      "                                       'memory module by using a '\n",
      "                                       'gradient-based formulation, resulting '\n",
      "                                       'in higher\\n'\n",
      "                                       'expressive power.\\n'\n",
      "                                       '• Non-Linear Recurrence: While '\n",
      "                                       'DeltaNet and Gated DeltaNet are based '\n",
      "                                       'on linear recurrence, our LMM is '\n",
      "                                       'using\\n'\n",
      "                                       'inter-chunk non-linear recurrence and '\n",
      "                                       'intra-chunk linear recurrence. This '\n",
      "                                       'design allows LMM having a higher\\n'\n",
      "                                       'expressive power.\\n'\n",
      "                                       'Here, we discussed Gated DeltaNet as a '\n",
      "                                       'sample of recent generation of '\n",
      "                                       'recurrent models. Similar approaches '\n",
      "                                       'such\\n'\n",
      "                                       'as RWKV-7 (Peng 2021) are also using '\n",
      "                                       'the same formulation and loss '\n",
      "                                       'function, and so LMM is generalizing '\n",
      "                                       'all such\\n'\n",
      "                                       'models.\\n'\n",
      "                                       'LMM is Generalized Longhorn. Similar '\n",
      "                                       'to DeltaNet, Longhorn (B. Liu et al. '\n",
      "                                       '2024) uses the same loss function but '\n",
      "                                       'it\\n'\n",
      "                                       'derives the closed form using implicit '\n",
      "                                       'online learning:\\n'\n",
      "                                       'S𝑡+1 = S𝑡\\n'\n",
      "                                       '\\x00I −𝛿𝑡k𝑡k⊤\\n'\n",
      "                                       '𝑡\\n'\n",
      "                                       '\\x01 +𝛿𝑡v𝑡k⊤\\n'\n",
      "                                       '𝑡 , (35)\\n'\n",
      "                                       'where 𝛿𝑡 = 𝜃𝑡\\n'\n",
      "                                       '1+𝜃𝑡 k𝑡 k⊤\\n'\n",
      "                                       '𝑡\\n'\n",
      "                                       '. It, however, lacks a forgetting '\n",
      "                                       'gate, resulting in a faster memory '\n",
      "                                       'overflow. Therefore, in addition two\\n'\n",
      "                                       'the abovementioned aspects of (1) '\n",
      "                                       'Momentum-based Rule, (2) Deep Memory, '\n",
      "                                       'and (3) Non-Linear Recurrence, LMM '\n",
      "                                       'has\\n'\n",
      "                                       'the advantage of using an additional '\n",
      "                                       '(4) Forget Gate, leading to a better '\n",
      "                                       'memory management.\\n'\n",
      "                                       'LMM is Generalized TTT Layer. To the '\n",
      "                                       'best of our knowledge, TTT (Yu Sun et '\n",
      "                                       'al. 2024), is the only modern linear\\n'\n",
      "                                       'recurrent models with a gradient-based '\n",
      "                                       'updating rule. In addition to '\n",
      "                                       'different architectural designs and '\n",
      "                                       'also objective\\n'\n",
      "                                       'functions, our LMM has three key '\n",
      "                                       'differences with presented TTT layers '\n",
      "                                       '(Yu Sun et al. 2024):\\n'\n",
      "                                       '1. Forgetting Mechanism : TTT layers '\n",
      "                                       'are updating memory at each time, '\n",
      "                                       'without having the chance to forget '\n",
      "                                       'the\\n'\n",
      "                                       'past data. Accordingly, when fixing '\n",
      "                                       'the memory size, the model cannot '\n",
      "                                       'manage the memory for long sequences. '\n",
      "                                       'A\\n'\n",
      "                                       'forget mechanism, such as LMM’s, '\n",
      "                                       'allows clearing the memory when very '\n",
      "                                       'past information is not needed '\n",
      "                                       'anymore.\\n'\n",
      "                                       'We show that in a general case, this '\n",
      "                                       'forget mechanism is equivalent to '\n",
      "                                       'weight decay and provide a fast method '\n",
      "                                       'to\\n'\n",
      "                                       'incorporate it into the parallel '\n",
      "                                       'training.\\n'\n",
      "                                       '2. Momentum-based Update Rule : TTT '\n",
      "                                       'layers are based on momentary '\n",
      "                                       'surprise, meaning that the flow of '\n",
      "                                       'tokens\\n'\n",
      "                                       'cannot affect the memory update rule. '\n",
      "                                       'LMM, however, is based on a momentum '\n",
      "                                       'rule, which consider both past and\\n'\n",
      "                                       'momentary surprise. See Section 3.1 '\n",
      "                                       'for the motivation of this design.\\n'\n",
      "                                       '3. Deep Memory : While TTT-layers '\n",
      "                                       'allows for deeper memory, the '\n",
      "                                       'advantages/disadvantages of such '\n",
      "                                       'deeper memory\\n'\n",
      "                                       'modules have not been experimentally '\n",
      "                                       'evaluated.\\n'\n",
      "                                       'To the best of our knowledge, our '\n",
      "                                       'neural long-term memory module is the '\n",
      "                                       'first linear recurrent model with '\n",
      "                                       'momentum-\\n'\n",
      "                                       'based update rule.\\n'\n",
      "                                       'Finally, as a key difference with all '\n",
      "                                       'the above and other recent linear '\n",
      "                                       'recurrent studies, note that the '\n",
      "                                       'hybrid variants of\\n'\n",
      "                                       'modern linear models–such as Griffin '\n",
      "                                       '(De et al. 2024), DeltaNet (S. Yang, '\n",
      "                                       'B. Wang, Yu Zhang, et al. 2024), Gated '\n",
      "                                       'DeltaNet (S.\\n'\n",
      "                                       'Yang, Kautz, and Hatamizadeh 2024), H3 '\n",
      "                                       '(D. Y. Fu et al. 2023), Mamba2 (Dao '\n",
      "                                       'and Gu 2024), Samba (Ren et al. 2024), '\n",
      "                                       'etc.–all\\n'\n",
      "                                       'are based on sequential layer-wise '\n",
      "                                       'design. We present Titans to show how '\n",
      "                                       'effectively one can incorporate such '\n",
      "                                       'memory\\n'\n",
      "                                       'modules into an architecture.\\n'\n",
      "                                       '27'}}}\n",
      "\u001b[36;1m\u001b[1;3m[1:tasks]\u001b[0m \u001b[1mStarting 1 task for step 1:\n",
      "\u001b[0m- \u001b[32;1m\u001b[1;3mextract_information\u001b[0m -> {'state': {'error': None,\n",
      "           'extracted_info': None,\n",
      "           'pdf_text': 'Titans: Learning to Memorize at Test Time\\n'\n",
      "                       'Ali Behrouz\\n'\n",
      "                       '†\\n'\n",
      "                       ', Peilin Zhong\\n'\n",
      "                       '†\\n'\n",
      "                       ', and Vahab Mirrokni\\n'\n",
      "                       '†\\n'\n",
      "                       '†\\n'\n",
      "                       'Google Research\\n'\n",
      "                       '{alibehrouz, peilinz, mirrokni}@google.com\\n'\n",
      "                       'Abstract\\n'\n",
      "                       'Over more than a decade there has been an extensive '\n",
      "                       'research effort of how effectively utilize recurrent '\n",
      "                       'models and\\n'\n",
      "                       'attentions. While recurrent models aim to compress the '\n",
      "                       'data into a fixed-size memory (called hidden state), '\n",
      "                       'attention allows\\n'\n",
      "                       'attending to the entire context window, capturing the '\n",
      "                       'direct dependencies of all tokens. This more accurate '\n",
      "                       'modeling\\n'\n",
      "                       'of dependencies, however, comes with a quadratic cost, '\n",
      "                       'limiting the model to a fixed-length context. We '\n",
      "                       'present a new\\n'\n",
      "                       'neural long-term memory module that learns to memorize '\n",
      "                       'historical context and helps an attention to attend to '\n",
      "                       'the\\n'\n",
      "                       'current context while utilizing long past information. '\n",
      "                       'We show that this neural memory has the advantage of a '\n",
      "                       'fast\\n'\n",
      "                       'parallelizable training while maintaining a fast '\n",
      "                       'inference. From a memory perspective, we argue that '\n",
      "                       'attention due to its\\n'\n",
      "                       'limited context but accurate dependency modeling '\n",
      "                       'performs as a short-term memory, while neural memory '\n",
      "                       'due to its\\n'\n",
      "                       'ability to memorize the data, acts as a long-term, '\n",
      "                       'more persistent, memory. Based on these two modules, '\n",
      "                       'we introduce\\n'\n",
      "                       'a new family of architectures, called Titans, and '\n",
      "                       'present three variants to address how one can '\n",
      "                       'effectively incorporate\\n'\n",
      "                       'memory into this architecture. Our experimental '\n",
      "                       'results on language modeling, common-sense reasoning, '\n",
      "                       'genomics,\\n'\n",
      "                       'and time series tasks show that Titans are more '\n",
      "                       'effective than Transformers and recent modern linear '\n",
      "                       'recurrent models.\\n'\n",
      "                       'They further can effectively scale to larger than 2M '\n",
      "                       'context window size with higher accuracy in '\n",
      "                       'needle-in-haystack tasks\\n'\n",
      "                       'compared to baselines.\\n'\n",
      "                       '1 Introduction\\n'\n",
      "                       '“The true art of memory is the art of attention!\"\\n'\n",
      "                       '— Samuel Johnson, 1787\\n'\n",
      "                       'T\\n'\n",
      "                       'ransformers, pure attention-based architectures '\n",
      "                       '(Vaswani et al. 2017), have been firmly established as '\n",
      "                       'state-of-\\n'\n",
      "                       'the-art models in sequence modeling, mainly due to '\n",
      "                       'their in-context learning and ability to learn at '\n",
      "                       'scale (Kaplan\\n'\n",
      "                       'et al. 2020). The primary building blocks of '\n",
      "                       'Transformers–attention modules—function as associative '\n",
      "                       'memory\\n'\n",
      "                       'blocks (Bietti et al. 2024), where they learn to store '\n",
      "                       'key-value associations and retrieve them by computing '\n",
      "                       'pairwise\\n'\n",
      "                       'similarity between queries (i.e., search signals) and '\n",
      "                       'keys (i.e., contexts). Accordingly, by design, the '\n",
      "                       'output of a Transformer\\n'\n",
      "                       'is exclusively conditioned on the direct dependencies '\n",
      "                       'of tokens in the current context window. This accurate '\n",
      "                       'modeling of\\n'\n",
      "                       'dependencies, however, comes with quadratic time and '\n",
      "                       'memory complexity in terms of the context length. In '\n",
      "                       'complex\\n'\n",
      "                       'real-world tasks (e.g., language modeling (N. F. Liu '\n",
      "                       'et al. 2024), video understanding (C.-Y. Wu et al. '\n",
      "                       '2019), long-term time\\n'\n",
      "                       'series forecasting (H. Zhou et al. 2021)), the context '\n",
      "                       'window can become extremely large, making the '\n",
      "                       'applicability of\\n'\n",
      "                       'Transformers challenging in these downstream tasks.\\n'\n",
      "                       'To overcome the scalability issue of Transformers, '\n",
      "                       'recent studies aim to design different variants of '\n",
      "                       'linear Transform-\\n'\n",
      "                       'ers (Kacham, Mirrokni, and P. Zhong 2024; '\n",
      "                       'Katharopoulos et al. 2020; S. Yang, B. Wang, Shen, et '\n",
      "                       'al. 2024), where softmax is\\n'\n",
      "                       'replaced by a kernel function in the attention (see '\n",
      "                       '§2.1 for details), resulting in a significant drop in '\n",
      "                       'memory consumption.\\n'\n",
      "                       'Despite efficiency and the ability to scale to longer '\n",
      "                       'context, linear Transformers do not show competitive '\n",
      "                       'performance\\n'\n",
      "                       'compared to Transformers as the kernel trick makes the '\n",
      "                       'model a linear recurrent network, in which the data is '\n",
      "                       'compressed\\n'\n",
      "                       'into a matrix-valued states (Katharopoulos et al. '\n",
      "                       '2020). This, however, brings a contradictory fact '\n",
      "                       'about linear recurrent (or\\n'\n",
      "                       'linear Transformers) models: On one hand, we use these '\n",
      "                       'linear models to enhance scalability and efficiency '\n",
      "                       '(linear vs.\\n'\n",
      "                       'quadratic complexity), whose advantages is appeared '\n",
      "                       'for very long context; On the other hand, a very long '\n",
      "                       'context cannot\\n'\n",
      "                       'be properly compressed in a small vector-valued or '\n",
      "                       'matrix-valued states (S. Wang 2024).\\n'\n",
      "                       '1\\n'\n",
      "                       'arXiv:2501.00663v1  [cs.LG]  31 Dec 2024\\n'\n",
      "                       'Furthermore, beyond efficiency, most existing '\n",
      "                       'architectures–ranging from Hopfield Networks (Hopfield '\n",
      "                       '1982) to LSTMs (Jür-\\n'\n",
      "                       'gen Schmidhuber and Hochreiter 1997) and Transformers '\n",
      "                       '(Vaswani et al. 2017)–face challenges when dealing '\n",
      "                       'with general-\\n'\n",
      "                       'ization, length extrapolation, and/or reasoning (Anil '\n",
      "                       'et al. 2022; Qin, Y. Zhong, and Deng 2024), all of '\n",
      "                       'which are inseparable\\n'\n",
      "                       'parts of many hard real-world tasks. Although these '\n",
      "                       'architectures draw inspiration from the human brain, '\n",
      "                       'each of which\\n'\n",
      "                       'are missing: (1) a crucial component for learning '\n",
      "                       'process—such as short-term memory, long-term memory, '\n",
      "                       'meta-memory,\\n'\n",
      "                       'attending to current context, etc. (Cowan 2008); (2) '\n",
      "                       'how these components are interconnected systems that '\n",
      "                       'can operate\\n'\n",
      "                       'independently; and/or (3) the ability to actively '\n",
      "                       'learn from data and memorize the abstraction of past '\n",
      "                       'history. We argue\\n'\n",
      "                       'that in an effective learning paradigm, similar to '\n",
      "                       'human brain, there aredistinct yet interconnected '\n",
      "                       'modules, each of which\\n'\n",
      "                       'is responsible for a component crucial to the learning '\n",
      "                       'process.\\n'\n",
      "                       'Memory Perspective\\n'\n",
      "                       'Memory is a fundamental mental process and is an '\n",
      "                       'inseparable component of human learning (Terry 2017). '\n",
      "                       'Without\\n'\n",
      "                       'a properly functioning memory system, humans and '\n",
      "                       'animals would be restricted to basic reflexes and '\n",
      "                       'stereotyped\\n'\n",
      "                       'behaviors. Accordingly, memory has been the '\n",
      "                       'inspiration for many seminal research in machine '\n",
      "                       'learning literature; e.g.,\\n'\n",
      "                       'Hopfield Networks (Hopfield 1982), LSTMs (Jürgen '\n",
      "                       'Schmidhuber and Hochreiter 1997), and Transformers '\n",
      "                       '(Vaswani et al.\\n'\n",
      "                       '2017).\\n'\n",
      "                       'Taking inspiration from the common definitions of '\n",
      "                       'memory and learning in neuropsychology literature '\n",
      "                       '(Okano, Hirano,\\n'\n",
      "                       'and Balaban 2000), most existing architectures '\n",
      "                       'consider memory as a neural update caused by an input, '\n",
      "                       'and define learning\\n'\n",
      "                       'as a process for acquiring effective and useful '\n",
      "                       'memory, given an objective. In this perspective, '\n",
      "                       'Recurrent Neural Networks\\n'\n",
      "                       '(RNNs) (Williams and Zipser 1989) can be defined as '\n",
      "                       'models with a vector-valued memory module M(also '\n",
      "                       'called hidden\\n'\n",
      "                       'state) with two main steps: Given a new input 𝑥𝑡 at '\n",
      "                       'time 𝑡, the model (1) updates the memory using a '\n",
      "                       'function 𝑓(M𝑡−1,𝑥𝑡)\\n'\n",
      "                       '(with compression); and (2) retrieves the '\n",
      "                       'corresponding memory of input using a function '\n",
      "                       '𝑔(M𝑡,𝑥𝑡)(see §2.1 for details).\\n'\n",
      "                       'Similarly, Transformers can be seen as architectures '\n",
      "                       'with a growing memory and two similar steps. That is, '\n",
      "                       'the pair of key\\n'\n",
      "                       'and value matrices acts as the model’s memory, and the '\n",
      "                       'model: (1) updates the memory by appending the key and '\n",
      "                       'value to\\n'\n",
      "                       'the memory (without compression), and (2) retrieves '\n",
      "                       'query vectors’ corresponding memory by finding the '\n",
      "                       'similarity of\\n'\n",
      "                       'query and key vectors, which is then used to weight '\n",
      "                       'the value vectors for the output.\\n'\n",
      "                       'This perspective, can help us better understand '\n",
      "                       'existing paradigms, their critical differences, and '\n",
      "                       'design more effective\\n'\n",
      "                       'architectures. For example, the main difference '\n",
      "                       'between Transformers (Vaswani et al. 2017) and linear '\n",
      "                       'Transform-\\n'\n",
      "                       'ers (Katharopoulos et al. 2020) is the memory '\n",
      "                       'structure as well as the memory updating step, in '\n",
      "                       'which linear Transformers\\n'\n",
      "                       'compress the historical data into a fixed-size '\n",
      "                       'matrix-valued memory while Transformers keep all '\n",
      "                       'historical data (within\\n'\n",
      "                       'the context length) without any compression. While '\n",
      "                       'both linear Transformers and linear RNNs (including '\n",
      "                       'state space\\n'\n",
      "                       'models) compress the information in memory update '\n",
      "                       'step, the critical difference lies in the structure of '\n",
      "                       'the memory,\\n'\n",
      "                       'where linear RNNs (vs. linear Transformers) use a '\n",
      "                       'vector-valued memory (vs. matrix-valued memory). '\n",
      "                       'Therefore, this\\n'\n",
      "                       'perspective motivates us to ask: (Q1) What constitute '\n",
      "                       'a good structure for the memory? (Q2) What is a proper '\n",
      "                       'memory\\n'\n",
      "                       'update mechanism? and (Q3) What is a good memory '\n",
      "                       'retrieval process?\\n'\n",
      "                       'Revisiting our understanding of human memory, it is '\n",
      "                       'neither a unitary process nor it serves a single '\n",
      "                       'function (Cowan\\n'\n",
      "                       '2008). In fact, memory is a confederation of '\n",
      "                       'systems–e.g., short-term, working, and long-term '\n",
      "                       'memory–each serving a\\n'\n",
      "                       'different function with different neural structures, '\n",
      "                       'and each capable of operating independently '\n",
      "                       '(Willingham 1997). This\\n'\n",
      "                       'fact motivates us to ask: (Q4) How to design an '\n",
      "                       'efficient architecture that incorporates different '\n",
      "                       'interconnected memory\\n'\n",
      "                       'modules. Finally, storing a memory is a neural process '\n",
      "                       'that requires to encode and store the abstraction of '\n",
      "                       'the past. It can\\n'\n",
      "                       'be over-simplification to assume a single vector or a '\n",
      "                       'matrix, whose parameters are encoding the data in a '\n",
      "                       'linear manner,\\n'\n",
      "                       'are enough for storing long-term history. (Q5) Is a '\n",
      "                       'deep memory module needed to effectively '\n",
      "                       'store/remember long\\n'\n",
      "                       'past?\\n'\n",
      "                       'Contributions and Roadmap\\n'\n",
      "                       'In this paper, we aim to answer the above five '\n",
      "                       'questions by designing a long-term neural memory '\n",
      "                       'module, that can\\n'\n",
      "                       'efficiently and effectively learn to memorize at test '\n",
      "                       'time. Building upon its design, we discuss how it can '\n",
      "                       'be incorporated\\n'\n",
      "                       'into an architecture.\\n'\n",
      "                       'Neural Memory (§3). We present a (deep) neural '\n",
      "                       'long-term memory that (as a meta in-context model) '\n",
      "                       'learns how to\\n'\n",
      "                       'memorize/store the data into its parameters at test '\n",
      "                       'time. Inspired by human long-term memory system '\n",
      "                       '(Mandler 2014),\\n'\n",
      "                       '2\\n'\n",
      "                       'we design this memory module so an event that violates '\n",
      "                       'the expectations (being surprising) is more memorable. '\n",
      "                       'To this\\n'\n",
      "                       'end, we measure the surprise of an input with the '\n",
      "                       'gradient of the neural network with respect to the '\n",
      "                       'input in associative\\n'\n",
      "                       'memory loss (see §3.1 for details). To better handle '\n",
      "                       'the limited memory, we present a decaying mechanism '\n",
      "                       'that consider the\\n'\n",
      "                       'proportion of memory size and the amount of data '\n",
      "                       'surprise, resulting in better memory management. We '\n",
      "                       'show that this\\n'\n",
      "                       'decay mechanism is in fact the generalization of '\n",
      "                       'forgetting mechanism in modern recurrent models (Dao '\n",
      "                       'and Gu 2024; Gu\\n'\n",
      "                       'and Dao 2024; S. Yang, Kautz, and Hatamizadeh 2024). '\n",
      "                       'Interestingly, we find that this mechanism is '\n",
      "                       'equivalent to optimizing\\n'\n",
      "                       'a meta neural network with mini-batch gradient '\n",
      "                       'descent, momentum, and weight decay. Building upon '\n",
      "                       'tensorizing\\n'\n",
      "                       'mini-batch gradient descent to use more matmul '\n",
      "                       'operations (Yu Sun et al. 2024), we present a fast and '\n",
      "                       'parallelizable\\n'\n",
      "                       'algorithm to train our deep neural long-term memory.\\n'\n",
      "                       'Titans Architectures (§4). After designing the '\n",
      "                       'long-term neural memory, an important remaining '\n",
      "                       'question is how to\\n'\n",
      "                       'effectively and efficiently incorporate memory into a '\n",
      "                       'deep learning architecture. We present Titans, a '\n",
      "                       'family of deep models\\n'\n",
      "                       'that consists of three hyper-heads: (1) Core: this '\n",
      "                       'module consists of the short-term memory, and is '\n",
      "                       'responsible for the main\\n'\n",
      "                       'flow of processing the data (we use attention with '\n",
      "                       'limited window size); (2) Long-term Memory: this '\n",
      "                       'branch is our neural\\n'\n",
      "                       'long-term memory module that is responsible to '\n",
      "                       'store/remember long past; (3) Persistent Memory: this '\n",
      "                       'is a set of learnable\\n'\n",
      "                       'but date-independent parameters that encodes the '\n",
      "                       'knowledge about a task. Finally, as a proof of '\n",
      "                       'concept, we present three\\n'\n",
      "                       'variants of Titans, in which we incorporate memory as: '\n",
      "                       '(i) a context, (ii) a layer, and (iii) a gated '\n",
      "                       'branch.\\n'\n",
      "                       'Experimental Results (§5). We perform experimental '\n",
      "                       'evaluations on language modeling, commonsense '\n",
      "                       'reasoning, recall-\\n'\n",
      "                       'intensive, needle in haystack, time series '\n",
      "                       'forecasting, and DNA modeling tasks. We observe that '\n",
      "                       'our Titan architecture\\n'\n",
      "                       'outperforms all modern recurrent models as well as '\n",
      "                       'their hybrid variants (combining with sliding-window '\n",
      "                       'attention) across\\n'\n",
      "                       'a comprehensive set of benchmarks. Furthermore, Titans '\n",
      "                       'outperforms Transformers with the same context window, '\n",
      "                       'and\\n'\n",
      "                       'show competitive performance with Transformers that '\n",
      "                       'use the entire context. This results are achieved '\n",
      "                       'while, contrary to\\n'\n",
      "                       'Transformers, Titans scale to larger than 2M context '\n",
      "                       'window size.\\n'\n",
      "                       '2 Preliminaries\\n'\n",
      "                       'I\\n'\n",
      "                       'n this section, we discuss the notation and some '\n",
      "                       'background concepts that we use though the paper. We '\n",
      "                       'let\\n'\n",
      "                       '𝑥 ∈R𝑁×𝑑in be the input, Mbe a neural network (neural '\n",
      "                       'memory module), Q,K,V be the query, key and value\\n'\n",
      "                       'of the attention mechanism, and M be the attention '\n",
      "                       'mask. When segmenting the sequence, we use S(𝑖)to '\n",
      "                       'refer to\\n'\n",
      "                       'the 𝑖-th segment. Through the paper, we abuse the '\n",
      "                       'notation and use subscripts to refer to a specific '\n",
      "                       'element of a matrix,\\n'\n",
      "                       'vector, or segments. For example, we let S(𝑖)\\n'\n",
      "                       '𝑗 be the 𝑗-th token in the 𝑖-th segment. The only '\n",
      "                       'exception is subscripts with 𝑡,\\n'\n",
      "                       'which we reserved to index recurrence over time, or '\n",
      "                       'the state of a neural network at time𝑡. Given a neural '\n",
      "                       'network Nand\\n'\n",
      "                       'a data sample 𝑥, we use N(𝑥)(resp. N∗(𝑥)) to refer to '\n",
      "                       'the forward pass with (resp. without) weight '\n",
      "                       'adjustment. Also, we\\n'\n",
      "                       'abuse the notation and use N(𝑘)to refer to the 𝑘-th '\n",
      "                       'layer of the neural network. In the following, we '\n",
      "                       'first, discuss the\\n'\n",
      "                       'backgrounds for attention and its efficient variants '\n",
      "                       'followed by a review of modern linear RNNs. Finally, '\n",
      "                       'we discuss a\\n'\n",
      "                       'memory perspective of these architectures that '\n",
      "                       'motivates us to design Titans.\\n'\n",
      "                       '2.1 Backgrounds\\n'\n",
      "                       'Attention. Transformers (Vaswani et al. 2017) as the '\n",
      "                       'de facto backbone for many deep learning models are '\n",
      "                       'based on\\n'\n",
      "                       'attention mechanism. Given input 𝑥 ∈R𝑁×𝑑in , causal '\n",
      "                       'attention computes output y ∈R𝑁×𝑑in based on softmax '\n",
      "                       'over input\\n'\n",
      "                       'dependent key, value, and query matrices:\\n'\n",
      "                       'Q = 𝑥WQ, K = 𝑥WK, V = 𝑥WV, (1)\\n'\n",
      "                       'y𝑖 =\\n'\n",
      "                       '𝑖∑︁\\n'\n",
      "                       '𝑗=1\\n'\n",
      "                       'exp\\n'\n",
      "                       '\\x10\\n'\n",
      "                       'Q⊤\\n'\n",
      "                       '𝑖 K𝑗/√𝑑in\\n'\n",
      "                       '\\x11\\n'\n",
      "                       'V𝑗\\n'\n",
      "                       'Í𝑖\\n'\n",
      "                       'ℓ=1 exp\\n'\n",
      "                       '\\x10\\n'\n",
      "                       'Q⊤\\n'\n",
      "                       '𝑖 Kℓ/√𝑑in\\n'\n",
      "                       '\\x11, (2)\\n'\n",
      "                       'where WQ,WK,and WV ∈R𝑑in ×𝑑in are learnable '\n",
      "                       'parameters. Despite the power and effectiveness in '\n",
      "                       'recall, transformers\\n'\n",
      "                       'need at least 𝑁 ×𝑑 operators to calculate the output, '\n",
      "                       'resulting in larger memory consumption and '\n",
      "                       'lower-throughput for\\n'\n",
      "                       'longer sequences.\\n'\n",
      "                       'Efficient Attentions. To improve the memory '\n",
      "                       'consumption and throughput of softmax attention for '\n",
      "                       'longer sequences,\\n'\n",
      "                       'various studies focused on I/O aware implementations '\n",
      "                       'of attention (Dao 2024; Dao, D. Fu, et al. 2022), '\n",
      "                       'designing more\\n'\n",
      "                       '3\\n'\n",
      "                       'efficient attention mechanisms by sparsifying the '\n",
      "                       'attention matrix (B. Chen et al. 2021; Choromanski et '\n",
      "                       'al. 2021; Dai et al.\\n'\n",
      "                       '2019), approximating the softmax (Arora et al. 2024), '\n",
      "                       'or developing kernel-based (linear) attentions '\n",
      "                       '(Aksenov et al. 2024;\\n'\n",
      "                       'Kacham, Mirrokni, and P. Zhong 2024; Schlag, Irie, and '\n",
      "                       'Jürgen Schmidhuber 2021; S. Yang, B. Wang, Shen, et '\n",
      "                       'al. 2024). In\\n'\n",
      "                       'this part, we focus on the later, i.e., linear '\n",
      "                       'attentions, where the softmax in standard attention is '\n",
      "                       'replaced with an alternative\\n'\n",
      "                       'kernel function 𝜙(.,.), such that 𝜙(𝑥,𝑦)= 𝜙(𝑥)𝜙(𝑦). '\n",
      "                       'Accordingly, the attention can be written as:\\n'\n",
      "                       'y𝑖 =\\n'\n",
      "                       '𝑖∑︁\\n'\n",
      "                       '𝑗=1\\n'\n",
      "                       '𝜙(𝑄⊤\\n'\n",
      "                       '𝑖 𝐾𝑗)\\n'\n",
      "                       'Í𝑖\\n'\n",
      "                       'ℓ=1 𝜙(𝑄⊤\\n'\n",
      "                       '𝑖 𝐾ℓ)\\n'\n",
      "                       '𝑉𝑗 =\\n'\n",
      "                       '𝑖∑︁\\n'\n",
      "                       '𝑗=1\\n'\n",
      "                       '𝜙(𝑄𝑖)⊤𝜙(𝐾𝑗)\\n'\n",
      "                       'Í𝑖\\n'\n",
      "                       'ℓ=1 𝜙(𝑄𝑖)⊤𝜙(𝐾ℓ)\\n'\n",
      "                       '𝑉𝑗 =\\n'\n",
      "                       '𝜙(𝑄𝑖)⊤Í𝑖\\n'\n",
      "                       '𝑗=1 𝜙(𝐾𝑗)𝑉𝑗\\n'\n",
      "                       '𝜙(𝑄𝑖)⊤Í𝑖\\n'\n",
      "                       'ℓ=1 𝜙(𝐾ℓ)\\n'\n",
      "                       ', (3)\\n'\n",
      "                       'resulting in a higher-throughput as terms Í𝑖\\n'\n",
      "                       '𝑗=1 𝜙(𝐾𝑗)and Í𝑖\\n'\n",
      "                       'ℓ=1 𝜙(𝐾ℓ)are re-using in each step. When choosing the '\n",
      "                       'kernel\\n'\n",
      "                       'as identity matrix (Yutao Sun et al. 2023), the above '\n",
      "                       'formulation can also be written in a recurrent '\n",
      "                       'format:\\n'\n",
      "                       'M𝑡 = M𝑡−1 +𝐾⊤\\n'\n",
      "                       '𝑡 𝑉𝑡 , (4)\\n'\n",
      "                       'y𝑡 = 𝑄𝑡M𝑡 , (5)\\n'\n",
      "                       'which allows efficient inference for linear '\n",
      "                       'attentions.\\n'\n",
      "                       'Modern Linear Models and Their Memory Perspective. As '\n",
      "                       'discussed earlier, one can define learning as a '\n",
      "                       'process for\\n'\n",
      "                       'acquiring effective and useful memory. Building upon '\n",
      "                       'this, one can see the hidden state of Recurrent Neural '\n",
      "                       'Networks\\n'\n",
      "                       '(RNNs) as a memory unit, which the model aims to '\n",
      "                       'compress the information into. Accordingly, in a '\n",
      "                       'general form of\\n'\n",
      "                       'recurrent neural network, the hidden state can be '\n",
      "                       'treated as a memory unit and the recurrence process '\n",
      "                       'can be split into the\\n'\n",
      "                       'read and write operations in the memory unit. That is, '\n",
      "                       'we let 𝑥 ∈R𝑁×𝑑in be the input, M∈ R𝑑 is the memory '\n",
      "                       'unit, and\\n'\n",
      "                       'y ∈R𝑑in is the output, then the general form of the '\n",
      "                       'recurrent neural network is defined as:\\n'\n",
      "                       'M𝑡 = 𝑓(M𝑡−1,𝑥𝑡), Write Operation (6)\\n'\n",
      "                       'y𝑡 = 𝑔(M𝑡,𝑥𝑡), Read Operation (7)\\n'\n",
      "                       'where 𝑓(.,.)is the read and 𝑔(.,.)is the write '\n",
      "                       'corresponding functions. Note that here the subscript '\n",
      "                       'of M𝑡 shows the state\\n'\n",
      "                       'of the memory at time 𝑡.\\n'\n",
      "                       'In this perspective, the recurrence formula of linear '\n",
      "                       'Transformers (see Equation 4) is equivalent to '\n",
      "                       'additively compress\\n'\n",
      "                       'and write keys and values, (𝐾𝑡,𝑉𝑡), into a '\n",
      "                       'matrix-valued memory unit M𝑡. Therefore, when dealing '\n",
      "                       'with long context\\n'\n",
      "                       'data, this additive nature of the process results in '\n",
      "                       'memory overflow, significantly damaging the '\n",
      "                       'performance of the model.\\n'\n",
      "                       'To address this, studies have focused on two promising '\n",
      "                       'directions: (1) Adding forget mechanism: several '\n",
      "                       'studies have\\n'\n",
      "                       'presented adaptive (data-dependent) forgetting gate '\n",
      "                       'mechanisms for linear models, where it can erase the '\n",
      "                       'memory when it\\n'\n",
      "                       'is needed. As examples of such models, we refer to GLA '\n",
      "                       '(S. Yang, B. Wang, Shen, et al. 2024), LRU (Orvieto et '\n",
      "                       'al. 2023),\\n'\n",
      "                       'Griffin (De et al. 2024), xLSTM (Beck et al. 2024), '\n",
      "                       'and Mamba2 (Dao and Gu 2024), which the later is also '\n",
      "                       'connected to the\\n'\n",
      "                       'discretized version of traditional state space models '\n",
      "                       '(Gu and Dao 2024).(2) Improving the write operation: '\n",
      "                       'To overcome the\\n'\n",
      "                       'additive nature of memory write operation in '\n",
      "                       'traditional recurrent models, Widrow and Hoff (1988) '\n",
      "                       'presented Delta Rule,\\n'\n",
      "                       'in which before adding a memory (i.e., a pair of key '\n",
      "                       'and value), the model first removes its past value. To '\n",
      "                       'enhance the\\n'\n",
      "                       'parallelizable training and scaling, S. Yang, B. Wang, '\n",
      "                       'Yu Zhang, et al. (2024) present a fast paralellizable '\n",
      "                       'algorithm. Finally,\\n'\n",
      "                       'very recently, S. Yang, Kautz, and Hatamizadeh (2024) '\n",
      "                       'improved the DeltaNets by adding a forget gate.\\n'\n",
      "                       'Memory Modules. Memory has always been one of the core '\n",
      "                       'parts of the neural network designs (Graves, Wayne,\\n'\n",
      "                       'and Danihelka 2014; JH Schmidhuber 1992; Jürgen '\n",
      "                       'Schmidhuber and Hochreiter 1997; J. Zhang et al. '\n",
      "                       '2024). The idea of\\n'\n",
      "                       'seeing linear layers as the key-value (associative) '\n",
      "                       'memory system backs to fast weight programs, in which '\n",
      "                       'dynamic fast\\n'\n",
      "                       'programs are incorporated into recurrent neural '\n",
      "                       'networks to serve as writable memory (JH Schmidhuber '\n",
      "                       '1992). The two\\n'\n",
      "                       'learning rules of Hebbian (Hebb 2005) and delta '\n",
      "                       '(Prados and Kak 1989) are the most popular learning '\n",
      "                       'rules for fast weight\\n'\n",
      "                       'programs, which have been extensively explored in '\n",
      "                       'various studies (Irie, Schlag, et al. 2021; '\n",
      "                       'Munkhdalai, Sordoni, et al.\\n'\n",
      "                       '2019; Munkhdalai and H. Yu 2017; Schlag, Irie, and '\n",
      "                       'Jürgen Schmidhuber 2021; JH Schmidhuber 1992; S. Yang, '\n",
      "                       'Kautz, and\\n'\n",
      "                       'Hatamizadeh 2024; S. Yang, B. Wang, Yu Zhang, et al. '\n",
      "                       '2024). All these models, however, are based on '\n",
      "                       'momentary surprise,\\n'\n",
      "                       'missing the token flow in the sequences (see Section '\n",
      "                       '3.1), and most of them lacks a forgetting gate, '\n",
      "                       'resulting in a poor\\n'\n",
      "                       'memory management.\\n'\n",
      "                       'We further discuss the connection of our architectures '\n",
      "                       'with recent models in Appendix C. Additional related '\n",
      "                       'work are\\n'\n",
      "                       'discussed in Appendix A.\\n'\n",
      "                       '4\\n'\n",
      "                       '3 Learning to Memorize at Test Time\\n'\n",
      "                       'T\\n'\n",
      "                       'o overcome the lack of long-term memory and to enable '\n",
      "                       'the model to learn, forget, and retrieve information, '\n",
      "                       'in\\n'\n",
      "                       'this section, we present a neural long-term memory '\n",
      "                       'module, which is a meta models that learns to memorize '\n",
      "                       'at\\n'\n",
      "                       'test time. In Section 3.1, we first discuss the '\n",
      "                       'motivation and the design of the neural memory. In '\n",
      "                       'Section 3.2, we\\n'\n",
      "                       'discuss how our architecture design can benefit from a '\n",
      "                       'fast and parallelizable training. Finally, in Section '\n",
      "                       '3.3, we augment\\n'\n",
      "                       'our architecture using persistent memory module, in '\n",
      "                       'which we use learnable but data-independent parameters '\n",
      "                       'to learn\\n'\n",
      "                       'meta information about the task.\\n'\n",
      "                       '3.1 Long-term Memory\\n'\n",
      "                       'To design a neural long-term memory module, we need a '\n",
      "                       'model that can encode the abstraction of the past '\n",
      "                       'history into its\\n'\n",
      "                       'parameters. An example of this can be LLMs that are '\n",
      "                       'shown to be memorizing their training data (Leybzon '\n",
      "                       'and Kervadec\\n'\n",
      "                       '2024; Schwarzschild et al. 2024; Staab et al. 2024). '\n",
      "                       'Therefore, a simple idea is to train a neural network '\n",
      "                       'and expect it to\\n'\n",
      "                       'memorize its training data. Memorization, however, has '\n",
      "                       'almost always been known as an undesirable phenomena '\n",
      "                       'in\\n'\n",
      "                       'neural networks as it limits the model generalization '\n",
      "                       '(Bayat et al. 2024), causes privacy concerns (Staab et '\n",
      "                       'al. 2024), and\\n'\n",
      "                       'so results in poor performance at test time. Moreover, '\n",
      "                       'the memorization of the training data might not be '\n",
      "                       'helpful at test\\n'\n",
      "                       'time, in which the data might be out-of-distribution. '\n",
      "                       'We argue that, we need an online meta-model that '\n",
      "                       'learns how to\\n'\n",
      "                       'memorize/forget the data at test time. In this setup, '\n",
      "                       'the model is learning a function that is capable of '\n",
      "                       'memorization, but it\\n'\n",
      "                       'is not overfitting to the training data, resulting in '\n",
      "                       'a better generalization at test time.\\n'\n",
      "                       'Learning Process and Surprise Metric. The key idea to '\n",
      "                       'train a long-term memory is to treat its training as '\n",
      "                       'an online\\n'\n",
      "                       'learning problem, in which we aim to compress the past '\n",
      "                       'information 𝑥1,...,𝑥 𝑡−1 into the parameters of our '\n",
      "                       'long-term\\n'\n",
      "                       'neural memory module M𝑡. As discussed earlier, an '\n",
      "                       'event that violates the expectations (i.e., is '\n",
      "                       'surprising) is more\\n'\n",
      "                       'memorable for humans (Mandler 2014). Inspired by this, '\n",
      "                       'a simple definition of surprise for a model can be its '\n",
      "                       'gradient with\\n'\n",
      "                       'respect to the input. The larger the gradient is, the '\n",
      "                       'more different the input data is from the past data. '\n",
      "                       'Accordingly, using\\n'\n",
      "                       'this surprise score, we can update the memory as:\\n'\n",
      "                       'M𝑡 = M𝑡−1 −𝜃𝑡 ∇ℓ(M𝑡−1; 𝑥𝑡)|           {z           }\\n'\n",
      "                       'Surprise\\n'\n",
      "                       '. (8)\\n'\n",
      "                       'This surprise metric, however, can result in missing '\n",
      "                       'important information that comes after a big '\n",
      "                       'surprising moment.\\n'\n",
      "                       'That is, the gradient can become extremely small after '\n",
      "                       'several surprising steps, leading to stocking in a '\n",
      "                       'flat area (i.e., local\\n'\n",
      "                       'minima), and missing information about some parts of '\n",
      "                       'the sequence. From the human memory perspective, an '\n",
      "                       'event might\\n'\n",
      "                       'not consistently surprise us through a long-period of '\n",
      "                       'time although it is memorable. The reason is that the '\n",
      "                       'initial moment\\n'\n",
      "                       'is surprising enough to get our attention through a '\n",
      "                       'long time frame, leading to memorizing the entire time '\n",
      "                       'frame. To\\n'\n",
      "                       'improve the above surprise metric (Equation 8), we '\n",
      "                       'break the surprise metric into (1) past surprise , '\n",
      "                       'which measures the\\n'\n",
      "                       'surprise amount of a very recent past; and (2) '\n",
      "                       'momentary surprise , which measures the surprise of '\n",
      "                       'incoming data:\\n'\n",
      "                       'M𝑡 = M𝑡−1 +𝑆𝑡, (9)\\n'\n",
      "                       '𝑆𝑡 = 𝜂𝑡 𝑆𝑡−1\\n'\n",
      "                       '|{z}\\n'\n",
      "                       'Past Surprise\\n'\n",
      "                       '−𝜃𝑡 ∇ℓ(𝑀𝑡−1; 𝑥𝑡)|          {z          }\\n'\n",
      "                       'Momentary Surprise\\n'\n",
      "                       '. (10)\\n'\n",
      "                       'Interestingly, this formulation is similar to gradient '\n",
      "                       'descent with momentum, where𝑆𝑡 is the momentum '\n",
      "                       'element. Therefore,\\n'\n",
      "                       'the momentum here act as a memory of surprise across '\n",
      "                       'time (sequence length). In this formulation, the term '\n",
      "                       '𝜂𝑡 is a\\n'\n",
      "                       'data-dependent surprise decay (a function of 𝑥𝑡), '\n",
      "                       'controlling how surprise decays over time, and the '\n",
      "                       'term 𝜃𝑡 is controlling\\n'\n",
      "                       'how much of momentary surprise should be incorporated '\n",
      "                       'into the final surprise metric in a data-dependent '\n",
      "                       'manner. This\\n'\n",
      "                       'data-dependency is particularly important in this '\n",
      "                       'design: While surprise of previous tokens might be '\n",
      "                       'needed to affect\\n'\n",
      "                       'the surprise of the next token, it is mostly valid if '\n",
      "                       'all tokens are relevant and are in the same context. '\n",
      "                       'Accordingly, a\\n'\n",
      "                       'data-dependent 𝜂can control if memory needs to: (1) '\n",
      "                       'ignore the last surprise by setting 𝜂𝑡 →0 (possibly '\n",
      "                       'due to the change\\n'\n",
      "                       'of context), or (2) fully incorporate the last '\n",
      "                       'surprise by setting 𝜂𝑡 →1 (possibly as the token is '\n",
      "                       'highly relevant to its recent\\n'\n",
      "                       'past tokens).\\n'\n",
      "                       'Objective. Our above surprise metric is based on a '\n",
      "                       'loss function ℓ(.; .), which is the objective that our '\n",
      "                       'memory is learning\\n'\n",
      "                       'to act as it at test time. That is, our memory module '\n",
      "                       'is a meta model that learns a function based on the '\n",
      "                       'loss function ℓ(.; .).\\n'\n",
      "                       '5\\n'\n",
      "                       'In this work, we focus on associative memory , in '\n",
      "                       'which we aim to store the past data as the pairs of '\n",
      "                       'keys and values. Given\\n'\n",
      "                       '𝑥𝑡, similar to Transformers (Vaswani et al. 2017), we '\n",
      "                       'use two linear layers to project𝑥𝑡 into a key and '\n",
      "                       'value:\\n'\n",
      "                       'k𝑡 = 𝑥𝑡𝑊𝐾, v𝑡 = 𝑥𝑡𝑊𝑉, (11)\\n'\n",
      "                       'where 𝑊𝐾 and 𝑊𝑉 ∈R𝑑in ×𝑑in . Next, we expect our '\n",
      "                       'memory module to learn the associations between keys '\n",
      "                       'and values. To\\n'\n",
      "                       'this end, we define the loss as follows:\\n'\n",
      "                       'ℓ(M𝑡−1; 𝑥𝑡)= ∥M𝑡−1 (k𝑡)−v𝑡∥2\\n'\n",
      "                       '2 (12)\\n'\n",
      "                       'By optimizing the above loss function in the '\n",
      "                       'inner-loop of our meta model (memory), the model '\n",
      "                       'learns how to memorize\\n'\n",
      "                       'the mapping between keys and values at test time. Note '\n",
      "                       'that, similar to meta-learning models (Nichol 2018; '\n",
      "                       'Zintgraf et al.\\n'\n",
      "                       '2019), training of the memory is in the inner-loop, '\n",
      "                       'and so parameters 𝑊𝐾 and 𝑊𝑉 are hyperparameters in the '\n",
      "                       'above loss\\n'\n",
      "                       'function. Accordingly, in the inner loop, we optimize '\n",
      "                       'M’s weights, while in the outer-loop, we optimize '\n",
      "                       'other parameters\\n'\n",
      "                       'of the entire architecture.\\n'\n",
      "                       'Forgetting Mechanism. When dealing with very large '\n",
      "                       'sequences (e.g., millions of tokens), it is crucial to '\n",
      "                       'manage which\\n'\n",
      "                       'past information should be forgotten–even with a deep '\n",
      "                       'or a very large matrix-valued memory. To this end, we '\n",
      "                       'use an\\n'\n",
      "                       'adaptive forgetting mechanism that allows the memory '\n",
      "                       'to forget the information that is not needed anymore, '\n",
      "                       'resulting in\\n'\n",
      "                       'better managing the memory’s limited capacity. That '\n",
      "                       'is, given the next token 𝑥𝑡, we modify the update rule '\n",
      "                       'as:\\n'\n",
      "                       'M𝑡 = (1 −𝛼𝑡)M𝑡−1 +𝑆𝑡, (13)\\n'\n",
      "                       '𝑆𝑡 = 𝜂𝑡𝑆𝑡−1 −𝜃𝑡 ∇ℓ(𝑀𝑡−1; 𝑥𝑡), (14)\\n'\n",
      "                       'where 𝛼𝑡 ∈[0,1]is the gating mechanism that flexibly '\n",
      "                       'controls the memory; i.e., decides how much '\n",
      "                       'information should be\\n'\n",
      "                       'forgotten. For example, it can update the memory '\n",
      "                       'without affecting the past abstraction by letting 𝛼𝑡 '\n",
      "                       '→0, and can clear\\n'\n",
      "                       'the entire memory by letting 𝛼𝑡 →1. Later in this '\n",
      "                       'section, we show that this weight decay mechanism is '\n",
      "                       'closely related to\\n'\n",
      "                       'the gating mechanism in modern RNNs (Dao and Gu 2024; '\n",
      "                       'Orvieto et al. 2023).\\n'\n",
      "                       'Memory Architecture. In this paper, we focus on simple '\n",
      "                       'MLPs with 𝐿M ≥1 layers as the architecture of our '\n",
      "                       'long-term\\n'\n",
      "                       'memory. The main reason behind this choice is that we '\n",
      "                       'want to focus on better motivating the design of the '\n",
      "                       'long-term\\n'\n",
      "                       'memory and ways that it can be incorporated into an '\n",
      "                       'architecture. However, our formulation and '\n",
      "                       'architectural design\\n'\n",
      "                       'opens a new research direction to design neural '\n",
      "                       'architectures that are more effective and efficient in '\n",
      "                       'memorization of data.\\n'\n",
      "                       'Recently, there has been a promising line of work to '\n",
      "                       'design such architectures (Berges et al. 2024; Cetin '\n",
      "                       'et al. 2024; J. Zhang\\n'\n",
      "                       'et al. 2024), which incorporating them into our '\n",
      "                       'framework (i.e., replacing simple MLPs with such '\n",
      "                       'architectures) can be an\\n'\n",
      "                       'interesting future work.\\n'\n",
      "                       'When using vector-valued or matrix-valued memory (De '\n",
      "                       'et al. 2024; Orvieto et al. 2023; S. Yang, B. Wang, '\n",
      "                       'Shen, et\\n'\n",
      "                       'al. 2024), the memory module is compressing the past '\n",
      "                       'data and fit it into a line. That is, from the meta '\n",
      "                       'learning or\\n'\n",
      "                       'online learning perspective (Yu Sun et al. 2024), '\n",
      "                       'using a matrix-valued memory M = 𝑊 ∈R𝑑in ×𝑑in is '\n",
      "                       'equivalent to\\n'\n",
      "                       'optimize ℓ(𝑊𝑡−1; 𝑥𝑡)= ∥𝑊𝑡−1k𝑡 −v𝑡∥2\\n'\n",
      "                       '2, which is an online linear regression objective and '\n",
      "                       'so the optimal solution assumes\\n'\n",
      "                       'the underlying dependency of historical data is '\n",
      "                       'linear. On the other hand, we argue that deep memory '\n",
      "                       'modules (i.e.,\\n'\n",
      "                       '𝐿M ≥2) . Aligning with the theoretical results that '\n",
      "                       'MLPs with at least two layers are strictly more '\n",
      "                       'expressive than linear\\n'\n",
      "                       'models (Hornik, Stinchcombe, and White 1989), in '\n",
      "                       'Section 5.5, we show that deep memory modules are more '\n",
      "                       'effective in\\n'\n",
      "                       'practice.\\n'\n",
      "                       'Retrieving a Memory. In the above, we discuss how one '\n",
      "                       'can design and train a long-term memory module that '\n",
      "                       'learns to\\n'\n",
      "                       'memorize at test time. A key remaining question is: '\n",
      "                       'How one can retrieve information from the memory? We '\n",
      "                       'simply use the\\n'\n",
      "                       'forward pass without weight update (i.e., inference) '\n",
      "                       'to retrieve a memory correspond to a query. Formally, '\n",
      "                       'given an input\\n'\n",
      "                       '𝑥𝑡, we use a linear layer 𝑊𝑄 to project the input, '\n",
      "                       'i.e., q𝑡 = 𝑥𝑡𝑊𝑄 and retrieve the corresponding (or '\n",
      "                       'useful) information\\n'\n",
      "                       'from the memory 𝑦𝑡 by:\\n'\n",
      "                       '𝑦𝑡 = M∗(q𝑡). (15)\\n'\n",
      "                       '6\\n'\n",
      "                       'Figure 1: The illustration of how the training of '\n",
      "                       'neural memory can be done in parallel and using '\n",
      "                       'matmuls.\\n'\n",
      "                       '3.2 How to Parallelize the Long-term Memory Training\\n'\n",
      "                       'As discussed above, the design of our long-term memory '\n",
      "                       'module is equivalent to training a meta model by '\n",
      "                       'optimizing\\n'\n",
      "                       'associative memory loss function ℓ(M𝑡−1; 𝑥𝑡)= ∥M𝑡−1 '\n",
      "                       '(k𝑡)−v𝑡∥2\\n'\n",
      "                       '2 using gradient descent with momentum and weight\\n'\n",
      "                       'decay. Therefore, in theory, the training of long-term '\n",
      "                       'memory module requires O(𝑁)FLOPs, where 𝑁 is the '\n",
      "                       'sequence\\n'\n",
      "                       'length. However, in practice, we need to parallelize '\n",
      "                       'the training process and to fully take advantage of '\n",
      "                       'hardware accelerators\\n'\n",
      "                       '(e.g., TPUs, GPUs), we need to tensorize the process '\n",
      "                       'and use more matmuls.\\n'\n",
      "                       'Next, we show that calculating the weights in the '\n",
      "                       'inner loop with mini-batch gradient descent, '\n",
      "                       'data-dependent learning\\n'\n",
      "                       'rate, and weight decay can be reformulated so that it '\n",
      "                       'uses only matmuls and sum. We build upon the work of '\n",
      "                       'Yu Sun et al.\\n'\n",
      "                       '(2024) that shows forward pass of a model optimizing '\n",
      "                       'with the mini-batch gradient descent (with constant '\n",
      "                       'learning rate)\\n'\n",
      "                       'can be calculated using matmuls. We can split the '\n",
      "                       'sequence into chunks of size 𝑏 ≥1, and write the '\n",
      "                       'mini-batch gradient\\n'\n",
      "                       'descent as:\\n'\n",
      "                       'M𝑡 = (1 −𝛼𝑡)M𝑡−1 −𝜃𝑡∇ℓ(M𝑡−1; 𝑥𝑡)= 𝛽𝑡M0 −\\n'\n",
      "                       '𝑡∑︁\\n'\n",
      "                       '𝑖=1\\n'\n",
      "                       '𝜃𝑖\\n'\n",
      "                       '𝛽𝑡\\n'\n",
      "                       '𝛽𝑖\\n'\n",
      "                       '∇ℓ(M𝑡′; 𝑥𝑖), (16)\\n'\n",
      "                       'where 𝑡′= 𝑡 −mod(𝑡,𝑏), and 𝛽𝑖 = Î𝑖\\n'\n",
      "                       '𝑗=1 (1 −𝛼𝑗). For the sake of simplicity, we focus on '\n",
      "                       'the first chunk, i.e., 𝑡 = 𝑏and so\\n'\n",
      "                       '𝑡′= 0. Also, we explain the process for the case that '\n",
      "                       'M𝑡 = 𝑊𝑡 is linear. The process for MLPs with 𝑁𝑝 ≥2 is '\n",
      "                       'similar. Using\\n'\n",
      "                       'our loss function, we have:\\n'\n",
      "                       '∇ℓ(𝑊0; 𝑥𝑡)= (𝑊0𝑥𝑡 −𝑥𝑡)𝑥⊤\\n'\n",
      "                       '𝑡 ⇒\\n'\n",
      "                       '𝑏∑︁\\n'\n",
      "                       '𝑖=1\\n'\n",
      "                       '𝜃𝑖\\n'\n",
      "                       '𝛽𝑏\\n'\n",
      "                       '𝛽𝑖\\n'\n",
      "                       '∇ℓ(𝑊0; 𝑥𝑖)= Θ𝑏B𝑏(𝑊0𝑋 −𝑋)𝑋⊤, (17)\\n'\n",
      "                       'where Θ𝑏 = diag \\x00\\x02𝜃1 𝜃2 ... 𝜃 𝑏\\n'\n",
      "                       '\\x03\\x01 and B𝑏 is defined analogously on 𝛽𝑏\\n'\n",
      "                       '𝛽𝑖\\n'\n",
      "                       's. Note that, we do not need to store allΘ𝑘𝑏 and\\n'\n",
      "                       'B𝑘𝑏 for 𝑘 = 1,...,𝑁 /𝑏, instead, we store these '\n",
      "                       'matrices for each chunk, resulting in using less '\n",
      "                       'memory. Next, we extend\\n'\n",
      "                       'this representation so we can also incorporate the '\n",
      "                       'momentum term. In a chunk wise gradient descent with '\n",
      "                       'momentum, if\\n'\n",
      "                       'we look at the momentum term, we have:\\n'\n",
      "                       '𝑆𝑡 = 𝜂𝑡𝑆𝑡−1 −𝜃𝑡 𝑢𝑡, (18)\\n'\n",
      "                       'where 𝑢𝑡 = ∇ℓ(𝑀𝑡′; 𝑥𝑡). Note that, we can compute all '\n",
      "                       '𝑢𝑡 at the same time, and so Equation 18 is a linear '\n",
      "                       'recurrence\\n'\n",
      "                       'with 𝑢𝑡 as an input, 𝑆𝑡 as the hidden state, and 𝜂𝑡 as '\n",
      "                       'input-dependent transition value. Accordingly, we can '\n",
      "                       'use parallel\\n'\n",
      "                       'associative scan (J. T. Smith, Warrington, and '\n",
      "                       'Linderman 2023) to calculate𝑆𝑡s in this chunk.\\n'\n",
      "                       'Parameters as the Function of Chunks. Instead of '\n",
      "                       'making parameters like𝛼𝑡,𝜃𝑡, and 𝜂𝑡 input-dependent '\n",
      "                       '(i.e., a function\\n'\n",
      "                       'of token 𝑥𝑡), we can make them functions of their '\n",
      "                       'chunk. Despite losing expressive power, this '\n",
      "                       'formulation can help to\\n'\n",
      "                       'make the training even faster. In this case, we are '\n",
      "                       'using the same value for each of 𝛼, 𝜃, and 𝜂in each '\n",
      "                       'chunk. Accordingly,\\n'\n",
      "                       'in Equation 17, we can store Θ using a single scaler. '\n",
      "                       'Similarly we can make Equation 18 faster. That is, '\n",
      "                       'when 𝜂and 𝜃 are\\n'\n",
      "                       'learnable but time-invariant inside each chunk, this '\n",
      "                       'equation becomes a linear time-invariant system (LTI), '\n",
      "                       'which can be\\n'\n",
      "                       'computed by a global convolution (Gu, Goel, and Re '\n",
      "                       '2022). In our experiments, we make these parameters as '\n",
      "                       'the functions\\n'\n",
      "                       'of tokens. However, such simplifications (i.e., as the '\n",
      "                       'function of chunks) can be the interest of future work '\n",
      "                       'to training\\n'\n",
      "                       'larger models in more efficient manner.\\n'\n",
      "                       '7\\n'\n",
      "                       'Figure 2: Memory as a Context (MAC) Architecture. This '\n",
      "                       'architecture includes three branches of (1) core, (2) '\n",
      "                       'contextual\\n'\n",
      "                       '(long-term) memory, and (3) persistent memory. The '\n",
      "                       'core branch concatenates the corresponding long-term '\n",
      "                       'and persistent\\n'\n",
      "                       'memories with the input sequence. Next, attention '\n",
      "                       'performs on the sequence and decides what part of the '\n",
      "                       'information\\n'\n",
      "                       'should store in the long-term memory. At the test '\n",
      "                       'time, parameters corresponds to contextual memory are '\n",
      "                       'still learning,\\n'\n",
      "                       'parameters corresponds to the core branch are '\n",
      "                       'responsible for in-context learning, and parameters of '\n",
      "                       'persistent memory\\n'\n",
      "                       'are responsible to store the knowledge about tasks and '\n",
      "                       'so are fixed.\\n'\n",
      "                       '3.3 Persistent Memory\\n'\n",
      "                       'Our long-term memory can also be seen as a contextual '\n",
      "                       'memory, meaning that the output is fully depend on the '\n",
      "                       'context.\\n'\n",
      "                       'Therefore, in addition to our long-term memory, we '\n",
      "                       'also use a set of learnable but input-independent '\n",
      "                       'parameters to act as\\n'\n",
      "                       'task-related memory. This type of memory has been '\n",
      "                       'referred to as persistent or meta-memory in the '\n",
      "                       'literature (X. Dong\\n'\n",
      "                       'et al. 2024; Sukhbaatar, Grave, et al. 2019). Given 𝑁𝑝 '\n",
      "                       '≥1, we use learnable parameters 𝑃 =\\n'\n",
      "                       '\\x02𝑝1 𝑝2 ... 𝑝 𝑁𝑝\\n'\n",
      "                       '\\x03\\n'\n",
      "                       'and\\n'\n",
      "                       'append it to the start of our sequence: i.e., given a '\n",
      "                       'context window size of 𝑁, we modify the input as:\\n'\n",
      "                       '𝑥new =\\n'\n",
      "                       '\\x02𝑝1 𝑝2 ... 𝑝 𝑁𝑝\\n'\n",
      "                       '\\x03\\n'\n",
      "                       '|| 𝑥, (19)\\n'\n",
      "                       'where ||is concatenation. Next, we discuss the '\n",
      "                       'motivation of persistent memory from three '\n",
      "                       'perspective:\\n'\n",
      "                       'Memory Perspective. As discussed earlier, our neural '\n",
      "                       'long-term memory is a contextual memory, in which all '\n",
      "                       'parameters\\n'\n",
      "                       'are input-dependent. An effective memory system, '\n",
      "                       'however, also needs input-independent parameters to '\n",
      "                       'store the\\n'\n",
      "                       'abstraction of the task knowledge. That is, mastering '\n",
      "                       'a task requires the memorization of the knowledge that '\n",
      "                       'how the task\\n'\n",
      "                       'can be done, and these parameters are responsible for '\n",
      "                       'storing such knowledge.\\n'\n",
      "                       'Feedforward Network Perspective. In the Transformer '\n",
      "                       'architectures, there are fully connected layers after '\n",
      "                       'the attention\\n'\n",
      "                       'module, which are shown to be similar to attention '\n",
      "                       'weights but with data-independent parameters. That is, '\n",
      "                       'Sukhbaatar,\\n'\n",
      "                       'Grave, et al. (2019) showed that replacing the ReLU in '\n",
      "                       'fully connected layers with Softmax can results in an '\n",
      "                       'attention-like\\n'\n",
      "                       'weights, in which weights are data-independent:\\n'\n",
      "                       '𝐹𝐹𝑁 (𝑥)= 𝑊𝑉 Softmax (𝑊𝐾𝑥). (20)\\n'\n",
      "                       'In fact, 𝑊𝐾 and 𝑊𝑉 are acting similar to 𝐾 and 𝑉 '\n",
      "                       'matrices in attention module when they are '\n",
      "                       'input-independent. The\\n'\n",
      "                       'persistent memory weights are expected to have the '\n",
      "                       'same functionality, meaning that using them in the '\n",
      "                       'first part of the\\n'\n",
      "                       'sequence leads to having input-independent attention '\n",
      "                       'weights (Sukhbaatar, Grave, et al. 2019).\\n'\n",
      "                       'Technical Perspective. Attention with causal mask has '\n",
      "                       'implicit bias toward initial tokens in the sequence, '\n",
      "                       'and so attention\\n'\n",
      "                       'weights are almost always highly active for initial '\n",
      "                       'tokens, resulting in performance damage. From the '\n",
      "                       'technical perspective,\\n'\n",
      "                       'these learnable parameters at the start of the '\n",
      "                       'sequence can mitigate such effect by redistributing '\n",
      "                       'the attention weights\\n'\n",
      "                       'more effectively (Han et al. 2024; Xiao et al. 2024).\\n'\n",
      "                       '8\\n'\n",
      "                       '(a) Memory as a Context (MAC). We segment the '\n",
      "                       'sequence\\n'\n",
      "                       'and use full causal attention in each window. Again, '\n",
      "                       'the first\\n'\n",
      "                       '𝑁𝑝 tokens are persistent memory and the next 𝑁𝑙 are '\n",
      "                       'long-term\\n'\n",
      "                       'memory tokens\\n'\n",
      "                       '(b) Memory as Gating (MAG). We use sliding window '\n",
      "                       'attention\\n'\n",
      "                       '(SWA) as a short-term memory and our neural memory '\n",
      "                       'module\\n'\n",
      "                       'as a long-term memory, combining by a gating.\\n'\n",
      "                       'Figure 3: Attention masks for different variants of '\n",
      "                       'Titans.\\n'\n",
      "                       '4 How to Incorporate Memory?\\n'\n",
      "                       'A\\n'\n",
      "                       'n important question that remained unanswered is: How '\n",
      "                       'one can effectively and efficiently incorporate the\\n'\n",
      "                       'designed neural memory into a deep learning '\n",
      "                       'architecture? As discussed earlier, from a memory '\n",
      "                       'perspective,\\n'\n",
      "                       'the pair of K and V matrices in transformers can be '\n",
      "                       'interpreted as an associative memory block. Due to '\n",
      "                       'their\\n'\n",
      "                       'accurate modeling of dependencies and so their limited '\n",
      "                       'context window, we interpret them as short-term memory '\n",
      "                       'modules,\\n'\n",
      "                       'attending to the current context window size. On the '\n",
      "                       'other hand, our neural memory with the ability to '\n",
      "                       'continuously\\n'\n",
      "                       'learn from data and store it in its weights can play '\n",
      "                       'the role of a a long-term memory. In this section, we '\n",
      "                       'aim to answer\\n'\n",
      "                       'the above question by proposing three different '\n",
      "                       'variants of Titans. Later in our experiments, we show '\n",
      "                       'that each of these\\n'\n",
      "                       'variants has its own advantages/disadvantages and also '\n",
      "                       'can show a trade-off between the efficiency and '\n",
      "                       'effectiveness in\\n'\n",
      "                       'very long-contexts.\\n'\n",
      "                       '4.1 Memory as a Context\\n'\n",
      "                       'In the first architecture design (see Figure 2), we '\n",
      "                       'treat the memory as a context to the current '\n",
      "                       'information. That is, given\\n'\n",
      "                       'a long sequence 𝑥 ∈R𝑁×𝑑in , we first chunk the '\n",
      "                       'sequence into fixed-size segments S(𝑖) for 𝑖 = 1,...,𝑁 '\n",
      "                       '/𝐶. Given the\\n'\n",
      "                       'incoming segment S(𝑡), we consider it as the current '\n",
      "                       'context and its past segment as the historical '\n",
      "                       'information. Therefore,\\n'\n",
      "                       'let M𝑡−1 be the state of long-term memory before '\n",
      "                       'segment S(𝑡), we use the input context as the query to '\n",
      "                       'the memory\\n'\n",
      "                       'M𝑡−1 to retrieve the corresponding information from '\n",
      "                       'the long-term memory. That is, we retrieve the past '\n",
      "                       'information that\\n'\n",
      "                       'corresponds to S(𝑡)as:\\n'\n",
      "                       'ℎ𝑡 = M∗\\n'\n",
      "                       '𝑡−1 (q𝑡), (21)\\n'\n",
      "                       'where q𝑡 = S(𝑡)𝑊𝑄. Next, we use this historical '\n",
      "                       'information along with our persistent memory '\n",
      "                       'parameters as the input\\n'\n",
      "                       'sequence to the attention module:\\n'\n",
      "                       '˜S\\n'\n",
      "                       '(𝑡)\\n'\n",
      "                       '=\\n'\n",
      "                       '\\x02𝑝1 𝑝2 ... 𝑝 𝑁𝑝\\n'\n",
      "                       '\\x03\\n'\n",
      "                       '|| ℎ𝑡 || S(𝑡), (22)\\n'\n",
      "                       '𝑦𝑡 = Attn\\n'\n",
      "                       '\\x10\\n'\n",
      "                       '˜S\\n'\n",
      "                       '(𝑡)\\x11\\n'\n",
      "                       '. (23)\\n'\n",
      "                       'The structure of the attention map over the entire '\n",
      "                       'sequence is shown in Figure 3a. We then use 𝑦𝑡 to '\n",
      "                       'update the long-term\\n'\n",
      "                       'memory module for the next segment and the final '\n",
      "                       'output:\\n'\n",
      "                       'M𝑡 = M𝑡−1 (𝑦𝑡), (24)\\n'\n",
      "                       '𝑜𝑡 = 𝑦𝑡 ⊗M∗\\n'\n",
      "                       '𝑡 (𝑦𝑡). (25)\\n'\n",
      "                       'Note that, in the above, we are updating the weight of '\n",
      "                       'M𝑡−1 through forward pass.\\n'\n",
      "                       'This architecture has two key advantages: (1) '\n",
      "                       'Attention by having both historical and current '\n",
      "                       'context, has the ability to\\n'\n",
      "                       'decides whether given the current data, the long-term '\n",
      "                       'memory information is needed. (2) The attention module '\n",
      "                       'helps\\n'\n",
      "                       '9\\n'\n",
      "                       'Figure 4: Memory as a Gate (MAG) Architecture. This '\n",
      "                       'architecture, similarly, has the three branches of (1) '\n",
      "                       'core, (2)\\n'\n",
      "                       'contextual memory, and (3) persistent memory. It, '\n",
      "                       'however, incorporates only persistent memory into the '\n",
      "                       'context and\\n'\n",
      "                       'combine memory with the core branch using a gating '\n",
      "                       'mechanism. At test time, the behavior is the same as '\n",
      "                       'Figure 2.\\n'\n",
      "                       'the long-term memory to store only useful information '\n",
      "                       'from the current context. That is, not all tokens in '\n",
      "                       'each segment\\n'\n",
      "                       'are useful and memorizing all of them can result in '\n",
      "                       'memory overflow. Therefore, attention is helping the '\n",
      "                       'memory to\\n'\n",
      "                       'understand which information is useful, better '\n",
      "                       'managing the memory capacity. (3) At test time: (i) '\n",
      "                       'persistent memory\\n'\n",
      "                       'parameters are fixed as they encodes the knowledge '\n",
      "                       'about the task, which should not be changed; (ii) the '\n",
      "                       'attention module\\n'\n",
      "                       'weights are in-context learner; and (iii) the '\n",
      "                       'long-term memory module is still learning (memorizing) '\n",
      "                       'the information at test\\n'\n",
      "                       'time. That is, we update the weights of the neural '\n",
      "                       'memory even at test time as weights are encoding the '\n",
      "                       'abstraction of\\n'\n",
      "                       'long past.\\n'\n",
      "                       '4.2 Gated Memory\\n'\n",
      "                       'In the next variant (see Figure 4), in one branch, we '\n",
      "                       'directly use the input data to update the long-term '\n",
      "                       'memory, and in the\\n'\n",
      "                       'second branch, we use a sliding window attention '\n",
      "                       '(SWA):\\n'\n",
      "                       '˜𝑥 =\\n'\n",
      "                       '\\x02𝑝1 𝑝2 ... 𝑝 𝑁𝑝\\n'\n",
      "                       '\\x03\\n'\n",
      "                       '|| 𝑥, (26)\\n'\n",
      "                       '𝑦 = SW-Attn∗(˜𝑥), (27)\\n'\n",
      "                       '𝑜 = 𝑦⊗M( ˜𝑥), (28)\\n'\n",
      "                       'where SW-Attn∗is sliding window attention with prefix '\n",
      "                       '(see Figure 3b). Note that, contrary to the previous '\n",
      "                       'design, we are\\n'\n",
      "                       'not segmenting the input data. Also, we abuse the '\n",
      "                       'notation and use M(𝑥)to refer to the final output of '\n",
      "                       'the memory after\\n'\n",
      "                       'all recursion over the tokens of the sequence. In the '\n",
      "                       'above equation, ⊗can be any non-linear gating. In our '\n",
      "                       'experiments,\\n'\n",
      "                       'we normalize the outputs 𝑦and M(˜𝑥)using learnable '\n",
      "                       'vector-valued weights, followed by a non-linearity '\n",
      "                       '𝜎(.).\\n'\n",
      "                       'The overall attention mask of this design is shown in '\n",
      "                       'Figure 3b. In this design, sliding window attention is '\n",
      "                       'act as a precise\\n'\n",
      "                       'short-term memory, while the neural memory module is '\n",
      "                       'acting as a fading memory for the model. This '\n",
      "                       'architecture design\\n'\n",
      "                       'can also be seen as a multi-head architecture where '\n",
      "                       'the structure of heads are different (X. Dong et al. '\n",
      "                       '2024).\\n'\n",
      "                       '4.3 Memory as a Layer\\n'\n",
      "                       'The last variant uses the neural Memory As a Layer '\n",
      "                       '(MAL) of a deep neural network (see Figure 5). This '\n",
      "                       'architecture\\n'\n",
      "                       'design is more common in the literature, where the '\n",
      "                       'hybrid models stack recurrent models with full or '\n",
      "                       'sliding window\\n'\n",
      "                       'attentions. Given input 𝑥, we have:\\n'\n",
      "                       '˜𝑥 =\\n'\n",
      "                       '\\x02𝑝1 𝑝2 ... 𝑝 𝑁𝑝\\n'\n",
      "                       '\\x03\\n'\n",
      "                       '|| 𝑥, (29)\\n'\n",
      "                       '𝑦 = M(˜𝑥), (30)\\n'\n",
      "                       '𝑜 = SW-Attn (𝑦), (31)\\n'\n",
      "                       '10\\n'\n",
      "                       'Figure 5: Memory as a Layer (MAL) Architecture. In '\n",
      "                       'this architecture, the memory layer is responsible to '\n",
      "                       'compress the\\n'\n",
      "                       'past and current context before the attention module.\\n'\n",
      "                       'where SW-Attn is sliding window attention. The main '\n",
      "                       'drawback of this design is that the power of the model '\n",
      "                       'is limited by\\n'\n",
      "                       'each of the layers and so it cannot take advantage of '\n",
      "                       'the complementary data processing of attention and '\n",
      "                       'neural memory\\n'\n",
      "                       'module. In our experiments, for evaluating memory in '\n",
      "                       'this design, we use a similar architecture as H3 (D. '\n",
      "                       'Y. Fu et al. 2023),\\n'\n",
      "                       'where we replace the the sequence model with our '\n",
      "                       'neural memory module (LMM).\\n'\n",
      "                       'Memory Without Attention. Although in the above, we '\n",
      "                       'discussed MAL as the combination of LMMs and attention '\n",
      "                       'in\\n'\n",
      "                       'a sequential manner, one simple variant of MAL is to '\n",
      "                       'treat LMM as a sequence model without any attention. '\n",
      "                       'From the\\n'\n",
      "                       'memory perspective, as discussed in Section 1, we '\n",
      "                       'expect each part of the memory system to work '\n",
      "                       'independently, even if\\n'\n",
      "                       'other components are disturbed. Therefore, a long-term '\n",
      "                       'memory module should still be a powerful model even '\n",
      "                       'without\\n'\n",
      "                       'short-term memory (i.e., attention). We refer to this '\n",
      "                       'variant as LMM or Titans (LMM) in our experiments. We '\n",
      "                       'provide\\n'\n",
      "                       'additional discussions on the connection of Titans and '\n",
      "                       'other modern recurrent models in Appendix C.\\n'\n",
      "                       '4.4 Architectural Details\\n'\n",
      "                       'For the sake of simplicity and presentation, we avoid '\n",
      "                       'discussing the implementation details like using '\n",
      "                       'residual connection,\\n'\n",
      "                       'gating with linear layer, and normalization. In all '\n",
      "                       'blocks, we use residual connections. In our '\n",
      "                       'implementation, we use\\n'\n",
      "                       'SiLU(.) activation (Elfwing, Uchibe, and Doya 2018) as '\n",
      "                       'the non-linear activation for computing query, key, '\n",
      "                       'and values and\\n'\n",
      "                       'normalize queries and keys using ℓ2-norm.\\n'\n",
      "                       'Convolution. Following the recent modern linear '\n",
      "                       'recurrent models (Gu and Dao 2024; S. Yang, Kautz, and '\n",
      "                       'Hatamizadeh\\n'\n",
      "                       '2024), we incorporate a 1D depthwise-separable '\n",
      "                       'convolution layer after each of the query, key, and '\n",
      "                       'value projections.\\n'\n",
      "                       'While not significantly affect the performance, these '\n",
      "                       '1D convolutions have shown performance improvement and '\n",
      "                       'are also\\n'\n",
      "                       'computationally efficient.\\n'\n",
      "                       'Gating. We also follow the recent architectures that '\n",
      "                       'use normalization and gating with a linear layer '\n",
      "                       'before the final\\n'\n",
      "                       'output projection (Mehta et al. 2023).\\n'\n",
      "                       'Theorem 4.1. Contrary to Transformers, diagonal linear '\n",
      "                       'recurrent models, and DeltaNet, all of which are '\n",
      "                       'limited to TC0 (Merrill,\\n'\n",
      "                       'Petty, and Sabharwal 2024), Titans are capable of '\n",
      "                       'solving problems beyond TC 0, meaning that Titans are '\n",
      "                       'theoretically more\\n'\n",
      "                       'expressive than Transformers and most modern linear '\n",
      "                       'recurrent models in state tracking tasks.\\n'\n",
      "                       '5 Experiments\\n'\n",
      "                       'N\\n'\n",
      "                       'ext, we evaluate the performance of Titans and its '\n",
      "                       'variants in language modeling, commonsense reasoning, '\n",
      "                       'needle\\n'\n",
      "                       'in haystack, DNA modeling, and time series forecasting '\n",
      "                       'tasks1. In more details, in this section, we answer '\n",
      "                       'the\\n'\n",
      "                       'following empirical questions: (1) How do Titans '\n",
      "                       'perform compared to baselines in downstream tasks? '\n",
      "                       '(see §5.2,\\n'\n",
      "                       '1In the first version of the work, we aim to provide '\n",
      "                       'insights/evidences about why the learning paradigms of '\n",
      "                       'Titans are effective. We are working on\\n'\n",
      "                       'finalizing the results of larger models and will '\n",
      "                       'report them in the next version.\\n'\n",
      "                       '11\\n'\n",
      "                       '§5.6, and §5.7); (2) What is the actual context length '\n",
      "                       'of Titans? (see §5.3 and §5.4); (3) How do Titans '\n",
      "                       'scale with respect to\\n'\n",
      "                       'context length? (see §5.8); (4) How the depth of '\n",
      "                       'memory can affect both performance and efficiency? '\n",
      "                       '(see §5.5); and (5)\\n'\n",
      "                       'What is the contribution of each Titans’ component in '\n",
      "                       'its performance? (see §5.9).\\n'\n",
      "                       '5.1 Experimental Setup\\n'\n",
      "                       'Models. In our experiments, we focus on the three '\n",
      "                       'variants of Titans, which we refer to as: Titans with '\n",
      "                       '(1) Memory as a\\n'\n",
      "                       'Context (MAC), (2) Memory as a Gate (MAG), and (3) '\n",
      "                       'Memory as a Layer (MAL) as well as (4) neural memory '\n",
      "                       'module\\n'\n",
      "                       'alone. The reason behind using our long-term memory as '\n",
      "                       'a separate module is based on our definition of '\n",
      "                       'learning. As\\n'\n",
      "                       'discussed in Section 1, we define learning a process '\n",
      "                       'for acquiring effective and useful memory. '\n",
      "                       'Accordingly, we expect our\\n'\n",
      "                       'long-term memory to effectively learn from data, even '\n",
      "                       'without attention. For each of these models, we '\n",
      "                       'consider four scales\\n'\n",
      "                       'with: (i) 170M, (ii) 340M, (iii) 400M, and (iv) 760M '\n",
      "                       'parameters. While the first three are trained on 15B '\n",
      "                       'tokens sampled\\n'\n",
      "                       'from FineWeb-Edu dataset (Penedo et al. 2024), the '\n",
      "                       'last one is trained on 30B tokens from the same '\n",
      "                       'dataset.\\n'\n",
      "                       'Baselines. We compare our models with the '\n",
      "                       'state-of-the-art linear recurrent models, '\n",
      "                       'Transformers, and hybrid models\\n'\n",
      "                       '(recurrent + attention). More specifically in language '\n",
      "                       'tasks, we compare with Transformer++ (Touvron et al. '\n",
      "                       '2023),\\n'\n",
      "                       'RetNet (Yutao Sun et al. 2023), Gated Linear Attention '\n",
      "                       '(GLA) (S. Yang, B. Wang, Shen, et al. 2024), Mamba (Gu '\n",
      "                       'and Dao\\n'\n",
      "                       '2024), Mamba2 (Dao and Gu 2024), DeltaNet (S. Yang, B. '\n",
      "                       'Wang, Yu Zhang, et al. 2024), TTT (Yu Sun et al. '\n",
      "                       '2024), and Gated\\n'\n",
      "                       'DeltaNet (S. Yang, Kautz, and Hatamizadeh 2024). In '\n",
      "                       'needle in haystack tasks, we also compare with GPT4 '\n",
      "                       '(Achiam et al.\\n'\n",
      "                       '2023), Llama3 with RAG (Touvron et al. 2023), '\n",
      "                       'RecurrentGemma2-9B (Botev et al. 2024), and Mistral '\n",
      "                       '(Jiang et al. 2023)\\n'\n",
      "                       'models, all of which are provided in the benchmark '\n",
      "                       '(Yuri Kuratov et al. 2024). In time series tasks, we '\n",
      "                       'compare with\\n'\n",
      "                       'Mamba-based (Behrouz, Santacatterina, and Zabih 2024), '\n",
      "                       'Transformer-based (Y. Liu et al. 2023; Nie et al. '\n",
      "                       '2022; Yunhao\\n'\n",
      "                       'Zhang and Yan 2023), and linear models (Das et al. '\n",
      "                       '2023; Z. Li et al. 2023; H. Wu et al. 2023; Zeng et '\n",
      "                       'al. 2023).\\n'\n",
      "                       'Training. In the training, we follow the training '\n",
      "                       'procedure of S. Yang, Kautz, and Hatamizadeh (2024), '\n",
      "                       'and use LLama 2\\n'\n",
      "                       'tokenizer with a vocabulary size of 32K and use '\n",
      "                       'training length of 4K tokens. We employ AdamW '\n",
      "                       'optimizer with learning\\n'\n",
      "                       'rate of 4𝑒-4 with cosine annealing schedule with batch '\n",
      "                       'size of 0.5M tokens, and weight decay of 0.1.\\n'\n",
      "                       '5.2 Language Modeling\\n'\n",
      "                       'We first focus on the perplexity in language modeling '\n",
      "                       'and also commonsense reasoning tasks. The results for '\n",
      "                       'Titans’\\n'\n",
      "                       'variants and also baselines with three different sizes '\n",
      "                       'of 340M, 400M, and 760M are reported in Table 1. Among '\n",
      "                       'non-hybrid\\n'\n",
      "                       'models, including Transformer++, our neural memory '\n",
      "                       'module achieves the best performance in both '\n",
      "                       'perplexity and\\n'\n",
      "                       'accuracy measures. Comparing our neural memory module '\n",
      "                       'and TTT, which is also a gradient-based recurrent '\n",
      "                       'model can\\n'\n",
      "                       'show us the importance of our weight decay as well as '\n",
      "                       'the momentum. As discussed earlier, the weight decay '\n",
      "                       'can be\\n'\n",
      "                       'interpreted as a gating mechanism to forget the past '\n",
      "                       'data, when it is needed. Also, momentum can help us '\n",
      "                       'better manage\\n'\n",
      "                       'the memory by providing additional memory for the '\n",
      "                       'surprise metric. While some baselines also take '\n",
      "                       'advantage of gating\\n'\n",
      "                       'mechanism, e.g., Mamba, Mamba2, and Gated DeltaNet, '\n",
      "                       'the superior performance of our neural memory module '\n",
      "                       'shows\\n'\n",
      "                       'the importance of both our surprise mechanism and '\n",
      "                       'having deep and non-linear memory. We further discuss '\n",
      "                       'the later in\\n'\n",
      "                       'Section 5.5.\\n'\n",
      "                       'Comparing the hybrid models, we found that all three '\n",
      "                       'variants of Titans (MAC, MAG, and MAL) outperform both '\n",
      "                       'Samba\\n'\n",
      "                       '(Mamba + attention) and Gated DeltaNet-H2 (Gated '\n",
      "                       'DeltaNet + atttention). We attribute the superior '\n",
      "                       'performance of Titans\\n'\n",
      "                       '(MAL) to the power of neural memory module as the '\n",
      "                       'architecture design and used attention are all the '\n",
      "                       'same. Comparing\\n'\n",
      "                       'Titans (MAG) and (MAC), we find that while their '\n",
      "                       'performance are close, MAC performs better when '\n",
      "                       'dealing with longer\\n'\n",
      "                       'dependencies in the data. Interestingly, both MAG and '\n",
      "                       'MAC outperform MAL variant, which due to using the '\n",
      "                       'same\\n'\n",
      "                       'modules, we attribute this to the architecture design '\n",
      "                       'of these models. This finding is particularly '\n",
      "                       'important as the current\\n'\n",
      "                       'hybrid models (except Hymba (X. Dong et al. 2024)) in '\n",
      "                       'the literature are using MAL-style combination of '\n",
      "                       'recurrent models\\n'\n",
      "                       'and attention.\\n'\n",
      "                       '5.3 Needle in a Haystack\\n'\n",
      "                       'Scaling a model to longer context window is not always '\n",
      "                       'equivalent to being effective for very long sequences '\n",
      "                       '(Hsieh\\n'\n",
      "                       'et al. 2024). The needle-in-a-haystack (NIAH) task is '\n",
      "                       'designed to measure the actual effective context '\n",
      "                       'length of models.\\n'\n",
      "                       'In this task, we evaluate the model on retrieving a '\n",
      "                       'piece of information (i.e., the “needle”) from long '\n",
      "                       'distractor texts (i.e.,\\n'\n",
      "                       '12\\n'\n",
      "                       'Table 1: Performance of Titans and recurrent- and '\n",
      "                       'Transformer-based baselines on language modeling and '\n",
      "                       'common-sense\\n'\n",
      "                       'reasoning tasks. Hybrid models are marked with ∗. The '\n",
      "                       'best results among simple and hybrid models are '\n",
      "                       'highlighted.\\n'\n",
      "                       'Model Wiki. LMB. LMB. PIQA Hella. Wino. ARC-e ARC-c '\n",
      "                       'SIQA BoolQ Avg.\\n'\n",
      "                       'ppl↓ ppl↓ acc↑ acc↑ acc_n↑ acc↑ acc↑ acc_n↑ acc↑ acc↑ '\n",
      "                       '↑\\n'\n",
      "                       '340M params / 15B tokens\\n'\n",
      "                       'Transformer++ 31.52 41.08 30.76 62.98 34.76 50.53 '\n",
      "                       '45.21 24.05 36.81 58.24 42.92\\n'\n",
      "                       'RetNet 32.50 49.73 28.24 62.61 34.15 50.91 44.27 23.62 '\n",
      "                       '36.79 59.72 42.54\\n'\n",
      "                       'GLA 28.51 43.02 28.73 64.05 35.96 50.00 54.19 24.29 '\n",
      "                       '37.13 58.39 44.09\\n'\n",
      "                       'Mamba 30.83 40.21 29.94 63.79 35.88 49.82 49.24 24.56 '\n",
      "                       '35.41 60.07 43.59\\n'\n",
      "                       'DeltaNet 28.65 47.30 28.43 63.52 35.95 49.63 52.68 '\n",
      "                       '25.37 37.96 58.79 44.04\\n'\n",
      "                       'TTT 27.44 34.19 30.06 63.97 35.71 50.08 53.01 26.11 '\n",
      "                       '37.32 59.83 44.51\\n'\n",
      "                       'Gated DeltaNet 27.01 30.94 34.11 63.08 38.12 51.60 '\n",
      "                       '55.28 26.77 34.89 59.54 45.42\\n'\n",
      "                       'Titans (LMM) 26.18 29.97 34.98 64.73 39.61 51.85 55.60 '\n",
      "                       '28.14 34.52 59.99 46.17\\n'\n",
      "                       'Titans (MAC)∗ 25.43 28.13 36.00 65.32 40.35 51.21 '\n",
      "                       '58.17 29.00 38.63 60.18 47.36\\n'\n",
      "                       'Titans (MAG)∗ 25.07 28.72 36.71 64.88 40.56 52.49 '\n",
      "                       '57.72 28.16 39.75 60.01 47.54\\n'\n",
      "                       'Titans (MAL)∗ 24.69 28.80 35.74 64.97 39.44 51.97 '\n",
      "                       '56.58 28.21 38.14 57.32 46.55\\n'\n",
      "                       '400M params / 15B tokens\\n'\n",
      "                       'Transformer++ 30.63 37.37 29.64 64.27 37.72 51.53 '\n",
      "                       '54.95 27.36 38.07 61.59 45.64\\n'\n",
      "                       'RetNet 29.92 46.83 29.16 65.23 36.97 51.85 56.01 27.55 '\n",
      "                       '37.30 59.66 45.47\\n'\n",
      "                       'HGRN2 32.33 47.14 26.12 64.52 35.45 52.24 55.97 25.51 '\n",
      "                       '37.35 59.02 44.52\\n'\n",
      "                       'GLA 27.96 36.66 27.86 65.94 37.41 49.56 56.01 26.36 '\n",
      "                       '38.94 59.84 45.24\\n'\n",
      "                       'Mamba 29.22 39.88 29.82 65.72 37.93 50.11 58.37 26.70 '\n",
      "                       '37.76 61.13 45.94\\n'\n",
      "                       'Mamba2 26.34 33.19 32.03 65.77 39.73 52.48 59.00 27.64 '\n",
      "                       '37.92 60.72 46.91\\n'\n",
      "                       'DeltaNet 27.69 44.04 29.96 64.52 37.03 50.82 56.77 '\n",
      "                       '27.13 38.22 60.09 45.57\\n'\n",
      "                       'TTT 26.11 31.52 33.25 65.70 39.11 51.68 58.04 28.99 '\n",
      "                       '38.26 59.87 46.86\\n'\n",
      "                       'Gated DeltaNet 25.47 29.24 34.40 65.94 40.46 51.46 '\n",
      "                       '59.80 28.58 37.43 60.03 47.26\\n'\n",
      "                       'Samba∗ 25.32 29.47 36.86 66.09 39.24 51.45 60.12 27.20 '\n",
      "                       '38.68 58.22 47.23\\n'\n",
      "                       'Gated DeltaNet-H2∗ 24.19 28.09 36.77 66.43 40.79 52.17 '\n",
      "                       '59.55 29.09 39.04 58.56 47.69\\n'\n",
      "                       'Titans (LMM) 25.03 28.99 35.21 65.85 40.91 52.19 59.97 '\n",
      "                       '29.20 38.74 60.85 47.83\\n'\n",
      "                       'Titans (MAC)∗ 25.61 27.73 36.92 66.39 41.18 52.80 '\n",
      "                       '60.24 29.69 40.07 61.93 48.65\\n'\n",
      "                       'Titans (MAG)∗ 23.59 27.81 37.24 66.80 40.92 53.21 '\n",
      "                       '60.01 29.45 39.91 61.28 48.60\\n'\n",
      "                       'Titans (MAL)∗ 23.93 27.89 36.84 66.29 40.74 52.26 '\n",
      "                       '59.85 29.71 38.92 58.40 47.87\\n'\n",
      "                       '760M params / 30B tokens\\n'\n",
      "                       'Transformer++ 25.21 27.64 35.78 66.92 42.19 51.95 '\n",
      "                       '60.38 32.46 39.51 60.37 48.69\\n'\n",
      "                       'RetNet 26.08 24.45 34.51 67.19 41.63 52.09 63.17 32.78 '\n",
      "                       '38.36 57.92 48.46\\n'\n",
      "                       'Mamba 28.12 23.96 32.80 66.04 39.15 52.38 61.49 30.34 '\n",
      "                       '37.96 57.62 47.22\\n'\n",
      "                       'Mamba2 22.94 28.37 33.54 67.90 42.71 49.77 63.48 31.09 '\n",
      "                       '40.06 58.15 48.34\\n'\n",
      "                       'DeltaNet 24.37 24.60 37.06 66.93 41.98 50.65 64.87 '\n",
      "                       '31.39 39.88 59.02 48.97\\n'\n",
      "                       'TTT 24.17 23.51 34.74 67.25 43.92 50.99 64.53 33.81 '\n",
      "                       '40.16 59.58 47.32\\n'\n",
      "                       'Gated DeltaNet 21.18 22.09 35.54 68.01 44.95 50.73 '\n",
      "                       '66.87 33.09 39.21 59.14 49.69\\n'\n",
      "                       'Samba∗ 20.63 22.71 39.72 69.19 47.35 52.01 66.92 33.20 '\n",
      "                       '38.98 61.24 51.08\\n'\n",
      "                       'Gated DeltaNet-H2∗ 19.88 20.83 39.18 68.95 48.22 52.57 '\n",
      "                       '67.01 35.49 39.39 61.11 51.49\\n'\n",
      "                       'Titans (LMM) 20.04 21.96 37.40 69.28 48.46 52.27 66.31 '\n",
      "                       '35.84 40.13 62.76 51.56\\n'\n",
      "                       'Titans (MAC) 19.93 20.12 39.62 70.46 49.01 53.18 67.86 '\n",
      "                       '36.01 41.87 62.05 52.51\\n'\n",
      "                       'Titans (MAG) 18.61 19.86 40.98 70.25 48.94 52.89 68.23 '\n",
      "                       '36.19 40.38 62.11 52.50\\n'\n",
      "                       'Titans (MAL) 19.07 20.33 40.05 69.99 48.82 53.02 67.54 '\n",
      "                       '35.65 30.98 61.72 50.97\\n'\n",
      "                       'the “haystack”). In this part, we use Single NIAH '\n",
      "                       '(S-NIAH) task from RULER benchmark (Hsieh et al. 2024) '\n",
      "                       'and evaluate\\n'\n",
      "                       'Titans and baselines on sequences with length 2K, 4K, '\n",
      "                       '8K, and 16K. The results are reported in Table 2. '\n",
      "                       'Neural Memory\\n'\n",
      "                       'module achieves the best results compare to baselines '\n",
      "                       'in all three tasks. We attribute this superior '\n",
      "                       'performance to three\\n'\n",
      "                       'key differences of Titans with existing sequence '\n",
      "                       'models: (1) Compared to TTT, our Neural Memory can '\n",
      "                       'better handle the\\n'\n",
      "                       'memory capacity by using momentum and also the '\n",
      "                       'forgetting mechanism (i.e., weight decay). Therefore, '\n",
      "                       'with increasing\\n'\n",
      "                       'the sequence length, the performance of Neural Memory '\n",
      "                       'does not drop and show a consistent trend; (2) '\n",
      "                       'Compared to\\n'\n",
      "                       'Mamba2, which has the gating (forgetting) mechanism, '\n",
      "                       'Titans have deep non-linear memory, resulting in '\n",
      "                       'better memory\\n'\n",
      "                       'management. Also, contrary to our neural memory and '\n",
      "                       'DeltaNet, Mamba2 is not capable of removing a memory '\n",
      "                       'and so\\n'\n",
      "                       '13\\n'\n",
      "                       'Table 2: Performance of Titans and baselines on S-NIAH '\n",
      "                       'task from RULER benchmark. The best results among '\n",
      "                       'simple\\n'\n",
      "                       'and hybrid models are highlighted.\\n'\n",
      "                       'Model S-NIAH-PK S-NIAH-N S-NIAH-W\\n'\n",
      "                       '2K 4K 8K 16K 2K 4K 8K 16K 2K 4K 8K 16K\\n'\n",
      "                       'TTT 98.4 98.8 98.0 88.4 60.2 36.6 10.2 4.4 78.8 28.0 '\n",
      "                       '4.4 0.0\\n'\n",
      "                       'Mamba2 98.6 61.4 31.0 5.4 98.4 55.8 14.2 0.0 42.2 4.2 '\n",
      "                       '0.0 0.0\\n'\n",
      "                       'DeltaNet 96.8 98.8 98.6 71.4 47.2 15.4 12.8 5.4 46.2 '\n",
      "                       '20.0 1.6 0.0\\n'\n",
      "                       'Titans (LMM)99.8 98.4 98.2 96.2 100.0 99.8 93.4 80.2 '\n",
      "                       '90.4 89.4 85.8 80.6\\n'\n",
      "                       'Titans (MAC) 99.2 98.8 99.0 98.4 99.6 98.2 97.6 97.4 '\n",
      "                       '98.2 98.2 95.6 95.2\\n'\n",
      "                       'Titans (MAG)99.4 98.0 97.4 97.4 99.2 98.8 97.2 98.6 '\n",
      "                       '98.0 98.0 90.2 88.2\\n'\n",
      "                       'Titans (MAL) 98.8 98.6 98.8 97.8 99.8 98.1 96.8 96.4 '\n",
      "                       '98.0 97.4 92.0 90.4\\n'\n",
      "                       '(a) Few-shot Setup\\n'\n",
      "                       ' (b) Fine-Tuning Setup\\n'\n",
      "                       'Figure 6: Performance of Titans and baselines on '\n",
      "                       'BABILong benchmark. Titans (MAC) outperforms all '\n",
      "                       'baselines, including\\n'\n",
      "                       'extremely large models, e.g., GPT4.\\n'\n",
      "                       'we can see a significant drop in performance when '\n",
      "                       'increasing the sequence length; (3) Compared to '\n",
      "                       'DeltaNet, although it\\n'\n",
      "                       'is capable of removing memory using delta rule, it '\n",
      "                       'cannot erase the memory, lacking forgetting mechanism. '\n",
      "                       'Finally, As\\n'\n",
      "                       'expected we can see on par or better results when '\n",
      "                       'using Titans variants, where the best results '\n",
      "                       'correspond to MAC.\\n'\n",
      "                       '5.4 BABILong Benchmark\\n'\n",
      "                       'In the previous section we discussed the results on a '\n",
      "                       'simple NIAH tasks where a single needle needs to be '\n",
      "                       'retrieved.\\n'\n",
      "                       'Although Titans showed better performance compared to '\n",
      "                       'baselines, their true advantage over very long '\n",
      "                       'sequences is still\\n'\n",
      "                       'hidden. To this end, in this section, we use a harder '\n",
      "                       'task from BABILong benchmark (Yuri Kuratov et al. '\n",
      "                       '2024), in which\\n'\n",
      "                       'the model needs to reason across facts distributed in '\n",
      "                       'extremely long documents. We follow the original '\n",
      "                       'experimental setup\\n'\n",
      "                       'and training process in the benchmark. There are two '\n",
      "                       'settings: (1) Few-shot setting, in which we use large '\n",
      "                       'pre-trained\\n'\n",
      "                       'models, and (2) fine-tuning setting, where we '\n",
      "                       'fine-tune the MAC variant of Titans to compare it with '\n",
      "                       'other fine-tuned\\n'\n",
      "                       'baselines. The results for few-shot setting are '\n",
      "                       'reported in Figure 6a. In this setup, we can see '\n",
      "                       'Titans outperform all\\n'\n",
      "                       'baselines–i.e., Mamba2.8B (Gu and Dao 2024), RWKV-6-7B '\n",
      "                       '(Peng, Goldstein, et al. 2024), RecurrentGemma-9B '\n",
      "                       '(Botev et al.\\n'\n",
      "                       '2024), Gemma-9B (Team et al. 2024), Llama3.1-8B '\n",
      "                       '(Touvron et al. 2023), GPT-4, and GPT4o-mini (Achiam '\n",
      "                       'et al. 2023). These\\n'\n",
      "                       'results are achieved while Titans (MAC) is having much '\n",
      "                       'less number of parameters than baselines.\\n'\n",
      "                       'In the fine-tuning setup, we compare the small '\n",
      "                       'fine-tuned version of Titans (MAC) with: (i) the '\n",
      "                       'fine-tuned version of small\\n'\n",
      "                       'models (almost the same number of parameters as '\n",
      "                       'Titans) such as Mamba (Gu and Dao 2024), RMT (Bulatov, '\n",
      "                       'Yury Kuratov,\\n'\n",
      "                       'and Burtsev 2022), (ii) large models with '\n",
      "                       'Retrieval-Augmented Generation (RAG) (P. Lewis et al. '\n",
      "                       '2020) such as Llama3.1-\\n'\n",
      "                       '8B (Touvron et al. 2023), and (iii) extremely large '\n",
      "                       'models such as GPT-4 (Achiam et al. 2023), GPT4o-mini, '\n",
      "                       'Qwen2.5-72B (A.\\n'\n",
      "                       'Yang et al. 2024), and Llama3.1-70B (Touvron et al. '\n",
      "                       '2023). Baseline results are reported by (Yuri Kuratov '\n",
      "                       'et al. 2024). The\\n'\n",
      "                       'results of Titans and baselines are reported in Figure '\n",
      "                       '6b. Titans outperform all models even extremely large '\n",
      "                       'models like\\n'\n",
      "                       'GPT4. Also, compared to Transformer-based with memory '\n",
      "                       'models like RMT, Titans show better performance mainly '\n",
      "                       'due\\n'\n",
      "                       'to their powerful memory. That is, RMT compress the '\n",
      "                       'historical data into 16 size vector-valued memory, '\n",
      "                       'while Titans with\\n'\n",
      "                       'in-context online memory learner are capable of '\n",
      "                       'encoding the past into the parameters of the model. '\n",
      "                       'Interestingly, even\\n'\n",
      "                       '14\\n'\n",
      "                       '(a) 170M Parameters\\n'\n",
      "                       ' (b) 360M Parameters\\n'\n",
      "                       ' (c) 760M Parameters\\n'\n",
      "                       'Figure 7: The effect of memory depth on the '\n",
      "                       'perplexity. Deeper long-term memory results in better '\n",
      "                       'scaling in longer\\n'\n",
      "                       'sequences.\\n'\n",
      "                       'Table 3: Performance on long-term forecasting. The '\n",
      "                       'best results are highlighted .\\n'\n",
      "                       'Neural MemorySimba iTransformerRLinear '\n",
      "                       'PatchTSTCrossformerTiDE TimesNet DLinear\\n'\n",
      "                       'MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE '\n",
      "                       'MAE MSE MAE MSE MAE\\n'\n",
      "                       'ETTm1 0.358 0.387 0.3830.3960.407 0.410 '\n",
      "                       '0.4140.4070.3870.4000.5130.496 '\n",
      "                       '0.4190.4190.4000.4060.4030.407\\n'\n",
      "                       'ETTm2 0.261 0.309 0.2710.3270.288 0.332 '\n",
      "                       '0.2860.3270.2810.3260.7570.610 '\n",
      "                       '0.3580.4040.2910.3330.3500.401\\n'\n",
      "                       'ETTh1 0.420 0.421 0.4410.4320.454 0.447 '\n",
      "                       '0.4460.4340.4690.4540.5290.522 '\n",
      "                       '0.5410.5070.4580.4500.4560.452\\n'\n",
      "                       'ETTh2 0.336 0.382 0.3610.3910.383 0.407 '\n",
      "                       '0.3740.3980.3870.4070.9420.684 '\n",
      "                       '0.6110.5500.4140.4270.5590.515\\n'\n",
      "                       'ECL 0.162 0.261 0.1690.2740.178 0.270 '\n",
      "                       '0.2190.2980.2050.2900.2440.334 '\n",
      "                       '0.2510.3440.1920.2950.2120.300\\n'\n",
      "                       'Traffic 0.415 0.289 0.4930.2910.428 0.282 '\n",
      "                       '0.6260.3780.4810.3040.5500.304 '\n",
      "                       '0.7600.4730.6200.3360.6250.383\\n'\n",
      "                       'Weather0.231 0.265 0.2550.2800.258 0.278 '\n",
      "                       '0.2720.2910.2590.2810.2590.315 '\n",
      "                       '0.2710.3200.2590.2870.2650.317\\n'\n",
      "                       'augmenting Llama3.1-8B model with RAG performs worse '\n",
      "                       'than Titans with about ×70 less parameters.\\n'\n",
      "                       '5.5 The Effect of Deep Memory\\n'\n",
      "                       'In this section, we evaluate the effect of deep memory '\n",
      "                       'in both wall-clock training time and model '\n",
      "                       'performance2. To this\\n'\n",
      "                       'end, we focus on different variants of our neural '\n",
      "                       'memory module, where 𝐿M= 1,2,3,4. We also use Mamba as '\n",
      "                       'a baseline\\n'\n",
      "                       'for the model performance. For a fair comparison, we '\n",
      "                       'use the same training process for all models and train '\n",
      "                       'them on a\\n'\n",
      "                       'subset of the Pile dataset (L. Gao et al. 2020).\\n'\n",
      "                       'We report the perplexity of our models and baselines '\n",
      "                       'as the function of the sequence length in Figure 7. '\n",
      "                       'Interestingly, with\\n'\n",
      "                       'the increase of memory depth, 𝐿M, the model can '\n",
      "                       'achieve better perplexity over all sequence length. '\n",
      "                       'Also, deeper memory\\n'\n",
      "                       'modules are more robust to the sequence length when '\n",
      "                       'the model has less number of parameters. With the '\n",
      "                       'increase of the\\n'\n",
      "                       'number of parameters, all models show better '\n",
      "                       'performance on longer sequences.\\n'\n",
      "                       'Figure 8: The effect of memory depth on\\n'\n",
      "                       'training throughput\\n'\n",
      "                       'We also evaluate the effect of memory depth ( 𝐿M = '\n",
      "                       '1,2,3,4) on the training\\n'\n",
      "                       'throughput. We report the training throughput (the '\n",
      "                       'number of tokens per\\n'\n",
      "                       'second) as the function of sequence length in Figure '\n",
      "                       '8. All models scale linearly\\n'\n",
      "                       'with respect to the context length (i.e., constant '\n",
      "                       'trend in the number of tokens\\n'\n",
      "                       'per second with respect to sequence length). Also, by '\n",
      "                       'increasing the memory\\n'\n",
      "                       'depth, as expected, we can see a linear trend that a '\n",
      "                       'deeper memory results in\\n'\n",
      "                       'a slower training. Therefore, it is not always '\n",
      "                       'efficient to use deeper memory\\n'\n",
      "                       'modules, showing a trade-off between effectiveness and '\n",
      "                       'efficiency.\\n'\n",
      "                       '5.6 Time Series Forecasting\\n'\n",
      "                       'To show the effectiveness of our memory module in a '\n",
      "                       'broader tasks, we also evaluate its performance in '\n",
      "                       'time series\\n'\n",
      "                       'forecasting tasks. To this end, we use Simba framework '\n",
      "                       '(Patro and Agneeswaran 2024) for time series '\n",
      "                       'forecasting, and\\n'\n",
      "                       '2Note that, in this experiment, we only focus on the '\n",
      "                       'neural memory module to evaluate the effect of memory '\n",
      "                       'depth in the memorization process.\\n'\n",
      "                       'Combining neural memory with attention as we do in '\n",
      "                       'Titans variants, can additionally enhance the '\n",
      "                       'performance of the model over long sequences.\\n'\n",
      "                       '15\\n'\n",
      "                       'Table 4: Downstream evaluation of pre-trained DNA '\n",
      "                       'models on GenomicsBenchmarks (Grešová et al. 2023). We '\n",
      "                       'report\\n'\n",
      "                       'top-1 classification accuracy (%).\\n'\n",
      "                       'Model Enhancer Cohn Enhancer Ens Human Reg. Non-TATA '\n",
      "                       'Promoters Human OCR Ens.\\n'\n",
      "                       'CNN 69.5 68.9 93.3 84.6 68.0\\n'\n",
      "                       'DNABERT 74.0 85.7 88.1 85.6 75.1\\n'\n",
      "                       'GPT 70.5 83.5 91.5 87.7 73.0\\n'\n",
      "                       'HyenaDNA 74.2 89.2 93.8 96.6 80.9\\n'\n",
      "                       'Transformer++ 73.4 89.5 89.9 94.4 79.5\\n'\n",
      "                       'Mamba 73.0 - - 96.6 -\\n'\n",
      "                       'Based 74.6 89.5 89.5 96.8 79.0\\n'\n",
      "                       'Neural Memory Module 75.2 89.6 89.3 96.6 79.9\\n'\n",
      "                       'replace its Mamba module with our neural memory. We '\n",
      "                       'report the results on common time series forecasting '\n",
      "                       'benchmark\\n'\n",
      "                       'datasets–ETT, ECL, Traffic, and Weather (H. Zhou et '\n",
      "                       'al. 2021). The results are reported in Table 3. Our '\n",
      "                       'neural memory\\n'\n",
      "                       'module is outperforming all baselines, including '\n",
      "                       'Mamba-based, linear-based, and Transformer-based '\n",
      "                       'architectures.\\n'\n",
      "                       '5.7 DNA Modeling\\n'\n",
      "                       'In order to understand the capability of Titans beyond '\n",
      "                       'natural language, we further evaluate the performance '\n",
      "                       'of our\\n'\n",
      "                       'neural memory module on DNA modeling tasks. To this '\n",
      "                       'end, we evaluate pre-trained models on the downstream '\n",
      "                       'tasks\\n'\n",
      "                       'in GenomicsBenchmarks (Grešová et al. 2023). We follow '\n",
      "                       'the same experimental setups from Nguyen et al. '\n",
      "                       '(2024), and\\n'\n",
      "                       're-use the reported results of baselines by Arora et '\n",
      "                       'al. (2024). The performance of Titans (LMM) and '\n",
      "                       'baselines are reported\\n'\n",
      "                       'in Table 4. We find that LMM is competitive with '\n",
      "                       'state-of-the-art architectures across different '\n",
      "                       'downstream genomics\\n'\n",
      "                       'tasks.\\n'\n",
      "                       '5.8 Efficiency\\n'\n",
      "                       'Figure 9: Training throughput compari-\\n'\n",
      "                       'son of Titans and baselines.\\n'\n",
      "                       'In this part, we compare the efficiency of our neural '\n",
      "                       'memory as well as Titans\\n'\n",
      "                       'with state-of-the-art sequence models. The training '\n",
      "                       'throughput of models for\\n'\n",
      "                       'different sequence length ×batch size are reported in '\n",
      "                       'Figure 9. Comparing\\n'\n",
      "                       'recurrent models, including our neural memory module, '\n",
      "                       'we can see our memory\\n'\n",
      "                       'module is slightly slower than Mamba2 and Gated '\n",
      "                       'DeltaNet, mainly due to: (1)\\n'\n",
      "                       'having deep memory and more expressive transition '\n",
      "                       'process (memory update),\\n'\n",
      "                       'and (2) highly optimized kernel in the implementation '\n",
      "                       'of Mamba2. Interestingly,\\n'\n",
      "                       'Titans (MAL) are faster than baselines as well as the '\n",
      "                       'memory module. The\\n'\n",
      "                       'main reason for this better throughput is the highly '\n",
      "                       'optimized kernel of Flash-\\n'\n",
      "                       'Attention (Dao 2024), which is used for implementing '\n",
      "                       'SWA and full attention\\n'\n",
      "                       'module in Titans.\\n'\n",
      "                       '5.9 Ablation Study\\n'\n",
      "                       'Finally, we perform ablation studies on the different '\n",
      "                       'architectural choices in Titans. We consider our '\n",
      "                       'neural memory\\n'\n",
      "                       'module as a base model and then changing one component '\n",
      "                       'at a time: (1) replacing deep memory with linear '\n",
      "                       'memory,\\n'\n",
      "                       'removing (2) convolution, (3) momentum in the surprise '\n",
      "                       'measure, (4) weight decay (or forgot mechanism), and '\n",
      "                       '(5) persistent\\n'\n",
      "                       'memory. The results are reported in Table 5. All '\n",
      "                       'components of neural memory design are positively '\n",
      "                       'contributing to its\\n'\n",
      "                       'performance, where the greatest contribution comes '\n",
      "                       'from weight decay, momentum, convolution, and '\n",
      "                       'persistent memory,\\n'\n",
      "                       'respectively.\\n'\n",
      "                       'The Effect of Architectural Design. To evaluate the '\n",
      "                       'effect of architecture design, we compare the '\n",
      "                       'performance of three\\n'\n",
      "                       'represented variants of Titans in three aspects of (i) '\n",
      "                       'language modeling, (ii) commen-sense reasoning, and '\n",
      "                       '(iii) long context\\n'\n",
      "                       'NIAH (BABILong) tasks. The results are reported in '\n",
      "                       'Table 5. We find that MAC and MAG have close '\n",
      "                       'performance in\\n'\n",
      "                       'language modeling and common-sense reasoning tasks, '\n",
      "                       'while MAC achieve significantly better performance in '\n",
      "                       'long-context\\n'\n",
      "                       'NIAH. Both of these models achieve better performance '\n",
      "                       'than MAL. These results along with Figure 9, show a '\n",
      "                       'trade-off\\n'\n",
      "                       'between fast training and more expressive design.\\n'\n",
      "                       '16\\n'\n",
      "                       'Table 5: Ablation Study on Titans. All components of '\n",
      "                       'Titans are positively contributing to its '\n",
      "                       'performance.\\n'\n",
      "                       'Model Language Modeling Reasoning Long Context\\n'\n",
      "                       'ppl↓ acc↑ acc↑\\n'\n",
      "                       'LMM 27.01 47.83 92.68\\n'\n",
      "                       '+Attn(MAC) 26.67 48.65 97.95\\n'\n",
      "                       '+Attn(MAG) 25.70 48.60 96.70\\n'\n",
      "                       '+Attn(MAL) 25.91 47.87 96.91\\n'\n",
      "                       'Linear Memory 28.49 46.97 85.34\\n'\n",
      "                       'w/o Convolution 28.73 45.82 90.28\\n'\n",
      "                       'w/o Momentum 28.98 45.49 87.12\\n'\n",
      "                       'w/o Weight Decay 29.04 45.11 85.60\\n'\n",
      "                       'w/o Persistent Memory 27.63 46.35 92.49\\n'\n",
      "                       '6 Conclusion\\n'\n",
      "                       'In this paper, we present a neural long-term memory '\n",
      "                       'that, as a meta in-context learner, learns to memorize '\n",
      "                       'at test time.\\n'\n",
      "                       'The neural memory module is a recurrent model in '\n",
      "                       'nature, and is adaptively memorizing tokens that are '\n",
      "                       'more surprising\\n'\n",
      "                       'or are close to surprising tokens. Comparing to modern '\n",
      "                       'recurrent models, it has more expressive memory update '\n",
      "                       'and\\n'\n",
      "                       'storing mechanism. Using this memory, we present '\n",
      "                       'Titans architectures, and its three variants, in which '\n",
      "                       'we suggest to\\n'\n",
      "                       'incorporate the memory module as (1) a context, (2) '\n",
      "                       'gating, and (3) a layer. Our experimental evaluation '\n",
      "                       'on diverse tasks\\n'\n",
      "                       'tasks validate that Titans are more effective than '\n",
      "                       'Transformers and recent modern linear recurrent '\n",
      "                       'models, specifically for\\n'\n",
      "                       'long context. That is, Titans can scale to larger than '\n",
      "                       '2M context window size with better accuracy than '\n",
      "                       'baselines.\\n'\n",
      "                       'Titans are implemented in Pytorch and JAX and we '\n",
      "                       'intend to make the code we used to train and evaluate '\n",
      "                       'our models\\n'\n",
      "                       'available soon.\\n'\n",
      "                       '17\\n'\n",
      "                       'References\\n'\n",
      "                       '[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama '\n",
      "                       'Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo\\n'\n",
      "                       'Almeida, Janko Altenschmidt, Sam Altman, Shyamal '\n",
      "                       'Anadkat, et al. “Gpt-4 technical report”. In: arXiv '\n",
      "                       'preprint\\n'\n",
      "                       'arXiv:2303.08774 (2023).\\n'\n",
      "                       '[2] Yaroslav Aksenov, Nikita Balagansky, Sofia Maria '\n",
      "                       'Lo Cicero Vaina, Boris Shaposhnikov, Alexey '\n",
      "                       'Gorbatovski, and\\n'\n",
      "                       'Daniil Gavrilov. “Linear Transformers with Learnable '\n",
      "                       'Kernel Functions are Better In-Context Models”. In: '\n",
      "                       'arXiv\\n'\n",
      "                       'preprint arXiv:2402.10644 (2024).\\n'\n",
      "                       '[3] Marcin Andrychowicz, Misha Denil, Sergio Gomez, '\n",
      "                       'Matthew W Hoffman, David Pfau, Tom Schaul, Brendan\\n'\n",
      "                       'Shillingford, and Nando De Freitas. “Learning to learn '\n",
      "                       'by gradient descent by gradient descent”. In: Advances '\n",
      "                       'in\\n'\n",
      "                       'neural information processing systems 29 (2016).\\n'\n",
      "                       '[4] Cem Anil, Yuhuai Wu, Anders Andreassen, Aitor '\n",
      "                       'Lewkowycz, Vedant Misra, Vinay Ramasesh, Ambrose '\n",
      "                       'Slone,\\n'\n",
      "                       'Guy Gur-Ari, Ethan Dyer, and Behnam Neyshabur. '\n",
      "                       '“Exploring length generalization in large language '\n",
      "                       'models”. In:\\n'\n",
      "                       'Advances in Neural Information Processing Systems 35 '\n",
      "                       '(2022), pp. 38546–38556.\\n'\n",
      "                       '[5] Simran Arora, Sabri Eyuboglu, Michael Zhang, Aman '\n",
      "                       'Timalsina, Silas Alberti, James Zou, Atri Rudra, and '\n",
      "                       'Christo-\\n'\n",
      "                       'pher Re. “Simple linear attention language models '\n",
      "                       'balance the recall-throughput tradeoff”. '\n",
      "                       'In:Forty-first International\\n'\n",
      "                       'Conference on Machine Learning . 2024. url: '\n",
      "                       'https://openreview.net/forum?id=e93ffDcpH3.\\n'\n",
      "                       '[6] Dzmitry Bahdanau. “Neural machine translation by '\n",
      "                       'jointly learning to align and translate”. In: arXiv '\n",
      "                       'preprint\\n'\n",
      "                       'arXiv:1409.0473 (2014).\\n'\n",
      "                       '[7] Reza Bayat, Mohammad Pezeshki, Elvis Dohmatob, '\n",
      "                       'David Lopez-Paz, and Pascal Vincent. “The Pitfalls of '\n",
      "                       'Memo-\\n'\n",
      "                       'rization: When Memorization Hurts Generalization”. In: '\n",
      "                       'arXiv preprint arXiv:2412.07684 (2024).\\n'\n",
      "                       '[8] Maximilian Beck, Korbinian Pöppel, Markus '\n",
      "                       'Spanring, Andreas Auer, Oleksandra Prudnikova, Michael '\n",
      "                       'Kopp,\\n'\n",
      "                       'Günter Klambauer, Johannes Brandstetter, and Sepp '\n",
      "                       'Hochreiter. “xLSTM: Extended Long Short-Term Memory”. '\n",
      "                       'In:\\n'\n",
      "                       'arXiv preprint arXiv:2405.04517 (2024).\\n'\n",
      "                       '[9] Ali Behrouz, Michele Santacatterina, and Ramin '\n",
      "                       'Zabih. “Mambamixer: Efficient selective state space '\n",
      "                       'models with\\n'\n",
      "                       'dual token and channel selection”. In: arXiv preprint '\n",
      "                       'arXiv:2403.19888 (2024).\\n'\n",
      "                       '[10] Vincent-Pierre Berges, Barlas Oğuz, Daniel '\n",
      "                       'Haziza, Wen-tau Yih, Luke Zettlemoyer, and Gargi Gosh. '\n",
      "                       '“Memory\\n'\n",
      "                       'Layers at Scale”. In: arXiv preprint arXiv:2412.09764 '\n",
      "                       '(2024).\\n'\n",
      "                       '[11] Alberto Bietti, Vivien Cabannes, Diane '\n",
      "                       'Bouchacourt, Herve Jegou, and Leon Bottou. “Birth of a '\n",
      "                       'transformer: A\\n'\n",
      "                       'memory viewpoint”. In: Advances in Neural Information '\n",
      "                       'Processing Systems 36 (2024).\\n'\n",
      "                       '[12] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin '\n",
      "                       'Choi, et al. “Piqa: Reasoning about physical '\n",
      "                       'commonsense in\\n'\n",
      "                       'natural language”. In: Proceedings of the AAAI '\n",
      "                       'conference on artificial intelligence . Vol. 34. 05. '\n",
      "                       '2020, pp. 7432–7439.\\n'\n",
      "                       '[13] Aleksandar Botev, Soham De, Samuel L Smith, '\n",
      "                       'Anushan Fernando, George-Cristian Muraru, Ruba Haroun, '\n",
      "                       'Leonard\\n'\n",
      "                       'Berrada, Razvan Pascanu, Pier Giuseppe Sessa, Robert '\n",
      "                       'Dadashi, et al. “RecurrentGemma: Moving Past '\n",
      "                       'Transformers\\n'\n",
      "                       'for Efficient Open Language Models”. In: arXiv '\n",
      "                       'preprint arXiv:2404.07839 (2024).\\n'\n",
      "                       '[14] Léon Bottou and Vladimir Vapnik. “Local learning '\n",
      "                       'algorithms”. In: Neural computation 4.6 (1992), pp. '\n",
      "                       '888–900.\\n'\n",
      "                       '[15] Aydar Bulatov, Yuri Kuratov, Yermek Kapushev, and '\n",
      "                       'Mikhail S Burtsev. “Scaling transformer to 1m tokens '\n",
      "                       'and\\n'\n",
      "                       'beyond with rmt”. In: arXiv preprint arXiv:2304.11062 '\n",
      "                       '(2023).\\n'\n",
      "                       '[16] Aydar Bulatov, Yury Kuratov, and Mikhail Burtsev. '\n",
      "                       '“Recurrent memory transformer”. In: Advances in '\n",
      "                       'Neural\\n'\n",
      "                       'Information Processing Systems 35 (2022), pp. '\n",
      "                       '11079–11091.\\n'\n",
      "                       '[17] Edoardo Cetin, Qi Sun, Tianyu Zhao, and Yujin '\n",
      "                       'Tang. “An Evolved Universal Transformer Memory”. In: '\n",
      "                       'arXiv\\n'\n",
      "                       'preprint arXiv:2410.13166 (2024).\\n'\n",
      "                       '[18] Beidi Chen, Tri Dao, Eric Winsor, Zhao Song, Atri '\n",
      "                       'Rudra, and Christopher Ré. “Scatterbrain: Unifying '\n",
      "                       'sparse and\\n'\n",
      "                       'low-rank attention”. In: Advances in Neural '\n",
      "                       'Information Processing Systems 34 (2021), pp. '\n",
      "                       '17413–17426.\\n'\n",
      "                       '[19] Krzysztof Marcin Choromanski, Valerii '\n",
      "                       'Likhosherstov, David Dohan, Xingyou Song, Andreea '\n",
      "                       'Gane, Tamas Sarlos,\\n'\n",
      "                       'Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, '\n",
      "                       'Lukasz Kaiser, David Benjamin Belanger, Lucy J '\n",
      "                       'Colwell, and\\n'\n",
      "                       'Adrian Weller. “Rethinking Attention with Performers”. '\n",
      "                       'In: International Conference on Learning '\n",
      "                       'Representations .\\n'\n",
      "                       '2021. url: '\n",
      "                       'https://openreview.net/forum?id=Ua6zuk0WRH.\\n'\n",
      "                       '[20] Christopher Clark, Kenton Lee, Ming-Wei Chang, '\n",
      "                       'Tom Kwiatkowski, Michael Collins, and Kristina '\n",
      "                       'Toutanova.\\n'\n",
      "                       '“BoolQ: Exploring the Surprising Difficulty of Natural '\n",
      "                       'Yes/No Questions”. In: Proceedings of the 2019 '\n",
      "                       'Conference\\n'\n",
      "                       'of the North American Chapter of the Association for '\n",
      "                       'Computational Linguistics: Human Language '\n",
      "                       'Technologies,\\n'\n",
      "                       'Volume 1 (Long and Short Papers) . Ed. by Jill '\n",
      "                       'Burstein, Christy Doran, and Thamar Solorio. '\n",
      "                       'Minneapolis, Minnesota:\\n'\n",
      "                       'Association for Computational Linguistics, June 2019, '\n",
      "                       'pp. 2924–2936. doi: 10.18653/v1/N19-1300. url: https:\\n'\n",
      "                       '//aclanthology.org/N19-1300/.\\n'\n",
      "                       '18\\n'\n",
      "                       '[21] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar '\n",
      "                       'Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind '\n",
      "                       'Tafjord.\\n'\n",
      "                       '“Think you have solved question answering? try arc, '\n",
      "                       'the ai2 reasoning challenge”. In:arXiv preprint '\n",
      "                       'arXiv:1803.05457\\n'\n",
      "                       '(2018).\\n'\n",
      "                       '[22] Nelson Cowan. “What are the differences between '\n",
      "                       'long-term, short-term, and working memory?” In: '\n",
      "                       'Progress in\\n'\n",
      "                       'brain research 169 (2008), pp. 323–338.\\n'\n",
      "                       '[23] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G. '\n",
      "                       'Carbonell, Quoc Viet Le, and Ruslan Salakhutdinov. '\n",
      "                       '“Transformer-\\n'\n",
      "                       'XL: Attentive Language Models beyond a Fixed-Length '\n",
      "                       'Context”. In: ACL (1). Ed. by Anna Korhonen, David R.\\n'\n",
      "                       'Traum, and Lluís Màrquez. Association for '\n",
      "                       'Computational Linguistics, 2019, pp. 2978–2988.isbn: '\n",
      "                       '978-1-950737-48-2.\\n'\n",
      "                       '[24] Tri Dao. “FlashAttention-2: Faster Attention with '\n",
      "                       'Better Parallelism and Work Partitioning”. In: The '\n",
      "                       'Twelfth Inter-\\n'\n",
      "                       'national Conference on Learning Representations . '\n",
      "                       '2024. url: '\n",
      "                       'https://openreview.net/forum?id=mZn2Xyh9Ec.\\n'\n",
      "                       '[25] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and '\n",
      "                       'Christopher Ré. “FlashAttention: Fast and '\n",
      "                       'Memory-Efficient\\n'\n",
      "                       'Exact Attention with IO-Awareness”. In:Advances in '\n",
      "                       'Neural Information Processing Systems . Ed. by S. '\n",
      "                       'Koyejo, S.\\n'\n",
      "                       'Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh. '\n",
      "                       'Vol. 35. Curran Associates, Inc., 2022, pp. '\n",
      "                       '16344–16359. url:\\n'\n",
      "                       'https://proceedings.neurips.cc/paper_files/paper/2022/file/67d57c32e20fd0a7a302cb81d36e40d5-\\n'\n",
      "                       'Paper-Conference.pdf.\\n'\n",
      "                       '[26] Tri Dao and Albert Gu. “Transformers are SSMs: '\n",
      "                       'Generalized models and efficient algorithms through '\n",
      "                       'structured\\n'\n",
      "                       'state space duality”. In: arXiv preprint '\n",
      "                       'arXiv:2405.21060 (2024).\\n'\n",
      "                       '[27] Abhimanyu Das, Weihao Kong, Andrew Leach, Shaan K '\n",
      "                       'Mathur, Rajat Sen, and Rose Yu. “Long-term '\n",
      "                       'Forecasting\\n'\n",
      "                       'with TiDE: Time-series Dense Encoder”. In: '\n",
      "                       'Transactions on Machine Learning Research (2023). '\n",
      "                       'issn: 2835-8856. url:\\n'\n",
      "                       'https://openreview.net/forum?id=pCbC3aQB5W.\\n'\n",
      "                       '[28] Soham De, Samuel L Smith, Anushan Fernando, '\n",
      "                       'Aleksandar Botev, George Cristian-Muraru, Albert Gu, '\n",
      "                       'Ruba\\n'\n",
      "                       'Haroun, Leonard Berrada, Yutian Chen, Srivatsan '\n",
      "                       'Srinivasan, et al. “Griffin: Mixing gated linear '\n",
      "                       'recurrences with\\n'\n",
      "                       'local attention for efficient language models”. In: '\n",
      "                       'arXiv preprint arXiv:2402.19427 (2024).\\n'\n",
      "                       '[29] Juechu Dong, Boyuan Feng, Driss Guessous, Yanbo '\n",
      "                       'Liang, and Horace He. “Flex Attention: A Programming '\n",
      "                       'Model\\n'\n",
      "                       'for Generating Optimized Attention Kernels”. In: arXiv '\n",
      "                       'preprint arXiv:2412.05496 (2024).\\n'\n",
      "                       '[30] Xin Dong, Yonggan Fu, Shizhe Diao, Wonmin Byeon, '\n",
      "                       'Zijia Chen, Ameya Sunil Mahabaleshwarkar, Shih-Yang '\n",
      "                       'Liu,\\n'\n",
      "                       'Matthijs Van Keirsbilck, Min-Hung Chen, Yoshi Suhara, '\n",
      "                       'et al. “Hymba: A Hybrid-head Architecture for Small\\n'\n",
      "                       'Language Models”. In: arXiv preprint arXiv:2411.13676 '\n",
      "                       '(2024).\\n'\n",
      "                       '[31] Stefan Elfwing, Eiji Uchibe, and Kenji Doya. '\n",
      "                       '“Sigmoid-weighted linear units for neural network '\n",
      "                       'function approxi-\\n'\n",
      "                       'mation in reinforcement learning”. In: Neural networks '\n",
      "                       '107 (2018), pp. 3–11.\\n'\n",
      "                       '[32] Yukun Feng, Feng Li, Ziang Song, Boyuan Zheng, '\n",
      "                       'and Philipp Koehn. “Learn to remember: Transformer '\n",
      "                       'with\\n'\n",
      "                       'recurrent memory for document-level machine '\n",
      "                       'translation”. In: arXiv preprint arXiv:2205.01546 '\n",
      "                       '(2022).\\n'\n",
      "                       '[33] Daniel Y Fu, Tri Dao, Khaled Kamal Saab, Armin W '\n",
      "                       'Thomas, Atri Rudra, and Christopher Re. “Hungry '\n",
      "                       'Hungry\\n'\n",
      "                       'Hippos: Towards Language Modeling with State Space '\n",
      "                       'Models”. In:The Eleventh International Conference on '\n",
      "                       'Learning\\n'\n",
      "                       'Representations. 2023. url: '\n",
      "                       'https://openreview.net/forum?id=COZDy0WYGg.\\n'\n",
      "                       '[34] Yossi Gandelsman, Yu Sun, Xinlei Chen, and Alexei '\n",
      "                       'Efros. “Test-time training with masked autoencoders”. '\n",
      "                       'In:\\n'\n",
      "                       'Advances in Neural Information Processing Systems 35 '\n",
      "                       '(2022), pp. 29374–29385.\\n'\n",
      "                       '[35] Leo Gao, Stella Biderman, Sid Black, Laurence '\n",
      "                       'Golding, Travis Hoppe, Charles Foster, Jason Phang, '\n",
      "                       'Horace He,\\n'\n",
      "                       'Anish Thite, Noa Nabeshima, et al. “The pile: An 800gb '\n",
      "                       'dataset of diverse text for language modeling”. In: '\n",
      "                       'arXiv\\n'\n",
      "                       'preprint arXiv:2101.00027 (2020).\\n'\n",
      "                       '[36] Felix A Gers, Jürgen Schmidhuber, and Fred '\n",
      "                       'Cummins. “Learning to forget: Continual prediction '\n",
      "                       'with LSTM”. In:\\n'\n",
      "                       'Neural computation 12.10 (2000), pp. 2451–2471.\\n'\n",
      "                       '[37] Alex Graves, Greg Wayne, and Ivo Danihelka. '\n",
      "                       'Neural Turing Machines . 2014. arXiv: 1410.5401 '\n",
      "                       '[cs.NE]. url:\\n'\n",
      "                       'https://arxiv.org/abs/1410.5401.\\n'\n",
      "                       '[38] Klaus Greff, Rupesh K Srivastava, Jan Koutník, '\n",
      "                       'Bas R Steunebrink, and Jürgen Schmidhuber. “LSTM: A '\n",
      "                       'search space\\n'\n",
      "                       'odyssey”. In: IEEE transactions on neural networks and '\n",
      "                       'learning systems 28.10 (2016), pp. 2222–2232.\\n'\n",
      "                       '[39] Katarína Grešová, Vlastimil Martinek, David '\n",
      "                       'Čechák, Petr Šimeček, and Panagiotis Alexiou. “Genomic '\n",
      "                       'benchmarks:\\n'\n",
      "                       'a collection of datasets for genomic sequence '\n",
      "                       'classification”. In: BMC Genomic Data 24.1 (2023), p. '\n",
      "                       '25.\\n'\n",
      "                       '[40] Albert Gu and Tri Dao. “Mamba: Linear-Time '\n",
      "                       'Sequence Modeling with Selective State Spaces”. In: '\n",
      "                       'First Conference\\n'\n",
      "                       'on Language Modeling . 2024. url: '\n",
      "                       'https://openreview.net/forum?id=tEYskw1VY2.\\n'\n",
      "                       '[41] Albert Gu, Karan Goel, and Christopher Re. '\n",
      "                       '“Efficiently Modeling Long Sequences with Structured '\n",
      "                       'State Spaces”.\\n'\n",
      "                       'In: International Conference on Learning '\n",
      "                       'Representations . 2022. url: https : / / openreview . '\n",
      "                       'net / forum ? id =\\n'\n",
      "                       'uYLFoz1vlAC.\\n'\n",
      "                       '19\\n'\n",
      "                       '[42] Chi Han, Qifan Wang, Hao Peng, Wenhan Xiong, Yu '\n",
      "                       'Chen, Heng Ji, and Sinong Wang. “LM-Infinite: '\n",
      "                       'Zero-Shot\\n'\n",
      "                       'Extreme Length Generalization for Large Language '\n",
      "                       'Models”. In: Proceedings of the 2024 Conference of the '\n",
      "                       'North\\n'\n",
      "                       'American Chapter of the Association for Computational '\n",
      "                       'Linguistics: Human Language Technologies (Volume 1: '\n",
      "                       'Long\\n'\n",
      "                       'Papers). Ed. by Kevin Duh, Helena Gomez, and Steven '\n",
      "                       'Bethard. Mexico City, Mexico: Association for '\n",
      "                       'Computational\\n'\n",
      "                       'Linguistics, June 2024, pp. 3991–4008. doi: '\n",
      "                       '10.18653/v1/2024.naacl-long.222. url: '\n",
      "                       'https://aclanthology.\\n'\n",
      "                       'org/2024.naacl-long.222.\\n'\n",
      "                       '[43] Ramin Hasani, Mathias Lechner, Tsun-Hsuan Wang, '\n",
      "                       'Makram Chahine, Alexander Amini, and Daniela Rus. '\n",
      "                       '“Liquid\\n'\n",
      "                       'Structural State-Space Models”. In: The Eleventh '\n",
      "                       'International Conference on Learning Representations . '\n",
      "                       '2023. url:\\n'\n",
      "                       'https://openreview.net/forum?id=g4OTKRKfS7R.\\n'\n",
      "                       '[44] Zexue He, Leonid Karlinsky, Donghyun Kim, Julian '\n",
      "                       'McAuley, Dmitry Krotov, and Rogerio Feris. “CAMELoT:\\n'\n",
      "                       'Towards Large Language Models with Training-Free '\n",
      "                       'Consolidated Associative Memory”. In: arXiv preprint\\n'\n",
      "                       'arXiv:2402.13449 (2024).\\n'\n",
      "                       '[45] Donald Olding Hebb. The organization of behavior: '\n",
      "                       'A neuropsychological theory . Psychology press, 2005.\\n'\n",
      "                       '[46] John J Hopfield. “Neural networks and physical '\n",
      "                       'systems with emergent collective computational '\n",
      "                       'abilities.” In:\\n'\n",
      "                       'Proceedings of the national academy of sciences 79.8 '\n",
      "                       '(1982), pp. 2554–2558.\\n'\n",
      "                       '[47] Kurt Hornik, Maxwell Stinchcombe, and Halbert '\n",
      "                       'White. “Multilayer feedforward networks are universal '\n",
      "                       'approxi-\\n'\n",
      "                       'mators”. In: Neural networks 2.5 (1989), pp. 359–366.\\n'\n",
      "                       '[48] Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, '\n",
      "                       'Shantanu Acharya, Dima Rekesh, Fei Jia, and Boris '\n",
      "                       'Ginsburg.\\n'\n",
      "                       '“RULER: What’s the Real Context Size of Your '\n",
      "                       'Long-Context Language Models?” In: First Conference on '\n",
      "                       'Language\\n'\n",
      "                       'Modeling. 2024. url: '\n",
      "                       'https://openreview.net/forum?id=kIoBbc76Sy.\\n'\n",
      "                       '[49] DeLesley Hutchins, Imanol Schlag, Yuhuai Wu, '\n",
      "                       'Ethan Dyer, and Behnam Neyshabur. “Block-recurrent '\n",
      "                       'transformers”.\\n'\n",
      "                       'In: Advances in neural information processing systems '\n",
      "                       '35 (2022), pp. 33248–33261.\\n'\n",
      "                       '[50] Kazuki Irie, Róbert Csordás, and Jürgen '\n",
      "                       'Schmidhuber. “The dual form of neural networks '\n",
      "                       'revisited: Connecting test\\n'\n",
      "                       'time predictions to training patterns via spotlights '\n",
      "                       'of attention”. In: International Conference on Machine '\n",
      "                       'Learning .\\n'\n",
      "                       'PMLR. 2022, pp. 9639–9659.\\n'\n",
      "                       '[51] Kazuki Irie, Imanol Schlag, Róbert Csordás, and '\n",
      "                       'Jürgen Schmidhuber. “Going beyond linear transformers '\n",
      "                       'with\\n'\n",
      "                       'recurrent fast weight programmers”. In: Advances in '\n",
      "                       'neural information processing systems 34 (2021), pp. '\n",
      "                       '7703–7717.\\n'\n",
      "                       '[52] Vidit Jain and Erik Learned-Miller. “Online '\n",
      "                       'domain adaptation of a pre-trained cascade of '\n",
      "                       'classifiers”. In: CVPR\\n'\n",
      "                       '2011. IEEE. 2011, pp. 577–584.\\n'\n",
      "                       '[53] Albert Q Jiang, Alexandre Sablayrolles, Arthur '\n",
      "                       'Mensch, Chris Bamford, Devendra Singh Chaplot, Diego '\n",
      "                       'de las\\n'\n",
      "                       'Casas, Florian Bressand, Gianna Lengyel, Guillaume '\n",
      "                       'Lample, Lucile Saulnier, et al. “Mistral 7B”. In: '\n",
      "                       'arXiv preprint\\n'\n",
      "                       'arXiv:2310.06825 (2023).\\n'\n",
      "                       '[54] Praneeth Kacham, Vahab Mirrokni, and Peilin '\n",
      "                       'Zhong. “PolySketchFormer: Fast Transformers via '\n",
      "                       'Sketching Polyno-\\n'\n",
      "                       'mial Kernels”. In: Forty-first International '\n",
      "                       'Conference on Machine Learning . 2024. url: '\n",
      "                       'https://openreview.net/\\n'\n",
      "                       'forum?id=ghYrfdJfjK.\\n'\n",
      "                       '[55] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B '\n",
      "                       'Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec\\n'\n",
      "                       'Radford, Jeffrey Wu, and Dario Amodei. “Scaling laws '\n",
      "                       'for neural language models”. In:arXiv preprint '\n",
      "                       'arXiv:2001.08361\\n'\n",
      "                       '(2020).\\n'\n",
      "                       '[56] Angelos Katharopoulos, Apoorv Vyas, Nikolaos '\n",
      "                       'Pappas, and François Fleuret. “Transformers are rnns: '\n",
      "                       'Fast au-\\n'\n",
      "                       'toregressive transformers with linear attention”. In: '\n",
      "                       'International conference on machine learning . PMLR. '\n",
      "                       '2020,\\n'\n",
      "                       'pp. 5156–5165.\\n'\n",
      "                       '[57] Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke '\n",
      "                       'Zettlemoyer, and Mike Lewis. “Generalization through\\n'\n",
      "                       'Memorization: Nearest Neighbor Language Models”. In: '\n",
      "                       'International Conference on Learning Representations . '\n",
      "                       '2020.\\n'\n",
      "                       'url: https://openreview.net/forum?id=HklBjCEKvH.\\n'\n",
      "                       '[58] Yuri Kuratov, Aydar Bulatov, Petr Anokhin, Ivan '\n",
      "                       'Rodkin, Dmitry Igorevich Sorokin, Artyom Sorokin, and '\n",
      "                       'Mikhail\\n'\n",
      "                       'Burtsev. “BABILong: Testing the Limits of LLMs with '\n",
      "                       'Long Context Reasoning-in-a-Haystack”. In: The '\n",
      "                       'Thirty-\\n'\n",
      "                       'eight Conference on Neural Information Processing '\n",
      "                       'Systems Datasets and Benchmarks Track . 2024. url: '\n",
      "                       'https:\\n'\n",
      "                       '//openreview.net/forum?id=u7m2CG84BQ.\\n'\n",
      "                       '[59] Hung Le, Truyen Tran, and Svetha Venkatesh. '\n",
      "                       '“Self-attentive associative memory”. In:International '\n",
      "                       'conference on\\n'\n",
      "                       'machine learning . PMLR. 2020, pp. 5682–5691.\\n'\n",
      "                       '[60] Patrick Lewis, Ethan Perez, Aleksandra Piktus, '\n",
      "                       'Fabio Petroni, Vladimir Karpukhin, Naman Goyal, '\n",
      "                       'Heinrich Küttler,\\n'\n",
      "                       'Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. '\n",
      "                       '“Retrieval-augmented generation for '\n",
      "                       'knowledge-intensive nlp\\n'\n",
      "                       'tasks”. In: Advances in Neural Information Processing '\n",
      "                       'Systems 33 (2020), pp. 9459–9474.\\n'\n",
      "                       '20\\n'\n",
      "                       '[61] Danny Leybzon and Corentin Kervadec. “Learning, '\n",
      "                       'Forgetting, Remembering: Insights From Tracking LLM '\n",
      "                       'Mem-\\n'\n",
      "                       'orization During Training”. In: Proceedings of the 7th '\n",
      "                       'BlackboxNLP Workshop: Analyzing and Interpreting '\n",
      "                       'Neural\\n'\n",
      "                       'Networks for NLP . 2024, pp. 43–57.\\n'\n",
      "                       '[62] Zhe Li, Shiyi Qi, Yiduo Li, and Zenglin Xu. '\n",
      "                       '“Revisiting long-term time series forecasting: An '\n",
      "                       'investigation on linear\\n'\n",
      "                       'mapping”. In: arXiv preprint arXiv:2305.10721 (2023).\\n'\n",
      "                       '[63] Bo Liu, Rui Wang, Lemeng Wu, Yihao Feng, Peter '\n",
      "                       'Stone, and Qiang Liu. “Longhorn: State space models '\n",
      "                       'are amortized\\n'\n",
      "                       'online learners”. In: arXiv preprint arXiv:2407.14207 '\n",
      "                       '(2024).\\n'\n",
      "                       '[64] Nelson F Liu, Kevin Lin, John Hewitt, Ashwin '\n",
      "                       'Paranjape, Michele Bevilacqua, Fabio Petroni, and '\n",
      "                       'Percy Liang.\\n'\n",
      "                       '“Lost in the middle: How language models use long '\n",
      "                       'contexts”. In: Transactions of the Association for '\n",
      "                       'Computational\\n'\n",
      "                       'Linguistics 12 (2024), pp. 157–173.\\n'\n",
      "                       '[65] Yong Liu, Tengge Hu, Haoran Zhang, Haixu Wu, '\n",
      "                       'Shiyu Wang, Lintao Ma, and Mingsheng Long. '\n",
      "                       '“itransformer:\\n'\n",
      "                       'Inverted transformers are effective for time series '\n",
      "                       'forecasting”. In: arXiv preprint arXiv:2310.06625 '\n",
      "                       '(2023).\\n'\n",
      "                       '[66] George Mandler. “The structure of value: '\n",
      "                       'Accounting for taste”. In: Affect and cognition . '\n",
      "                       'Psychology Press, 2014,\\n'\n",
      "                       'pp. 3–36.\\n'\n",
      "                       '[67] Harsh Mehta, Ankit Gupta, Ashok Cutkosky, and '\n",
      "                       'Behnam Neyshabur. “Long Range Language Modeling via\\n'\n",
      "                       'Gated State Spaces”. In: The Eleventh International '\n",
      "                       'Conference on Learning Representations . 2023. url: '\n",
      "                       'https:\\n'\n",
      "                       '//openreview.net/forum?id=5MkYIYCbva.\\n'\n",
      "                       '[68] Stephen Merity, Caiming Xiong, James Bradbury, '\n",
      "                       'and Richard Socher. “Pointer Sentinel Mixture Models”. '\n",
      "                       'In:\\n'\n",
      "                       'International Conference on Learning Representations . '\n",
      "                       '2017. url: https://openreview.net/forum?id=Byj72udxe.\\n'\n",
      "                       '[69] William Merrill, Jackson Petty, and Ashish '\n",
      "                       'Sabharwal. “The Illusion of State in State-Space '\n",
      "                       'Models”. In: Forty-first\\n'\n",
      "                       'International Conference on Machine Learning . 2024. '\n",
      "                       'url: https://openreview.net/forum?id=QZgo9JZpLq.\\n'\n",
      "                       '[70] Ravi Teja Mullapudi, Steven Chen, Keyi Zhang, '\n",
      "                       'Deva Ramanan, and Kayvon Fatahalian. “Online model '\n",
      "                       'distillation\\n'\n",
      "                       'for efficient video inference”. In: Proceedings of the '\n",
      "                       'IEEE/CVF International conference on computer vision . '\n",
      "                       '2019,\\n'\n",
      "                       'pp. 3573–3582.\\n'\n",
      "                       '[71] Tsendsuren Munkhdalai, Manaal Faruqui, and '\n",
      "                       'Siddharth Gopal. “Leave no context behind: Efficient '\n",
      "                       'infinite context\\n'\n",
      "                       'transformers with infini-attention”. In: arXiv '\n",
      "                       'preprint arXiv:2404.07143 (2024).\\n'\n",
      "                       '[72] Tsendsuren Munkhdalai, Alessandro Sordoni, Tong '\n",
      "                       'Wang, and Adam Trischler. “Metalearned neural memory”. '\n",
      "                       'In:\\n'\n",
      "                       'Advances in Neural Information Processing Systems 32 '\n",
      "                       '(2019).\\n'\n",
      "                       '[73] Tsendsuren Munkhdalai and Hong Yu. “Neural '\n",
      "                       'semantic encoders”. In: Proceedings of the conference. '\n",
      "                       'Association for\\n'\n",
      "                       'Computational Linguistics. Meeting . Vol. 1. NIH '\n",
      "                       'Public Access. 2017, p. 397.\\n'\n",
      "                       '[74] Eric Nguyen, Michael Poli, Marjan Faizi, Armin '\n",
      "                       'Thomas, Michael Wornow, Callum Birch-Sykes, Stefano '\n",
      "                       'Massaroli,\\n'\n",
      "                       'Aman Patel, Clayton Rabideau, Yoshua Bengio, et al. '\n",
      "                       '“Hyenadna: Long-range genomic sequence modeling at '\n",
      "                       'single\\n'\n",
      "                       'nucleotide resolution”. In: Advances in neural '\n",
      "                       'information processing systems 36 (2024).\\n'\n",
      "                       '[75] A Nichol. “On first-order meta-learning '\n",
      "                       'algorithms”. In: arXiv preprint arXiv:1803.02999 '\n",
      "                       '(2018).\\n'\n",
      "                       '[76] Yuqi Nie, Nam H Nguyen, Phanwadee Sinthong, and '\n",
      "                       'Jayant Kalagnanam. “A time series is worth 64 words:\\n'\n",
      "                       'Long-term forecasting with transformers”. In: arXiv '\n",
      "                       'preprint arXiv:2211.14730 (2022).\\n'\n",
      "                       '[77] Hideyuki Okano, Tomoo Hirano, and Evan Balaban. '\n",
      "                       '“Learning and memory”. In:Proceedings of the National '\n",
      "                       'Academy\\n'\n",
      "                       'of Sciences 97.23 (2000), pp. 12403–12404.\\n'\n",
      "                       '[78] Antonio Orvieto, Samuel L Smith, Albert Gu, '\n",
      "                       'Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and '\n",
      "                       'Soham De.\\n'\n",
      "                       '“Resurrecting recurrent neural networks for long '\n",
      "                       'sequences”. In: International Conference on Machine '\n",
      "                       'Learning .\\n'\n",
      "                       'PMLR. 2023, pp. 26670–26698.\\n'\n",
      "                       '[79] Denis Paperno, Germán Kruszewski, Angeliki '\n",
      "                       'Lazaridou, Ngoc Quan Pham, Raffaella Bernardi, Sandro '\n",
      "                       'Pezzelle,\\n'\n",
      "                       'Marco Baroni, Gemma Boleda, and Raquel Fernández. “The '\n",
      "                       'LAMBADA dataset: Word prediction requiring a broad\\n'\n",
      "                       'discourse context”. In:Proceedings of the 54th Annual '\n",
      "                       'Meeting of the Association for Computational '\n",
      "                       'Linguistics (Volume\\n'\n",
      "                       '1: Long Papers) . Ed. by Katrin Erk and Noah A. Smith. '\n",
      "                       'Berlin, Germany: Association for Computational '\n",
      "                       'Linguistics,\\n'\n",
      "                       'Aug. 2016, pp. 1525–1534. doi: 10.18653/v1/P16-1144. '\n",
      "                       'url: https://aclanthology.org/P16-1144/.\\n'\n",
      "                       '[80] Badri N. Patro and Vijay S. Agneeswaran. SiMBA: '\n",
      "                       'Simplified Mamba-Based Architecture for Vision and '\n",
      "                       'Multivariate\\n'\n",
      "                       'Time series . 2024. arXiv: 2403.15360 [cs.CV].\\n'\n",
      "                       '[81] Guilherme Penedo, Hynek Kydlíček, Loubna Ben '\n",
      "                       'allal, Anton Lozhkov, Margaret Mitchell, Colin Raffel, '\n",
      "                       'Leandro\\n'\n",
      "                       'Von Werra, and Thomas Wolf. “The FineWeb Datasets: '\n",
      "                       'Decanting the Web for the Finest Text Data at Scale”. '\n",
      "                       'In:\\n'\n",
      "                       'The Thirty-eight Conference on Neural Information '\n",
      "                       'Processing Systems Datasets and Benchmarks Track . '\n",
      "                       '2024. url:\\n'\n",
      "                       'https://openreview.net/forum?id=n6SCkn2QaG.\\n'\n",
      "                       '[82] Bo Peng. RWKV-LM. Version 1.0.0. Aug. 2021. doi: '\n",
      "                       '10.5281/zenodo.5196577 . url: https://github.com/\\n'\n",
      "                       'BlinkDL/RWKV-LM.\\n'\n",
      "                       '21\\n'\n",
      "                       '[83] Bo Peng, Eric Alcaide, Quentin Gregory Anthony, '\n",
      "                       'Alon Albalak, Samuel Arcadinho, Stella Biderman, '\n",
      "                       'Huanqi Cao,\\n'\n",
      "                       'Xin Cheng, Michael Nguyen Chung, Leon Derczynski, '\n",
      "                       'Xingjian Du, Matteo Grella, Kranthi Kiran GV, Xuzheng '\n",
      "                       'He,\\n'\n",
      "                       'Haowen Hou, Przemyslaw Kazienko, Jan Kocon, Jiaming '\n",
      "                       'Kong, Bartłomiej Koptyra, Hayden Lau, Jiaju Lin, '\n",
      "                       'Krishna\\n'\n",
      "                       'Sri Ipsit Mantri, Ferdinand Mom, Atsushi Saito, '\n",
      "                       'Guangyu Song, Xiangru Tang, Johan S. Wind, Stanisław '\n",
      "                       'Woźniak,\\n'\n",
      "                       'Zhenyuan Zhang, Qinghua Zhou, Jian Zhu, and Rui-Jie '\n",
      "                       'Zhu. “RWKV: Reinventing RNNs for the Transformer '\n",
      "                       'Era”.\\n'\n",
      "                       'In: The 2023 Conference on Empirical Methods in '\n",
      "                       'Natural Language Processing . 2023. url: '\n",
      "                       'https://openreview.\\n'\n",
      "                       'net/forum?id=7SaXczaBpG.\\n'\n",
      "                       '[84] Bo Peng, Daniel Goldstein, Quentin Anthony, Alon '\n",
      "                       'Albalak, Eric Alcaide, Stella Biderman, Eugene Cheah, '\n",
      "                       'Xingjian\\n'\n",
      "                       'Du, Teddy Ferdinan, Haowen Hou, et al. “Eagle and '\n",
      "                       'finch: Rwkv with matrix-valued states and dynamic '\n",
      "                       'recurrence”.\\n'\n",
      "                       'In: arXiv preprint arXiv:2404.05892 (2024).\\n'\n",
      "                       '[85] DL Prados and SC Kak. “Neural network capacity '\n",
      "                       'using delta rule”. In: Electronics Letters 25.3 '\n",
      "                       '(1989), pp. 197–199.\\n'\n",
      "                       '[86] Zhen Qin, Yiran Zhong, and Hui Deng. “Exploring '\n",
      "                       'Transformer Extrapolation”. In: Proceedings of the '\n",
      "                       'AAAI\\n'\n",
      "                       'Conference on Artificial Intelligence . Vol. 38. 17. '\n",
      "                       '2024, pp. 18897–18905.\\n'\n",
      "                       '[87] Liliang Ren, Yang Liu, Yadong Lu, Yelong Shen, '\n",
      "                       'Chen Liang, and Weizhu Chen. “Samba: Simple Hybrid '\n",
      "                       'State Space\\n'\n",
      "                       'Models for Efficient Unlimited Context Language '\n",
      "                       'Modeling”. In: arXiv preprint arXiv:2406.07522 '\n",
      "                       '(2024).\\n'\n",
      "                       '[88] Ivan Rodkin, Yuri Kuratov, Aydar Bulatov, and '\n",
      "                       'Mikhail Burtsev. “Associative recurrent memory '\n",
      "                       'transformer”. In:\\n'\n",
      "                       'arXiv preprint arXiv:2407.04841 (2024).\\n'\n",
      "                       '[89] Aurko Roy, Mohammad Saffar, Ashish Vaswani, and '\n",
      "                       'David Grangier. “Efficient content-based sparse '\n",
      "                       'attention with\\n'\n",
      "                       'routing transformers”. In: Transactions of the '\n",
      "                       'Association for Computational Linguistics 9 (2021), '\n",
      "                       'pp. 53–68.\\n'\n",
      "                       '[90] Keisuke Sakaguchi, Ronan Le Bras, Chandra '\n",
      "                       'Bhagavatula, and Yejin Choi. “Winogrande: An '\n",
      "                       'adversarial winograd\\n'\n",
      "                       'schema challenge at scale”. In: Communications of the '\n",
      "                       'ACM 64.9 (2021), pp. 99–106.\\n'\n",
      "                       '[91] Maarten Sap, Hannah Rashkin, Derek Chen, Ronan Le '\n",
      "                       'Bras, and Yejin Choi. “Social IQa: Commonsense '\n",
      "                       'Reasoning\\n'\n",
      "                       'about Social Interactions”. In:Proceedings of the 2019 '\n",
      "                       'Conference on Empirical Methods in Natural Language '\n",
      "                       'Processing\\n'\n",
      "                       'and the 9th International Joint Conference on Natural '\n",
      "                       'Language Processing (EMNLP-IJCNLP) . Ed. by Kentaro '\n",
      "                       'Inui,\\n'\n",
      "                       'Jing Jiang, Vincent Ng, and Xiaojun Wan. Hong Kong, '\n",
      "                       'China: Association for Computational Linguistics, Nov. '\n",
      "                       '2019,\\n'\n",
      "                       'pp. 4463–4473. doi: 10.18653/v1/D19-1454. url: '\n",
      "                       'https://aclanthology.org/D19-1454/.\\n'\n",
      "                       '[92] Imanol Schlag, Kazuki Irie, and Jürgen '\n",
      "                       'Schmidhuber. “Linear transformers are secretly fast '\n",
      "                       'weight programmers”.\\n'\n",
      "                       'In: International Conference on Machine Learning . '\n",
      "                       'PMLR. 2021, pp. 9355–9366.\\n'\n",
      "                       '[93] JH Schmidhuber. “Learning to control fast-weight '\n",
      "                       'memories: An alternative to recurrent nets. Accepted '\n",
      "                       'for\\n'\n",
      "                       'publication in”. In: Neural Computation (1992).\\n'\n",
      "                       '[94] Jürgen Schmidhuber. “Reducing the ratio between '\n",
      "                       'learning complexity and number of time varying '\n",
      "                       'variables\\n'\n",
      "                       'in fully recurrent nets”. In: ICANN’93: Proceedings of '\n",
      "                       'the International Conference on Artificial Neural '\n",
      "                       'Networks\\n'\n",
      "                       'Amsterdam, The Netherlands 13–16 September 1993 3 . '\n",
      "                       'Springer. 1993, pp. 460–463.\\n'\n",
      "                       '[95] Jürgen Schmidhuber and Sepp Hochreiter. “Long '\n",
      "                       'Short-term Memory”. In: Neural Computation MIT-Press '\n",
      "                       '(1997).\\n'\n",
      "                       '[96] Avi Schwarzschild, Zhili Feng, Pratyush Maini, '\n",
      "                       'Zachary C Lipton, and J Zico Kolter. “Rethinking llm '\n",
      "                       'memorization\\n'\n",
      "                       'through the lens of adversarial compression”. In: '\n",
      "                       'arXiv preprint arXiv:2404.15146 (2024).\\n'\n",
      "                       '[97] Jimmy T.H. Smith, Andrew Warrington, and Scott '\n",
      "                       'Linderman. “Simplified State Space Layers for Sequence '\n",
      "                       'Modeling”.\\n'\n",
      "                       'In: The Eleventh International Conference on Learning '\n",
      "                       'Representations . 2023. url: '\n",
      "                       'https://openreview.net/forum?\\n'\n",
      "                       'id=Ai8Hw3AXqks.\\n'\n",
      "                       '[98] Robin Staab, Mark Vero, Mislav Balunovic, and '\n",
      "                       'Martin Vechev. “Beyond Memorization: Violating Privacy '\n",
      "                       'via\\n'\n",
      "                       'Inference with Large Language Models”. In: The Twelfth '\n",
      "                       'International Conference on Learning Representations . '\n",
      "                       '2024.\\n'\n",
      "                       'url: https://openreview.net/forum?id=kmn0BhQk7p.\\n'\n",
      "                       '[99] Sainbayar Sukhbaatar, Edouard Grave, Guillaume '\n",
      "                       'Lample, Herve Jegou, and Armand Joulin. “Augmenting '\n",
      "                       'self-\\n'\n",
      "                       'attention with persistent memory”. In: arXiv preprint '\n",
      "                       'arXiv:1907.01470 (2019).\\n'\n",
      "                       '[100] Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, '\n",
      "                       'et al. “End-to-end memory networks”. In: Advances in '\n",
      "                       'neural\\n'\n",
      "                       'information processing systems 28 (2015).\\n'\n",
      "                       '[101] Yu Sun, Xinhao Li, Karan Dalal, Jiarui Xu, Arjun '\n",
      "                       'Vikram, Genghan Zhang, Yann Dubois, Xinlei Chen, '\n",
      "                       'Xiaolong\\n'\n",
      "                       'Wang, Sanmi Koyejo, et al. “Learning to (learn at test '\n",
      "                       'time): Rnns with expressive hidden states”. In: arXiv '\n",
      "                       'preprint\\n'\n",
      "                       'arXiv:2407.04620 (2024).\\n'\n",
      "                       '[102] Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, '\n",
      "                       'Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei. '\n",
      "                       '“Retentive\\n'\n",
      "                       'network: A successor to transformer for large language '\n",
      "                       'models”. In: arXiv preprint arXiv:2307.08621 (2023).\\n'\n",
      "                       '[103] Gemma Team, Thomas Mesnard, Cassidy Hardin, '\n",
      "                       'Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, '\n",
      "                       'Laurent Sifre,\\n'\n",
      "                       'Morgane Rivière, Mihir Sanjay Kale, Juliette Love, et '\n",
      "                       'al. “Gemma: Open models based on gemini research and\\n'\n",
      "                       'technology”. In: arXiv preprint arXiv:2403.08295 '\n",
      "                       '(2024).\\n'\n",
      "                       '22\\n'\n",
      "                       '[104] W Scott Terry. Learning and memory: Basic '\n",
      "                       'principles, processes, and procedures . Routledge, '\n",
      "                       '2017.\\n'\n",
      "                       '[105] Matteo Tiezzi, Michele Casoni, Alessandro Betti, '\n",
      "                       'Tommaso Guidi, Marco Gori, and Stefano Melacci. “On '\n",
      "                       'the\\n'\n",
      "                       'resurgence of recurrent models for long sequences: '\n",
      "                       'Survey and research opportunities in the transformer '\n",
      "                       'era”. In:\\n'\n",
      "                       'arXiv preprint arXiv:2402.08132 (2024).\\n'\n",
      "                       '[106] Hugo Touvron, Thibaut Lavril, Gautier Izacard, '\n",
      "                       'Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, '\n",
      "                       'Baptiste\\n'\n",
      "                       'Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et '\n",
      "                       'al. “Llama: Open and efficient foundation language '\n",
      "                       'models”.\\n'\n",
      "                       'In: arXiv preprint arXiv:2302.13971 (2023).\\n'\n",
      "                       '[107] Jos Van Der Westhuizen and Joan Lasenby. “The '\n",
      "                       'unreasonable effectiveness of the forget gate”. '\n",
      "                       'In:arXiv preprint\\n'\n",
      "                       'arXiv:1804.04849 (2018).\\n'\n",
      "                       '[108] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob '\n",
      "                       'Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser,\\n'\n",
      "                       'and Illia Polosukhin. “Attention is All you Need”. In: '\n",
      "                       'Advances in Neural Information Processing Systems . '\n",
      "                       'Ed.\\n'\n",
      "                       'by I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. '\n",
      "                       'Fergus, S. Vishwanathan, and R. Garnett. Vol. 30. '\n",
      "                       'Cur-\\n'\n",
      "                       'ran Associates, Inc., 2017. url: https : / / '\n",
      "                       'proceedings . neurips . cc / paper _ files / paper / '\n",
      "                       '2017 / file /\\n'\n",
      "                       '3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.\\n'\n",
      "                       '[109] Shida Wang. “LongSSM: On the Length Extension of '\n",
      "                       'State-space Models in Language Modelling”. In: arXiv '\n",
      "                       'preprint\\n'\n",
      "                       'arXiv:2406.02080 (2024).\\n'\n",
      "                       '[110] Yu Wang, Yifan Gao, Xiusi Chen, Haoming Jiang, '\n",
      "                       'Shiyang Li, Jingfeng Yang, Qingyu Yin, Zheng Li, Xian '\n",
      "                       'Li, Bing Yin,\\n'\n",
      "                       'Jingbo Shang, and Julian McAuley. “MEMORYLLM: Towards '\n",
      "                       'Self-Updatable Large Language Models”. In:Forty-first\\n'\n",
      "                       'International Conference on Machine Learning . 2024. '\n",
      "                       'url: https://openreview.net/forum?id=p0lKWzdikQ.\\n'\n",
      "                       '[111] Yu Wang, Chi Han, Tongtong Wu, Xiaoxin He, '\n",
      "                       'Wangchunshu Zhou, Nafis Sadeq, Xiusi Chen, Zexue He, '\n",
      "                       'Wei Wang,\\n'\n",
      "                       'Gholamreza Haffari, et al. “Towards LifeSpan Cognitive '\n",
      "                       'Systems”. In: arXiv preprint arXiv:2409.13265 (2024).\\n'\n",
      "                       '[112] Zhiwei Wang, Yao Ma, Zitao Liu, and Jiliang '\n",
      "                       'Tang. “R-transformer: Recurrent neural network '\n",
      "                       'enhanced transformer”.\\n'\n",
      "                       'In: arXiv preprint arXiv:1907.05572 (2019).\\n'\n",
      "                       '[113] Jason Weston, Sumit Chopra, and Antoine Bordes. '\n",
      "                       '“Memory networks”. In: arXiv preprint arXiv:1410.3916 '\n",
      "                       '(2014).\\n'\n",
      "                       '[114] Bernard Widrow and Marcian E Hoff. “Adaptive '\n",
      "                       'switching circuits”. In: Neurocomputing: foundations '\n",
      "                       'of research .\\n'\n",
      "                       '1988, pp. 123–134.\\n'\n",
      "                       '[115] Ronald J Williams and David Zipser. “A learning '\n",
      "                       'algorithm for continually running fully recurrent '\n",
      "                       'neural networks”.\\n'\n",
      "                       'In: Neural computation 1.2 (1989), pp. 270–280.\\n'\n",
      "                       '[116] Daniel B Willingham. “Systems of memory in the '\n",
      "                       'human brain”. In: Neuron 18.1 (1997), pp. 5–8.\\n'\n",
      "                       '[117] Chao-Yuan Wu, Christoph Feichtenhofer, Haoqi '\n",
      "                       'Fan, Kaiming He, Philipp Krahenbuhl, and Ross '\n",
      "                       'Girshick. “Long-\\n'\n",
      "                       'term feature banks for detailed video understanding”. '\n",
      "                       'In: Proceedings of the IEEE/CVF conference on computer '\n",
      "                       'vision\\n'\n",
      "                       'and pattern recognition . 2019, pp. 284–293.\\n'\n",
      "                       '[118] Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, '\n",
      "                       'Jianmin Wang, and Mingsheng Long. “TimesNet: Temporal '\n",
      "                       '2D-\\n'\n",
      "                       'Variation Modeling for General Time Series Analysis”. '\n",
      "                       'In: The Eleventh International Conference on Learning\\n'\n",
      "                       'Representations. 2023. url: '\n",
      "                       'https://openreview.net/forum?id=ju_Uqw384Oq.\\n'\n",
      "                       '[119] Qingyang Wu, Zhenzhong Lan, Kun Qian, Jing Gu, '\n",
      "                       'Alborz Geramifard, and Zhou Yu. “Memformer: A memory-\\n'\n",
      "                       'augmented transformer for sequence modeling”. In: '\n",
      "                       'arXiv preprint arXiv:2010.06891 (2020).\\n'\n",
      "                       '[120] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song '\n",
      "                       'Han, and Mike Lewis. “Efficient Streaming Language '\n",
      "                       'Models\\n'\n",
      "                       'with Attention Sinks”. In: The Twelfth International '\n",
      "                       'Conference on Learning Representations . 2024. url: '\n",
      "                       'https:\\n'\n",
      "                       '//openreview.net/forum?id=NG7sS51zVF.\\n'\n",
      "                       '[121] An Yang, Baosong Yang, Beichen Zhang, Binyuan '\n",
      "                       'Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, '\n",
      "                       'Fei\\n'\n",
      "                       'Huang, Haoran Wei, et al. “Qwen2. 5 Technical Report”. '\n",
      "                       'In:arXiv preprint arXiv:2412.15115 (2024).\\n'\n",
      "                       '[122] Songlin Yang, Jan Kautz, and Ali Hatamizadeh. '\n",
      "                       '“Gated Delta Networks: Improving Mamba2 with Delta '\n",
      "                       'Rule”. In:\\n'\n",
      "                       'arXiv preprint arXiv:2412.06464 (2024).\\n'\n",
      "                       '[123] Songlin Yang, Bailin Wang, Yikang Shen, Rameswar '\n",
      "                       'Panda, and Yoon Kim. “Gated Linear Attention '\n",
      "                       'Transformers\\n'\n",
      "                       'with Hardware-Efficient Training”. In: Forty-first '\n",
      "                       'International Conference on Machine Learning . 2024. '\n",
      "                       'url: https:\\n'\n",
      "                       '//openreview.net/forum?id=ia5XvxFUJT.\\n'\n",
      "                       '[124] Songlin Yang, Bailin Wang, Yu Zhang, Yikang '\n",
      "                       'Shen, and Yoon Kim. “Parallelizing Linear Transformers '\n",
      "                       'with the\\n'\n",
      "                       'Delta Rule over Sequence Length”. In:The Thirty-eighth '\n",
      "                       'Annual Conference on Neural Information Processing '\n",
      "                       'Systems .\\n'\n",
      "                       '2024. url: '\n",
      "                       'https://openreview.net/forum?id=y8Rm4VNRPH.\\n'\n",
      "                       '[125] Luca Zancato, Arjun Seshadri, Yonatan Dukler, '\n",
      "                       'Aditya Golatkar, Yantao Shen, Benjamin Bowman, Matthew '\n",
      "                       'Trager,\\n'\n",
      "                       'Alessandro Achille, and Stefano Soatto. “B’MOJO: '\n",
      "                       'Hybrid State Space Realizations of Foundation Models '\n",
      "                       'with\\n'\n",
      "                       'Eidetic and Fading Memory”. In: The Thirty-eighth '\n",
      "                       'Annual Conference on Neural Information Processing '\n",
      "                       'Systems .\\n'\n",
      "                       '2024. url: '\n",
      "                       'https://openreview.net/forum?id=RnQdRY1h5v.\\n'\n",
      "                       '23\\n'\n",
      "                       '[126] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali '\n",
      "                       'Farhadi, and Yejin Choi. “HellaSwag: Can a Machine '\n",
      "                       'Really Finish\\n'\n",
      "                       'Your Sentence?” In: Proceedings of the 57th Annual '\n",
      "                       'Meeting of the Association for Computational '\n",
      "                       'Linguistics . Ed. by\\n'\n",
      "                       'Anna Korhonen, David Traum, and Lluís Màrquez. '\n",
      "                       'Florence, Italy: Association for Computational '\n",
      "                       'Linguistics, July\\n'\n",
      "                       '2019, pp. 4791–4800. doi: 10.18653/v1/P19-1472. url: '\n",
      "                       'https://aclanthology.org/P19-1472/.\\n'\n",
      "                       '[127] Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu. '\n",
      "                       '“Are transformers effective for time series '\n",
      "                       'forecasting?” In:\\n'\n",
      "                       'Proceedings of the AAAI conference on artificial '\n",
      "                       'intelligence . Vol. 37. 2023, pp. 11121–11128.\\n'\n",
      "                       '[128] Hao Zhang, Alexander C Berg, Michael Maire, and '\n",
      "                       'Jitendra Malik. “SVM-KNN: Discriminative nearest '\n",
      "                       'neighbor\\n'\n",
      "                       'classification for visual category recognition”. In: '\n",
      "                       '2006 IEEE Computer Society Conference on Computer '\n",
      "                       'Vision and\\n'\n",
      "                       'Pattern Recognition (CVPR’06) . Vol. 2. IEEE. 2006, '\n",
      "                       'pp. 2126–2136.\\n'\n",
      "                       '[129] Jianyu Zhang, Niklas Nolte, Ranajoy Sadhukhan, '\n",
      "                       'Beidi Chen, and Léon Bottou. “Memory Mosaics”. '\n",
      "                       'In:arXiv preprint\\n'\n",
      "                       'arXiv:2405.06394 (2024).\\n'\n",
      "                       '[130] Yunhao Zhang and Junchi Yan. “Crossformer: '\n",
      "                       'Transformer utilizing cross-dimension dependency for '\n",
      "                       'multivariate\\n'\n",
      "                       'time series forecasting”. In: The eleventh '\n",
      "                       'international conference on learning representations . '\n",
      "                       '2023.\\n'\n",
      "                       '[131] Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai '\n",
      "                       'Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang. '\n",
      "                       '“Informer:\\n'\n",
      "                       'Beyond efficient transformer for long sequence '\n",
      "                       'time-series forecasting”. In: Proceedings of the AAAI '\n",
      "                       'conference on\\n'\n",
      "                       'artificial intelligence . Vol. 35. 12. 2021, pp. '\n",
      "                       '11106–11115.\\n'\n",
      "                       '[132] Luisa Zintgraf, Kyriacos Shiarli, Vitaly Kurin, '\n",
      "                       'Katja Hofmann, and Shimon Whiteson. “Fast context '\n",
      "                       'adaptation via\\n'\n",
      "                       'meta-learning”. In: International Conference on '\n",
      "                       'Machine Learning . PMLR. 2019, pp. 7693–7702.\\n'\n",
      "                       '24\\n'\n",
      "                       'A Related Work\\n'\n",
      "                       'There are diverse perspectives that can independently '\n",
      "                       'lead to the design of Titans or its components. '\n",
      "                       'Accordingly, to\\n'\n",
      "                       'further situate our work in a broader context, we '\n",
      "                       'review three categories of studies:\\n'\n",
      "                       'A.1 Linear Recurrent Models\\n'\n",
      "                       'Recently, to address the computational cost of '\n",
      "                       'Transformers in both training and inference, linear '\n",
      "                       'recurrent models\\n'\n",
      "                       'have attracted much attention (Tiezzi et al. 2024), '\n",
      "                       'mainly due to their fast inference and training. The '\n",
      "                       'first generation\\n'\n",
      "                       'of models–such as RetNet (Yutao Sun et al. 2023), LRU '\n",
      "                       '(Orvieto et al. 2023), RWKV (Peng, Alcaide, et al. '\n",
      "                       '2023), S5 (J. T.\\n'\n",
      "                       'Smith, Warrington, and Linderman 2023), and S4 (Gu, '\n",
      "                       'Goel, and Re 2022)–uses data-independent transition '\n",
      "                       'matrix/decay\\n'\n",
      "                       'mechanism. The second generation of such models '\n",
      "                       'started to incorporate gating mechanism, a widely used '\n",
      "                       'techniques\\n'\n",
      "                       'in traditional RNNs (Gers, Jürgen Schmidhuber, and '\n",
      "                       'Cummins 2000; Greff et al. 2016; Van Der Westhuizen '\n",
      "                       'and Lasenby\\n'\n",
      "                       '2018), into such linear architectures–e.g., Griffin '\n",
      "                       '(De et al. 2024), SSMs (Behrouz, Santacatterina, and '\n",
      "                       'Zabih 2024; Dao\\n'\n",
      "                       'and Gu 2024; Gu and Dao 2024; Hasani et al. 2023), '\n",
      "                       'RWKV6 (Peng, Goldstein, et al. 2024). The third '\n",
      "                       'generation of linear\\n'\n",
      "                       'recurrent models are based on more complex memory '\n",
      "                       'updating rule based on meta-learning, online learning, '\n",
      "                       'and/or\\n'\n",
      "                       'delta-rule, resulting in more expressive and effective '\n",
      "                       'models such as: Longhorn (B. Liu et al. 2024), Gated '\n",
      "                       'DeltaNet (S. Yang,\\n'\n",
      "                       'Kautz, and Hatamizadeh 2024), TTT (Yu Sun et al. '\n",
      "                       '2024), and DeltaNet (S. Yang, B. Wang, Yu Zhang, et '\n",
      "                       'al. 2024). Our\\n'\n",
      "                       'LMM model can be seen as the next generation of such '\n",
      "                       'models, in which we incorporate the token flow into '\n",
      "                       'the memory\\n'\n",
      "                       'updating mechanism, having more powerful memory '\n",
      "                       'updating process. See Appendix C for a detailed '\n",
      "                       'discussion of\\n'\n",
      "                       'different recurrent models and Titans.\\n'\n",
      "                       'A.2 Transformer-based Architectures\\n'\n",
      "                       'Transformers. Transformers (Vaswani et al. 2017) as '\n",
      "                       'the de facto backbone for many deep learning models '\n",
      "                       'are based on\\n'\n",
      "                       'attention mechanism (Bahdanau 2014). They, however, '\n",
      "                       'suffer from quadratic computational cost, limiting '\n",
      "                       'their ability\\n'\n",
      "                       'to scale to long context window. To improve the memory '\n",
      "                       'consumption and throughput of softmax attention for '\n",
      "                       'longer\\n'\n",
      "                       'sequences, various studies focused on I/O aware '\n",
      "                       'implementations of attention (Dao 2024; Dao, D. Fu, et '\n",
      "                       'al. 2022), designing\\n'\n",
      "                       'more efficient attention mechanisms by sparsifying the '\n",
      "                       'attention matrix (B. Chen et al. 2021; Choromanski et '\n",
      "                       'al. 2021; Dai\\n'\n",
      "                       'et al. 2019; J. Dong et al. 2024; Roy et al. 2021), '\n",
      "                       'approximating the softmax (Arora et al. 2024), or '\n",
      "                       'developing kernel-based\\n'\n",
      "                       '(linear) attentions (Aksenov et al. 2024; Kacham, '\n",
      "                       'Mirrokni, and P. Zhong 2024; Schlag, Irie, and Jürgen '\n",
      "                       'Schmidhuber 2021;\\n'\n",
      "                       'S. Yang, B. Wang, Shen, et al. 2024).\\n'\n",
      "                       'Segment-based Transformers. Another line of research '\n",
      "                       'to improve the efficiency of Transformers is '\n",
      "                       'segment-based or\\n'\n",
      "                       'Chunk Transformers (Dai et al. 2019). The main '\n",
      "                       'drawback of chunk Transformers is that segments are '\n",
      "                       'fully separated and\\n'\n",
      "                       'so the context window is limited to the length of the '\n",
      "                       'chunks. To address this issue, various studies discuss '\n",
      "                       'the importance\\n'\n",
      "                       'of a memory so it can help the model to transfer '\n",
      "                       'information across chunks (Bulatov, Yuri Kuratov, et '\n",
      "                       'al. 2023; Bulatov,\\n'\n",
      "                       'Yury Kuratov, and Burtsev 2022; Feng et al. 2022; '\n",
      "                       'Hutchins et al. 2022; Rodkin et al. 2024; Z. Wang et '\n",
      "                       'al. 2019; Q. Wu\\n'\n",
      "                       'et al. 2020; Zancato et al. 2024). The key differences '\n",
      "                       'of Titans with these models are: (1) The memory in '\n",
      "                       'such models are\\n'\n",
      "                       'simple small size vectors, lacking expressive power to '\n",
      "                       'compress complex information; (2) The memory module '\n",
      "                       'lacks forget\\n'\n",
      "                       'mechanism, leading to a fast memory overflow; (3) only '\n",
      "                       'focus on momentary surprise, missing the information '\n",
      "                       'flow. More\\n'\n",
      "                       'specifically, recalling Recurrent Memory Transformers '\n",
      "                       '(RMT) (Bulatov, Yuri Kuratov, et al. 2023; Bulatov, '\n",
      "                       'Yury Kuratov,\\n'\n",
      "                       'and Burtsev 2022; Rodkin et al. 2024), one can treat '\n",
      "                       'Titans (MAC) as the generalization of RMT, where we '\n",
      "                       'use a neural\\n'\n",
      "                       'memory module instead of a vector-valued small size '\n",
      "                       'memory.\\n'\n",
      "                       'Memory for Large Language Models. Another interesting '\n",
      "                       'research direction has been to incorporate external '\n",
      "                       'memory\\n'\n",
      "                       'modules to LLMs after training (Z. He et al. 2024; '\n",
      "                       'Khandelwal et al. 2020; Y. Wang, Y. Gao, et al. 2024). '\n",
      "                       'Such models\\n'\n",
      "                       'are different from our approach as we incorporate the '\n",
      "                       'memory as a part of initial architecture and so we '\n",
      "                       'train it in\\n'\n",
      "                       'an end-to-end manner. Also, most of these explicit '\n",
      "                       'memory modules suffer from the same limitations as '\n",
      "                       'chunk-based\\n'\n",
      "                       'Transformers (mentioned above). For a detailed '\n",
      "                       'discussion of such models, we refer to the recent '\n",
      "                       'study of Y. Wang, Han,\\n'\n",
      "                       'et al. (2024).\\n'\n",
      "                       '25\\n'\n",
      "                       'A.3 Test Time Training and Fast Weight Programs\\n'\n",
      "                       'Memory Design and Augmentation with Memory. In the '\n",
      "                       'literature, a substantial research effort have been '\n",
      "                       'toward\\n'\n",
      "                       'designing memory modules that are capable of either '\n",
      "                       'memorizing the knowledge abstraction (e.g., persistent '\n",
      "                       'mem-\\n'\n",
      "                       'ory) (Sukhbaatar, Grave, et al. 2019), or memorizing '\n",
      "                       'the data-dependent information (also known as '\n",
      "                       'contextual memory),\\n'\n",
      "                       'through recurrence (Bulatov, Yury Kuratov, and Burtsev '\n",
      "                       '2022; Rodkin et al. 2024; Zancato et al. 2024), '\n",
      "                       'Transformers (Berges\\n'\n",
      "                       'et al. 2024; Cetin et al. 2024; Feng et al. 2022; Le, '\n",
      "                       'Tran, and Venkatesh 2020; Munkhdalai, Faruqui, and '\n",
      "                       'Gopal 2024; J. Zhang\\n'\n",
      "                       'et al. 2024), gradient (Irie, Csordás, and Jürgen '\n",
      "                       'Schmidhuber 2022; Munkhdalai, Sordoni, et al. 2019), '\n",
      "                       'or other learning\\n'\n",
      "                       'paradigms (Sukhbaatar, Weston, Fergus, et al. 2015; '\n",
      "                       'Weston, Chopra, and Bordes 2014). These memory models, '\n",
      "                       'however,\\n'\n",
      "                       'either (1) are based on momentary surprise, missing '\n",
      "                       'the data flow and events, (2) lack forget mechanisms '\n",
      "                       'to remove\\n'\n",
      "                       'the memory, leading to a fast memory overflow (3) are '\n",
      "                       'fixed-size shallow (matrix valued) memory, resulting '\n",
      "                       'in poor\\n'\n",
      "                       'performance in long context, and (4) are based on '\n",
      "                       'fixed parameters at test time, lacking test time '\n",
      "                       'adaption.\\n'\n",
      "                       'Fast Weight Programs. The idea of seeing linear layers '\n",
      "                       'as the key-value (associative) memory system backs to '\n",
      "                       'fast\\n'\n",
      "                       'weight programs, in which dynamic fast programs are '\n",
      "                       'incorporated into recurrent neural networks to serve '\n",
      "                       'as writable\\n'\n",
      "                       'memory (Schlag, Irie, and Jürgen Schmidhuber 2021; JH '\n",
      "                       'Schmidhuber 1992; Jürgen Schmidhuber 1993). The two '\n",
      "                       'learning\\n'\n",
      "                       'rules of Hebbian (Hebb 2005) and delta (Prados and Kak '\n",
      "                       '1989) are the most popular learning rules for fast '\n",
      "                       'weight programs,\\n'\n",
      "                       'which have been extensively explored in various '\n",
      "                       'studies (Irie, Schlag, et al. 2021; Munkhdalai, '\n",
      "                       'Sordoni, et al. 2019;\\n'\n",
      "                       'Munkhdalai and H. Yu 2017; Schlag, Irie, and Jürgen '\n",
      "                       'Schmidhuber 2021; JH Schmidhuber 1992; S. Yang, Kautz, '\n",
      "                       'and\\n'\n",
      "                       'Hatamizadeh 2024; S. Yang, B. Wang, Yu Zhang, et al. '\n",
      "                       '2024). All these models, however, are based on '\n",
      "                       'momentary surprise,\\n'\n",
      "                       'missing the token flow in the sequences (see Section '\n",
      "                       '3.1), and most of them lacks a forgetting gate, '\n",
      "                       'resulting in a poor\\n'\n",
      "                       'memory management.\\n'\n",
      "                       'Test Time Training. The key ideas of learning at test '\n",
      "                       'time or learning to learn (i.e., (Andrychowicz et al. '\n",
      "                       '2016)) backs to\\n'\n",
      "                       'very early studies on local learning Bottou and Vapnik '\n",
      "                       '1992, in which each test data sample is trained on its '\n",
      "                       'neighbors\\n'\n",
      "                       'before making a prediction (Gandelsman et al. 2022; H. '\n",
      "                       'Zhang et al. 2006). This approach further has shown '\n",
      "                       'promising\\n'\n",
      "                       'performance in vision tasks (Jain and Learned-Miller '\n",
      "                       '2011; Mullapudi et al. 2019), mostly due to their '\n",
      "                       'ability to mitigate\\n'\n",
      "                       'out-of-distribution samples. The most similar studies '\n",
      "                       'to ours in this direction are MNM (Munkhdalai, '\n",
      "                       'Sordoni, et al. 2019)\\n'\n",
      "                       'and TTT-layer (Yu Sun et al. 2024), which we discussed '\n",
      "                       'the key differences in Appendix C.\\n'\n",
      "                       'B Language Modeling and Common-sense Reasoning '\n",
      "                       'Datasets\\n'\n",
      "                       'Following recent studies on linear recurrent models '\n",
      "                       '(Dao and Gu 2024; S. Yang, Kautz, and Hatamizadeh '\n",
      "                       '2024; S. Yang,\\n'\n",
      "                       'B. Wang, Yu Zhang, et al. 2024), we use Wikitext '\n",
      "                       '(Merity et al. 2017), LMB (Paperno et al. 2016), PIQA '\n",
      "                       '(Bisk et al. 2020),\\n'\n",
      "                       'HellaSwag (Zellers et al. 2019), WinoGrande (Sakaguchi '\n",
      "                       'et al. 2021), ARC-easy (ARC-e) and ARC-challenge '\n",
      "                       '(ARC-c) (P.\\n'\n",
      "                       'Clark et al. 2018), SIQA (Sap et al. 2019), and BoolQ '\n",
      "                       '(C. Clark et al. 2019). Also, the baselines results '\n",
      "                       'for 400M models are\\n'\n",
      "                       'from the reported results by S. Yang, Kautz, and '\n",
      "                       'Hatamizadeh (2024).\\n'\n",
      "                       'C Long-term Memory Module (LMM) as a Sequence Model\\n'\n",
      "                       'In this section, we discuss how LMM as a sequence '\n",
      "                       'model is connected to modern linear recurrent models. '\n",
      "                       'For the sake\\n'\n",
      "                       'of simplicity, we start with a linear memory, where M𝑡 '\n",
      "                       '= 𝑊𝑡 ∈R𝑑in ×𝑑in . In this case, our objective function '\n",
      "                       'becomes\\n'\n",
      "                       'ℓ(M; 𝑥𝑡)= 1\\n'\n",
      "                       '2 ∥M𝑡k𝑡 −v𝑡∥2\\n'\n",
      "                       '2, in which we use gradient descent with momentum and '\n",
      "                       'weight decay for the optimization.\\n'\n",
      "                       'Accordingly, revisiting the recurrent formula in '\n",
      "                       'Equation 13:\\n'\n",
      "                       'M𝑡 = diag (1 −𝛼𝑡)M𝑡 +𝑆𝑡 (32)\\n'\n",
      "                       '𝑆𝑡 = diag (𝜂𝑡)𝑆𝑡−1 −diag (𝜃𝑡)\\x00M𝑡−1k⊤\\n'\n",
      "                       '𝑡 k𝑡 −v⊤\\n'\n",
      "                       '𝑡 k𝑡\\n'\n",
      "                       '\\x01 . (33)\\n'\n",
      "                       'LMM is Generalized Gated DeltaNet. As discussed by S. '\n",
      "                       'Yang, Kautz, and Hatamizadeh (2024), DeltaNet (S. '\n",
      "                       'Yang, B. Wang,\\n'\n",
      "                       'Yu Zhang, et al. 2024) can alternatively be '\n",
      "                       'interpreted as an online learning problem that '\n",
      "                       'optimizes the L= 1\\n'\n",
      "                       '2 ∥S𝑡k𝑡 −v𝑡∥2\\n'\n",
      "                       '2,\\n'\n",
      "                       'resulting in:\\n'\n",
      "                       'S𝑡+1 = S𝑡 −𝜃𝑡∇L= S𝑡\\n'\n",
      "                       '\\x00I −𝜃𝑡k𝑡k⊤\\n'\n",
      "                       '𝑡\\n'\n",
      "                       '\\x01 +𝜃𝑡v𝑡k⊤\\n'\n",
      "                       '𝑡 . (34)\\n'\n",
      "                       '26\\n'\n",
      "                       'In this formulation, Gated DeltaNet is the same as '\n",
      "                       'above but with an additional weight decay term (S. '\n",
      "                       'Yang, Kautz, and\\n'\n",
      "                       'Hatamizadeh 2024). Comparing Equation 32 and Equation '\n",
      "                       '34, we can see that setting 𝜂𝑡 = 0 results in both '\n",
      "                       'formulations to\\n'\n",
      "                       'be equivalent. Accordingly, we can say LMM is '\n",
      "                       'generalizing the very recent study of Gated DeltaNet '\n",
      "                       '(S. Yang, Kautz, and\\n'\n",
      "                       'Hatamizadeh 2024) from three aspects:\\n'\n",
      "                       '• Momentum-based Rule: The Delta Rule is based on '\n",
      "                       'momentary surprise, meaning that the flow of tokens '\n",
      "                       'cannot\\n'\n",
      "                       'affect the memory update rule. LMM, however, is based '\n",
      "                       'on a momentum rule, which consider both past and\\n'\n",
      "                       'momentary surprise.\\n'\n",
      "                       '• Deep Memory: While Gated DeltaNet is limited to a '\n",
      "                       'linear (matrix-valued) memory as it requires finding '\n",
      "                       'the closed\\n'\n",
      "                       'recurrence form, LMM allows using deep memory module '\n",
      "                       'by using a gradient-based formulation, resulting in '\n",
      "                       'higher\\n'\n",
      "                       'expressive power.\\n'\n",
      "                       '• Non-Linear Recurrence: While DeltaNet and Gated '\n",
      "                       'DeltaNet are based on linear recurrence, our LMM is '\n",
      "                       'using\\n'\n",
      "                       'inter-chunk non-linear recurrence and intra-chunk '\n",
      "                       'linear recurrence. This design allows LMM having a '\n",
      "                       'higher\\n'\n",
      "                       'expressive power.\\n'\n",
      "                       'Here, we discussed Gated DeltaNet as a sample of '\n",
      "                       'recent generation of recurrent models. Similar '\n",
      "                       'approaches such\\n'\n",
      "                       'as RWKV-7 (Peng 2021) are also using the same '\n",
      "                       'formulation and loss function, and so LMM is '\n",
      "                       'generalizing all such\\n'\n",
      "                       'models.\\n'\n",
      "                       'LMM is Generalized Longhorn. Similar to DeltaNet, '\n",
      "                       'Longhorn (B. Liu et al. 2024) uses the same loss '\n",
      "                       'function but it\\n'\n",
      "                       'derives the closed form using implicit online '\n",
      "                       'learning:\\n'\n",
      "                       'S𝑡+1 = S𝑡\\n'\n",
      "                       '\\x00I −𝛿𝑡k𝑡k⊤\\n'\n",
      "                       '𝑡\\n'\n",
      "                       '\\x01 +𝛿𝑡v𝑡k⊤\\n'\n",
      "                       '𝑡 , (35)\\n'\n",
      "                       'where 𝛿𝑡 = 𝜃𝑡\\n'\n",
      "                       '1+𝜃𝑡 k𝑡 k⊤\\n'\n",
      "                       '𝑡\\n'\n",
      "                       '. It, however, lacks a forgetting gate, resulting in a '\n",
      "                       'faster memory overflow. Therefore, in addition two\\n'\n",
      "                       'the abovementioned aspects of (1) Momentum-based Rule, '\n",
      "                       '(2) Deep Memory, and (3) Non-Linear Recurrence, LMM '\n",
      "                       'has\\n'\n",
      "                       'the advantage of using an additional (4) Forget Gate, '\n",
      "                       'leading to a better memory management.\\n'\n",
      "                       'LMM is Generalized TTT Layer. To the best of our '\n",
      "                       'knowledge, TTT (Yu Sun et al. 2024), is the only '\n",
      "                       'modern linear\\n'\n",
      "                       'recurrent models with a gradient-based updating rule. '\n",
      "                       'In addition to different architectural designs and '\n",
      "                       'also objective\\n'\n",
      "                       'functions, our LMM has three key differences with '\n",
      "                       'presented TTT layers (Yu Sun et al. 2024):\\n'\n",
      "                       '1. Forgetting Mechanism : TTT layers are updating '\n",
      "                       'memory at each time, without having the chance to '\n",
      "                       'forget the\\n'\n",
      "                       'past data. Accordingly, when fixing the memory size, '\n",
      "                       'the model cannot manage the memory for long sequences. '\n",
      "                       'A\\n'\n",
      "                       'forget mechanism, such as LMM’s, allows clearing the '\n",
      "                       'memory when very past information is not needed '\n",
      "                       'anymore.\\n'\n",
      "                       'We show that in a general case, this forget mechanism '\n",
      "                       'is equivalent to weight decay and provide a fast '\n",
      "                       'method to\\n'\n",
      "                       'incorporate it into the parallel training.\\n'\n",
      "                       '2. Momentum-based Update Rule : TTT layers are based '\n",
      "                       'on momentary surprise, meaning that the flow of '\n",
      "                       'tokens\\n'\n",
      "                       'cannot affect the memory update rule. LMM, however, is '\n",
      "                       'based on a momentum rule, which consider both past '\n",
      "                       'and\\n'\n",
      "                       'momentary surprise. See Section 3.1 for the motivation '\n",
      "                       'of this design.\\n'\n",
      "                       '3. Deep Memory : While TTT-layers allows for deeper '\n",
      "                       'memory, the advantages/disadvantages of such deeper '\n",
      "                       'memory\\n'\n",
      "                       'modules have not been experimentally evaluated.\\n'\n",
      "                       'To the best of our knowledge, our neural long-term '\n",
      "                       'memory module is the first linear recurrent model with '\n",
      "                       'momentum-\\n'\n",
      "                       'based update rule.\\n'\n",
      "                       'Finally, as a key difference with all the above and '\n",
      "                       'other recent linear recurrent studies, note that the '\n",
      "                       'hybrid variants of\\n'\n",
      "                       'modern linear models–such as Griffin (De et al. 2024), '\n",
      "                       'DeltaNet (S. Yang, B. Wang, Yu Zhang, et al. 2024), '\n",
      "                       'Gated DeltaNet (S.\\n'\n",
      "                       'Yang, Kautz, and Hatamizadeh 2024), H3 (D. Y. Fu et '\n",
      "                       'al. 2023), Mamba2 (Dao and Gu 2024), Samba (Ren et al. '\n",
      "                       '2024), etc.–all\\n'\n",
      "                       'are based on sequential layer-wise design. We present '\n",
      "                       'Titans to show how effectively one can incorporate '\n",
      "                       'such memory\\n'\n",
      "                       'modules into an architecture.\\n'\n",
      "                       '27'}}\n",
      "\u001b[36;1m\u001b[1;3m[1:writes]\u001b[0m \u001b[1mFinished step 1 with writes to 1 channel:\n",
      "\u001b[0m- \u001b[33;1m\u001b[1;3mextract_information\u001b[0m -> {'state': {'error': None,\n",
      "           'extracted_info': {'Key_words': ['Titans',\n",
      "                                            'neural memory',\n",
      "                                            'long-term memory',\n",
      "                                            'attention',\n",
      "                                            'recurrent models',\n",
      "                                            'language modeling',\n",
      "                                            'test-time learning'],\n",
      "                              'authors': ['Ali Behrouz',\n",
      "                                          'Peilin Zhong',\n",
      "                                          'Vahab Mirrokni'],\n",
      "                              'document_id': 'arXiv_2501.00663v1',\n",
      "                              'key_points': ['Introduces a new neural '\n",
      "                                             'long-term memory module that '\n",
      "                                             'learns to memorize historical '\n",
      "                                             'context.',\n",
      "                                             'Proposes Titans, a family of '\n",
      "                                             'architectures combining '\n",
      "                                             'short-term attention and '\n",
      "                                             'long-term memory.',\n",
      "                                             \"Demonstrates Titans' scalability \"\n",
      "                                             'to context windows larger than '\n",
      "                                             '2M tokens.',\n",
      "                                             'Shows Titans outperform '\n",
      "                                             'Transformers and modern linear '\n",
      "                                             'recurrent models in various '\n",
      "                                             'tasks.',\n",
      "                                             'Presents three variants of '\n",
      "                                             'Titans: Memory as a Context '\n",
      "                                             '(MAC), Memory as Gating (MAG), '\n",
      "                                             'and Memory as a Layer (MAL).'],\n",
      "                              'methodology': 'The study designs a neural '\n",
      "                                             'long-term memory module inspired '\n",
      "                                             'by human memory systems, '\n",
      "                                             'incorporating mechanisms for '\n",
      "                                             'surprise-based learning, '\n",
      "                                             'momentum, and adaptive '\n",
      "                                             \"forgetting. It evaluates Titans' \"\n",
      "                                             'performance across diverse '\n",
      "                                             'tasks, including language '\n",
      "                                             'modeling, reasoning, and time '\n",
      "                                             'series forecasting, using '\n",
      "                                             'benchmarks like RULER and '\n",
      "                                             'BABILong.',\n",
      "                              'processed_timestamp': '2024-01-21T10:00:00.000Z',\n",
      "                              'publication_date': '2024-12-31',\n",
      "                              'summary': 'The paper introduces Titans, a novel '\n",
      "                                         'family of architectures that '\n",
      "                                         'integrate a neural long-term memory '\n",
      "                                         'module with attention mechanisms to '\n",
      "                                         'address the limitations of '\n",
      "                                         'Transformers in handling long '\n",
      "                                         'contexts. The proposed models '\n",
      "                                         'demonstrate superior performance in '\n",
      "                                         'language modeling, reasoning, and '\n",
      "                                         'long-context tasks, scaling '\n",
      "                                         'effectively to over 2M tokens.',\n",
      "                              'title': 'Titans: Learning to Memorize at Test '\n",
      "                                       'Time'},\n",
      "           'pdf_text': 'Titans: Learning to Memorize at Test Time\\n'\n",
      "                       'Ali Behrouz\\n'\n",
      "                       '†\\n'\n",
      "                       ', Peilin Zhong\\n'\n",
      "                       '†\\n'\n",
      "                       ', and Vahab Mirrokni\\n'\n",
      "                       '†\\n'\n",
      "                       '†\\n'\n",
      "                       'Google Research\\n'\n",
      "                       '{alibehrouz, peilinz, mirrokni}@google.com\\n'\n",
      "                       'Abstract\\n'\n",
      "                       'Over more than a decade there has been an extensive '\n",
      "                       'research effort of how effectively utilize recurrent '\n",
      "                       'models and\\n'\n",
      "                       'attentions. While recurrent models aim to compress the '\n",
      "                       'data into a fixed-size memory (called hidden state), '\n",
      "                       'attention allows\\n'\n",
      "                       'attending to the entire context window, capturing the '\n",
      "                       'direct dependencies of all tokens. This more accurate '\n",
      "                       'modeling\\n'\n",
      "                       'of dependencies, however, comes with a quadratic cost, '\n",
      "                       'limiting the model to a fixed-length context. We '\n",
      "                       'present a new\\n'\n",
      "                       'neural long-term memory module that learns to memorize '\n",
      "                       'historical context and helps an attention to attend to '\n",
      "                       'the\\n'\n",
      "                       'current context while utilizing long past information. '\n",
      "                       'We show that this neural memory has the advantage of a '\n",
      "                       'fast\\n'\n",
      "                       'parallelizable training while maintaining a fast '\n",
      "                       'inference. From a memory perspective, we argue that '\n",
      "                       'attention due to its\\n'\n",
      "                       'limited context but accurate dependency modeling '\n",
      "                       'performs as a short-term memory, while neural memory '\n",
      "                       'due to its\\n'\n",
      "                       'ability to memorize the data, acts as a long-term, '\n",
      "                       'more persistent, memory. Based on these two modules, '\n",
      "                       'we introduce\\n'\n",
      "                       'a new family of architectures, called Titans, and '\n",
      "                       'present three variants to address how one can '\n",
      "                       'effectively incorporate\\n'\n",
      "                       'memory into this architecture. Our experimental '\n",
      "                       'results on language modeling, common-sense reasoning, '\n",
      "                       'genomics,\\n'\n",
      "                       'and time series tasks show that Titans are more '\n",
      "                       'effective than Transformers and recent modern linear '\n",
      "                       'recurrent models.\\n'\n",
      "                       'They further can effectively scale to larger than 2M '\n",
      "                       'context window size with higher accuracy in '\n",
      "                       'needle-in-haystack tasks\\n'\n",
      "                       'compared to baselines.\\n'\n",
      "                       '1 Introduction\\n'\n",
      "                       '“The true art of memory is the art of attention!\"\\n'\n",
      "                       '— Samuel Johnson, 1787\\n'\n",
      "                       'T\\n'\n",
      "                       'ransformers, pure attention-based architectures '\n",
      "                       '(Vaswani et al. 2017), have been firmly established as '\n",
      "                       'state-of-\\n'\n",
      "                       'the-art models in sequence modeling, mainly due to '\n",
      "                       'their in-context learning and ability to learn at '\n",
      "                       'scale (Kaplan\\n'\n",
      "                       'et al. 2020). The primary building blocks of '\n",
      "                       'Transformers–attention modules—function as associative '\n",
      "                       'memory\\n'\n",
      "                       'blocks (Bietti et al. 2024), where they learn to store '\n",
      "                       'key-value associations and retrieve them by computing '\n",
      "                       'pairwise\\n'\n",
      "                       'similarity between queries (i.e., search signals) and '\n",
      "                       'keys (i.e., contexts). Accordingly, by design, the '\n",
      "                       'output of a Transformer\\n'\n",
      "                       'is exclusively conditioned on the direct dependencies '\n",
      "                       'of tokens in the current context window. This accurate '\n",
      "                       'modeling of\\n'\n",
      "                       'dependencies, however, comes with quadratic time and '\n",
      "                       'memory complexity in terms of the context length. In '\n",
      "                       'complex\\n'\n",
      "                       'real-world tasks (e.g., language modeling (N. F. Liu '\n",
      "                       'et al. 2024), video understanding (C.-Y. Wu et al. '\n",
      "                       '2019), long-term time\\n'\n",
      "                       'series forecasting (H. Zhou et al. 2021)), the context '\n",
      "                       'window can become extremely large, making the '\n",
      "                       'applicability of\\n'\n",
      "                       'Transformers challenging in these downstream tasks.\\n'\n",
      "                       'To overcome the scalability issue of Transformers, '\n",
      "                       'recent studies aim to design different variants of '\n",
      "                       'linear Transform-\\n'\n",
      "                       'ers (Kacham, Mirrokni, and P. Zhong 2024; '\n",
      "                       'Katharopoulos et al. 2020; S. Yang, B. Wang, Shen, et '\n",
      "                       'al. 2024), where softmax is\\n'\n",
      "                       'replaced by a kernel function in the attention (see '\n",
      "                       '§2.1 for details), resulting in a significant drop in '\n",
      "                       'memory consumption.\\n'\n",
      "                       'Despite efficiency and the ability to scale to longer '\n",
      "                       'context, linear Transformers do not show competitive '\n",
      "                       'performance\\n'\n",
      "                       'compared to Transformers as the kernel trick makes the '\n",
      "                       'model a linear recurrent network, in which the data is '\n",
      "                       'compressed\\n'\n",
      "                       'into a matrix-valued states (Katharopoulos et al. '\n",
      "                       '2020). This, however, brings a contradictory fact '\n",
      "                       'about linear recurrent (or\\n'\n",
      "                       'linear Transformers) models: On one hand, we use these '\n",
      "                       'linear models to enhance scalability and efficiency '\n",
      "                       '(linear vs.\\n'\n",
      "                       'quadratic complexity), whose advantages is appeared '\n",
      "                       'for very long context; On the other hand, a very long '\n",
      "                       'context cannot\\n'\n",
      "                       'be properly compressed in a small vector-valued or '\n",
      "                       'matrix-valued states (S. Wang 2024).\\n'\n",
      "                       '1\\n'\n",
      "                       'arXiv:2501.00663v1  [cs.LG]  31 Dec 2024\\n'\n",
      "                       'Furthermore, beyond efficiency, most existing '\n",
      "                       'architectures–ranging from Hopfield Networks (Hopfield '\n",
      "                       '1982) to LSTMs (Jür-\\n'\n",
      "                       'gen Schmidhuber and Hochreiter 1997) and Transformers '\n",
      "                       '(Vaswani et al. 2017)–face challenges when dealing '\n",
      "                       'with general-\\n'\n",
      "                       'ization, length extrapolation, and/or reasoning (Anil '\n",
      "                       'et al. 2022; Qin, Y. Zhong, and Deng 2024), all of '\n",
      "                       'which are inseparable\\n'\n",
      "                       'parts of many hard real-world tasks. Although these '\n",
      "                       'architectures draw inspiration from the human brain, '\n",
      "                       'each of which\\n'\n",
      "                       'are missing: (1) a crucial component for learning '\n",
      "                       'process—such as short-term memory, long-term memory, '\n",
      "                       'meta-memory,\\n'\n",
      "                       'attending to current context, etc. (Cowan 2008); (2) '\n",
      "                       'how these components are interconnected systems that '\n",
      "                       'can operate\\n'\n",
      "                       'independently; and/or (3) the ability to actively '\n",
      "                       'learn from data and memorize the abstraction of past '\n",
      "                       'history. We argue\\n'\n",
      "                       'that in an effective learning paradigm, similar to '\n",
      "                       'human brain, there aredistinct yet interconnected '\n",
      "                       'modules, each of which\\n'\n",
      "                       'is responsible for a component crucial to the learning '\n",
      "                       'process.\\n'\n",
      "                       'Memory Perspective\\n'\n",
      "                       'Memory is a fundamental mental process and is an '\n",
      "                       'inseparable component of human learning (Terry 2017). '\n",
      "                       'Without\\n'\n",
      "                       'a properly functioning memory system, humans and '\n",
      "                       'animals would be restricted to basic reflexes and '\n",
      "                       'stereotyped\\n'\n",
      "                       'behaviors. Accordingly, memory has been the '\n",
      "                       'inspiration for many seminal research in machine '\n",
      "                       'learning literature; e.g.,\\n'\n",
      "                       'Hopfield Networks (Hopfield 1982), LSTMs (Jürgen '\n",
      "                       'Schmidhuber and Hochreiter 1997), and Transformers '\n",
      "                       '(Vaswani et al.\\n'\n",
      "                       '2017).\\n'\n",
      "                       'Taking inspiration from the common definitions of '\n",
      "                       'memory and learning in neuropsychology literature '\n",
      "                       '(Okano, Hirano,\\n'\n",
      "                       'and Balaban 2000), most existing architectures '\n",
      "                       'consider memory as a neural update caused by an input, '\n",
      "                       'and define learning\\n'\n",
      "                       'as a process for acquiring effective and useful '\n",
      "                       'memory, given an objective. In this perspective, '\n",
      "                       'Recurrent Neural Networks\\n'\n",
      "                       '(RNNs) (Williams and Zipser 1989) can be defined as '\n",
      "                       'models with a vector-valued memory module M(also '\n",
      "                       'called hidden\\n'\n",
      "                       'state) with two main steps: Given a new input 𝑥𝑡 at '\n",
      "                       'time 𝑡, the model (1) updates the memory using a '\n",
      "                       'function 𝑓(M𝑡−1,𝑥𝑡)\\n'\n",
      "                       '(with compression); and (2) retrieves the '\n",
      "                       'corresponding memory of input using a function '\n",
      "                       '𝑔(M𝑡,𝑥𝑡)(see §2.1 for details).\\n'\n",
      "                       'Similarly, Transformers can be seen as architectures '\n",
      "                       'with a growing memory and two similar steps. That is, '\n",
      "                       'the pair of key\\n'\n",
      "                       'and value matrices acts as the model’s memory, and the '\n",
      "                       'model: (1) updates the memory by appending the key and '\n",
      "                       'value to\\n'\n",
      "                       'the memory (without compression), and (2) retrieves '\n",
      "                       'query vectors’ corresponding memory by finding the '\n",
      "                       'similarity of\\n'\n",
      "                       'query and key vectors, which is then used to weight '\n",
      "                       'the value vectors for the output.\\n'\n",
      "                       'This perspective, can help us better understand '\n",
      "                       'existing paradigms, their critical differences, and '\n",
      "                       'design more effective\\n'\n",
      "                       'architectures. For example, the main difference '\n",
      "                       'between Transformers (Vaswani et al. 2017) and linear '\n",
      "                       'Transform-\\n'\n",
      "                       'ers (Katharopoulos et al. 2020) is the memory '\n",
      "                       'structure as well as the memory updating step, in '\n",
      "                       'which linear Transformers\\n'\n",
      "                       'compress the historical data into a fixed-size '\n",
      "                       'matrix-valued memory while Transformers keep all '\n",
      "                       'historical data (within\\n'\n",
      "                       'the context length) without any compression. While '\n",
      "                       'both linear Transformers and linear RNNs (including '\n",
      "                       'state space\\n'\n",
      "                       'models) compress the information in memory update '\n",
      "                       'step, the critical difference lies in the structure of '\n",
      "                       'the memory,\\n'\n",
      "                       'where linear RNNs (vs. linear Transformers) use a '\n",
      "                       'vector-valued memory (vs. matrix-valued memory). '\n",
      "                       'Therefore, this\\n'\n",
      "                       'perspective motivates us to ask: (Q1) What constitute '\n",
      "                       'a good structure for the memory? (Q2) What is a proper '\n",
      "                       'memory\\n'\n",
      "                       'update mechanism? and (Q3) What is a good memory '\n",
      "                       'retrieval process?\\n'\n",
      "                       'Revisiting our understanding of human memory, it is '\n",
      "                       'neither a unitary process nor it serves a single '\n",
      "                       'function (Cowan\\n'\n",
      "                       '2008). In fact, memory is a confederation of '\n",
      "                       'systems–e.g., short-term, working, and long-term '\n",
      "                       'memory–each serving a\\n'\n",
      "                       'different function with different neural structures, '\n",
      "                       'and each capable of operating independently '\n",
      "                       '(Willingham 1997). This\\n'\n",
      "                       'fact motivates us to ask: (Q4) How to design an '\n",
      "                       'efficient architecture that incorporates different '\n",
      "                       'interconnected memory\\n'\n",
      "                       'modules. Finally, storing a memory is a neural process '\n",
      "                       'that requires to encode and store the abstraction of '\n",
      "                       'the past. It can\\n'\n",
      "                       'be over-simplification to assume a single vector or a '\n",
      "                       'matrix, whose parameters are encoding the data in a '\n",
      "                       'linear manner,\\n'\n",
      "                       'are enough for storing long-term history. (Q5) Is a '\n",
      "                       'deep memory module needed to effectively '\n",
      "                       'store/remember long\\n'\n",
      "                       'past?\\n'\n",
      "                       'Contributions and Roadmap\\n'\n",
      "                       'In this paper, we aim to answer the above five '\n",
      "                       'questions by designing a long-term neural memory '\n",
      "                       'module, that can\\n'\n",
      "                       'efficiently and effectively learn to memorize at test '\n",
      "                       'time. Building upon its design, we discuss how it can '\n",
      "                       'be incorporated\\n'\n",
      "                       'into an architecture.\\n'\n",
      "                       'Neural Memory (§3). We present a (deep) neural '\n",
      "                       'long-term memory that (as a meta in-context model) '\n",
      "                       'learns how to\\n'\n",
      "                       'memorize/store the data into its parameters at test '\n",
      "                       'time. Inspired by human long-term memory system '\n",
      "                       '(Mandler 2014),\\n'\n",
      "                       '2\\n'\n",
      "                       'we design this memory module so an event that violates '\n",
      "                       'the expectations (being surprising) is more memorable. '\n",
      "                       'To this\\n'\n",
      "                       'end, we measure the surprise of an input with the '\n",
      "                       'gradient of the neural network with respect to the '\n",
      "                       'input in associative\\n'\n",
      "                       'memory loss (see §3.1 for details). To better handle '\n",
      "                       'the limited memory, we present a decaying mechanism '\n",
      "                       'that consider the\\n'\n",
      "                       'proportion of memory size and the amount of data '\n",
      "                       'surprise, resulting in better memory management. We '\n",
      "                       'show that this\\n'\n",
      "                       'decay mechanism is in fact the generalization of '\n",
      "                       'forgetting mechanism in modern recurrent models (Dao '\n",
      "                       'and Gu 2024; Gu\\n'\n",
      "                       'and Dao 2024; S. Yang, Kautz, and Hatamizadeh 2024). '\n",
      "                       'Interestingly, we find that this mechanism is '\n",
      "                       'equivalent to optimizing\\n'\n",
      "                       'a meta neural network with mini-batch gradient '\n",
      "                       'descent, momentum, and weight decay. Building upon '\n",
      "                       'tensorizing\\n'\n",
      "                       'mini-batch gradient descent to use more matmul '\n",
      "                       'operations (Yu Sun et al. 2024), we present a fast and '\n",
      "                       'parallelizable\\n'\n",
      "                       'algorithm to train our deep neural long-term memory.\\n'\n",
      "                       'Titans Architectures (§4). After designing the '\n",
      "                       'long-term neural memory, an important remaining '\n",
      "                       'question is how to\\n'\n",
      "                       'effectively and efficiently incorporate memory into a '\n",
      "                       'deep learning architecture. We present Titans, a '\n",
      "                       'family of deep models\\n'\n",
      "                       'that consists of three hyper-heads: (1) Core: this '\n",
      "                       'module consists of the short-term memory, and is '\n",
      "                       'responsible for the main\\n'\n",
      "                       'flow of processing the data (we use attention with '\n",
      "                       'limited window size); (2) Long-term Memory: this '\n",
      "                       'branch is our neural\\n'\n",
      "                       'long-term memory module that is responsible to '\n",
      "                       'store/remember long past; (3) Persistent Memory: this '\n",
      "                       'is a set of learnable\\n'\n",
      "                       'but date-independent parameters that encodes the '\n",
      "                       'knowledge about a task. Finally, as a proof of '\n",
      "                       'concept, we present three\\n'\n",
      "                       'variants of Titans, in which we incorporate memory as: '\n",
      "                       '(i) a context, (ii) a layer, and (iii) a gated '\n",
      "                       'branch.\\n'\n",
      "                       'Experimental Results (§5). We perform experimental '\n",
      "                       'evaluations on language modeling, commonsense '\n",
      "                       'reasoning, recall-\\n'\n",
      "                       'intensive, needle in haystack, time series '\n",
      "                       'forecasting, and DNA modeling tasks. We observe that '\n",
      "                       'our Titan architecture\\n'\n",
      "                       'outperforms all modern recurrent models as well as '\n",
      "                       'their hybrid variants (combining with sliding-window '\n",
      "                       'attention) across\\n'\n",
      "                       'a comprehensive set of benchmarks. Furthermore, Titans '\n",
      "                       'outperforms Transformers with the same context window, '\n",
      "                       'and\\n'\n",
      "                       'show competitive performance with Transformers that '\n",
      "                       'use the entire context. This results are achieved '\n",
      "                       'while, contrary to\\n'\n",
      "                       'Transformers, Titans scale to larger than 2M context '\n",
      "                       'window size.\\n'\n",
      "                       '2 Preliminaries\\n'\n",
      "                       'I\\n'\n",
      "                       'n this section, we discuss the notation and some '\n",
      "                       'background concepts that we use though the paper. We '\n",
      "                       'let\\n'\n",
      "                       '𝑥 ∈R𝑁×𝑑in be the input, Mbe a neural network (neural '\n",
      "                       'memory module), Q,K,V be the query, key and value\\n'\n",
      "                       'of the attention mechanism, and M be the attention '\n",
      "                       'mask. When segmenting the sequence, we use S(𝑖)to '\n",
      "                       'refer to\\n'\n",
      "                       'the 𝑖-th segment. Through the paper, we abuse the '\n",
      "                       'notation and use subscripts to refer to a specific '\n",
      "                       'element of a matrix,\\n'\n",
      "                       'vector, or segments. For example, we let S(𝑖)\\n'\n",
      "                       '𝑗 be the 𝑗-th token in the 𝑖-th segment. The only '\n",
      "                       'exception is subscripts with 𝑡,\\n'\n",
      "                       'which we reserved to index recurrence over time, or '\n",
      "                       'the state of a neural network at time𝑡. Given a neural '\n",
      "                       'network Nand\\n'\n",
      "                       'a data sample 𝑥, we use N(𝑥)(resp. N∗(𝑥)) to refer to '\n",
      "                       'the forward pass with (resp. without) weight '\n",
      "                       'adjustment. Also, we\\n'\n",
      "                       'abuse the notation and use N(𝑘)to refer to the 𝑘-th '\n",
      "                       'layer of the neural network. In the following, we '\n",
      "                       'first, discuss the\\n'\n",
      "                       'backgrounds for attention and its efficient variants '\n",
      "                       'followed by a review of modern linear RNNs. Finally, '\n",
      "                       'we discuss a\\n'\n",
      "                       'memory perspective of these architectures that '\n",
      "                       'motivates us to design Titans.\\n'\n",
      "                       '2.1 Backgrounds\\n'\n",
      "                       'Attention. Transformers (Vaswani et al. 2017) as the '\n",
      "                       'de facto backbone for many deep learning models are '\n",
      "                       'based on\\n'\n",
      "                       'attention mechanism. Given input 𝑥 ∈R𝑁×𝑑in , causal '\n",
      "                       'attention computes output y ∈R𝑁×𝑑in based on softmax '\n",
      "                       'over input\\n'\n",
      "                       'dependent key, value, and query matrices:\\n'\n",
      "                       'Q = 𝑥WQ, K = 𝑥WK, V = 𝑥WV, (1)\\n'\n",
      "                       'y𝑖 =\\n'\n",
      "                       '𝑖∑︁\\n'\n",
      "                       '𝑗=1\\n'\n",
      "                       'exp\\n'\n",
      "                       '\\x10\\n'\n",
      "                       'Q⊤\\n'\n",
      "                       '𝑖 K𝑗/√𝑑in\\n'\n",
      "                       '\\x11\\n'\n",
      "                       'V𝑗\\n'\n",
      "                       'Í𝑖\\n'\n",
      "                       'ℓ=1 exp\\n'\n",
      "                       '\\x10\\n'\n",
      "                       'Q⊤\\n'\n",
      "                       '𝑖 Kℓ/√𝑑in\\n'\n",
      "                       '\\x11, (2)\\n'\n",
      "                       'where WQ,WK,and WV ∈R𝑑in ×𝑑in are learnable '\n",
      "                       'parameters. Despite the power and effectiveness in '\n",
      "                       'recall, transformers\\n'\n",
      "                       'need at least 𝑁 ×𝑑 operators to calculate the output, '\n",
      "                       'resulting in larger memory consumption and '\n",
      "                       'lower-throughput for\\n'\n",
      "                       'longer sequences.\\n'\n",
      "                       'Efficient Attentions. To improve the memory '\n",
      "                       'consumption and throughput of softmax attention for '\n",
      "                       'longer sequences,\\n'\n",
      "                       'various studies focused on I/O aware implementations '\n",
      "                       'of attention (Dao 2024; Dao, D. Fu, et al. 2022), '\n",
      "                       'designing more\\n'\n",
      "                       '3\\n'\n",
      "                       'efficient attention mechanisms by sparsifying the '\n",
      "                       'attention matrix (B. Chen et al. 2021; Choromanski et '\n",
      "                       'al. 2021; Dai et al.\\n'\n",
      "                       '2019), approximating the softmax (Arora et al. 2024), '\n",
      "                       'or developing kernel-based (linear) attentions '\n",
      "                       '(Aksenov et al. 2024;\\n'\n",
      "                       'Kacham, Mirrokni, and P. Zhong 2024; Schlag, Irie, and '\n",
      "                       'Jürgen Schmidhuber 2021; S. Yang, B. Wang, Shen, et '\n",
      "                       'al. 2024). In\\n'\n",
      "                       'this part, we focus on the later, i.e., linear '\n",
      "                       'attentions, where the softmax in standard attention is '\n",
      "                       'replaced with an alternative\\n'\n",
      "                       'kernel function 𝜙(.,.), such that 𝜙(𝑥,𝑦)= 𝜙(𝑥)𝜙(𝑦). '\n",
      "                       'Accordingly, the attention can be written as:\\n'\n",
      "                       'y𝑖 =\\n'\n",
      "                       '𝑖∑︁\\n'\n",
      "                       '𝑗=1\\n'\n",
      "                       '𝜙(𝑄⊤\\n'\n",
      "                       '𝑖 𝐾𝑗)\\n'\n",
      "                       'Í𝑖\\n'\n",
      "                       'ℓ=1 𝜙(𝑄⊤\\n'\n",
      "                       '𝑖 𝐾ℓ)\\n'\n",
      "                       '𝑉𝑗 =\\n'\n",
      "                       '𝑖∑︁\\n'\n",
      "                       '𝑗=1\\n'\n",
      "                       '𝜙(𝑄𝑖)⊤𝜙(𝐾𝑗)\\n'\n",
      "                       'Í𝑖\\n'\n",
      "                       'ℓ=1 𝜙(𝑄𝑖)⊤𝜙(𝐾ℓ)\\n'\n",
      "                       '𝑉𝑗 =\\n'\n",
      "                       '𝜙(𝑄𝑖)⊤Í𝑖\\n'\n",
      "                       '𝑗=1 𝜙(𝐾𝑗)𝑉𝑗\\n'\n",
      "                       '𝜙(𝑄𝑖)⊤Í𝑖\\n'\n",
      "                       'ℓ=1 𝜙(𝐾ℓ)\\n'\n",
      "                       ', (3)\\n'\n",
      "                       'resulting in a higher-throughput as terms Í𝑖\\n'\n",
      "                       '𝑗=1 𝜙(𝐾𝑗)and Í𝑖\\n'\n",
      "                       'ℓ=1 𝜙(𝐾ℓ)are re-using in each step. When choosing the '\n",
      "                       'kernel\\n'\n",
      "                       'as identity matrix (Yutao Sun et al. 2023), the above '\n",
      "                       'formulation can also be written in a recurrent '\n",
      "                       'format:\\n'\n",
      "                       'M𝑡 = M𝑡−1 +𝐾⊤\\n'\n",
      "                       '𝑡 𝑉𝑡 , (4)\\n'\n",
      "                       'y𝑡 = 𝑄𝑡M𝑡 , (5)\\n'\n",
      "                       'which allows efficient inference for linear '\n",
      "                       'attentions.\\n'\n",
      "                       'Modern Linear Models and Their Memory Perspective. As '\n",
      "                       'discussed earlier, one can define learning as a '\n",
      "                       'process for\\n'\n",
      "                       'acquiring effective and useful memory. Building upon '\n",
      "                       'this, one can see the hidden state of Recurrent Neural '\n",
      "                       'Networks\\n'\n",
      "                       '(RNNs) as a memory unit, which the model aims to '\n",
      "                       'compress the information into. Accordingly, in a '\n",
      "                       'general form of\\n'\n",
      "                       'recurrent neural network, the hidden state can be '\n",
      "                       'treated as a memory unit and the recurrence process '\n",
      "                       'can be split into the\\n'\n",
      "                       'read and write operations in the memory unit. That is, '\n",
      "                       'we let 𝑥 ∈R𝑁×𝑑in be the input, M∈ R𝑑 is the memory '\n",
      "                       'unit, and\\n'\n",
      "                       'y ∈R𝑑in is the output, then the general form of the '\n",
      "                       'recurrent neural network is defined as:\\n'\n",
      "                       'M𝑡 = 𝑓(M𝑡−1,𝑥𝑡), Write Operation (6)\\n'\n",
      "                       'y𝑡 = 𝑔(M𝑡,𝑥𝑡), Read Operation (7)\\n'\n",
      "                       'where 𝑓(.,.)is the read and 𝑔(.,.)is the write '\n",
      "                       'corresponding functions. Note that here the subscript '\n",
      "                       'of M𝑡 shows the state\\n'\n",
      "                       'of the memory at time 𝑡.\\n'\n",
      "                       'In this perspective, the recurrence formula of linear '\n",
      "                       'Transformers (see Equation 4) is equivalent to '\n",
      "                       'additively compress\\n'\n",
      "                       'and write keys and values, (𝐾𝑡,𝑉𝑡), into a '\n",
      "                       'matrix-valued memory unit M𝑡. Therefore, when dealing '\n",
      "                       'with long context\\n'\n",
      "                       'data, this additive nature of the process results in '\n",
      "                       'memory overflow, significantly damaging the '\n",
      "                       'performance of the model.\\n'\n",
      "                       'To address this, studies have focused on two promising '\n",
      "                       'directions: (1) Adding forget mechanism: several '\n",
      "                       'studies have\\n'\n",
      "                       'presented adaptive (data-dependent) forgetting gate '\n",
      "                       'mechanisms for linear models, where it can erase the '\n",
      "                       'memory when it\\n'\n",
      "                       'is needed. As examples of such models, we refer to GLA '\n",
      "                       '(S. Yang, B. Wang, Shen, et al. 2024), LRU (Orvieto et '\n",
      "                       'al. 2023),\\n'\n",
      "                       'Griffin (De et al. 2024), xLSTM (Beck et al. 2024), '\n",
      "                       'and Mamba2 (Dao and Gu 2024), which the later is also '\n",
      "                       'connected to the\\n'\n",
      "                       'discretized version of traditional state space models '\n",
      "                       '(Gu and Dao 2024).(2) Improving the write operation: '\n",
      "                       'To overcome the\\n'\n",
      "                       'additive nature of memory write operation in '\n",
      "                       'traditional recurrent models, Widrow and Hoff (1988) '\n",
      "                       'presented Delta Rule,\\n'\n",
      "                       'in which before adding a memory (i.e., a pair of key '\n",
      "                       'and value), the model first removes its past value. To '\n",
      "                       'enhance the\\n'\n",
      "                       'parallelizable training and scaling, S. Yang, B. Wang, '\n",
      "                       'Yu Zhang, et al. (2024) present a fast paralellizable '\n",
      "                       'algorithm. Finally,\\n'\n",
      "                       'very recently, S. Yang, Kautz, and Hatamizadeh (2024) '\n",
      "                       'improved the DeltaNets by adding a forget gate.\\n'\n",
      "                       'Memory Modules. Memory has always been one of the core '\n",
      "                       'parts of the neural network designs (Graves, Wayne,\\n'\n",
      "                       'and Danihelka 2014; JH Schmidhuber 1992; Jürgen '\n",
      "                       'Schmidhuber and Hochreiter 1997; J. Zhang et al. '\n",
      "                       '2024). The idea of\\n'\n",
      "                       'seeing linear layers as the key-value (associative) '\n",
      "                       'memory system backs to fast weight programs, in which '\n",
      "                       'dynamic fast\\n'\n",
      "                       'programs are incorporated into recurrent neural '\n",
      "                       'networks to serve as writable memory (JH Schmidhuber '\n",
      "                       '1992). The two\\n'\n",
      "                       'learning rules of Hebbian (Hebb 2005) and delta '\n",
      "                       '(Prados and Kak 1989) are the most popular learning '\n",
      "                       'rules for fast weight\\n'\n",
      "                       'programs, which have been extensively explored in '\n",
      "                       'various studies (Irie, Schlag, et al. 2021; '\n",
      "                       'Munkhdalai, Sordoni, et al.\\n'\n",
      "                       '2019; Munkhdalai and H. Yu 2017; Schlag, Irie, and '\n",
      "                       'Jürgen Schmidhuber 2021; JH Schmidhuber 1992; S. Yang, '\n",
      "                       'Kautz, and\\n'\n",
      "                       'Hatamizadeh 2024; S. Yang, B. Wang, Yu Zhang, et al. '\n",
      "                       '2024). All these models, however, are based on '\n",
      "                       'momentary surprise,\\n'\n",
      "                       'missing the token flow in the sequences (see Section '\n",
      "                       '3.1), and most of them lacks a forgetting gate, '\n",
      "                       'resulting in a poor\\n'\n",
      "                       'memory management.\\n'\n",
      "                       'We further discuss the connection of our architectures '\n",
      "                       'with recent models in Appendix C. Additional related '\n",
      "                       'work are\\n'\n",
      "                       'discussed in Appendix A.\\n'\n",
      "                       '4\\n'\n",
      "                       '3 Learning to Memorize at Test Time\\n'\n",
      "                       'T\\n'\n",
      "                       'o overcome the lack of long-term memory and to enable '\n",
      "                       'the model to learn, forget, and retrieve information, '\n",
      "                       'in\\n'\n",
      "                       'this section, we present a neural long-term memory '\n",
      "                       'module, which is a meta models that learns to memorize '\n",
      "                       'at\\n'\n",
      "                       'test time. In Section 3.1, we first discuss the '\n",
      "                       'motivation and the design of the neural memory. In '\n",
      "                       'Section 3.2, we\\n'\n",
      "                       'discuss how our architecture design can benefit from a '\n",
      "                       'fast and parallelizable training. Finally, in Section '\n",
      "                       '3.3, we augment\\n'\n",
      "                       'our architecture using persistent memory module, in '\n",
      "                       'which we use learnable but data-independent parameters '\n",
      "                       'to learn\\n'\n",
      "                       'meta information about the task.\\n'\n",
      "                       '3.1 Long-term Memory\\n'\n",
      "                       'To design a neural long-term memory module, we need a '\n",
      "                       'model that can encode the abstraction of the past '\n",
      "                       'history into its\\n'\n",
      "                       'parameters. An example of this can be LLMs that are '\n",
      "                       'shown to be memorizing their training data (Leybzon '\n",
      "                       'and Kervadec\\n'\n",
      "                       '2024; Schwarzschild et al. 2024; Staab et al. 2024). '\n",
      "                       'Therefore, a simple idea is to train a neural network '\n",
      "                       'and expect it to\\n'\n",
      "                       'memorize its training data. Memorization, however, has '\n",
      "                       'almost always been known as an undesirable phenomena '\n",
      "                       'in\\n'\n",
      "                       'neural networks as it limits the model generalization '\n",
      "                       '(Bayat et al. 2024), causes privacy concerns (Staab et '\n",
      "                       'al. 2024), and\\n'\n",
      "                       'so results in poor performance at test time. Moreover, '\n",
      "                       'the memorization of the training data might not be '\n",
      "                       'helpful at test\\n'\n",
      "                       'time, in which the data might be out-of-distribution. '\n",
      "                       'We argue that, we need an online meta-model that '\n",
      "                       'learns how to\\n'\n",
      "                       'memorize/forget the data at test time. In this setup, '\n",
      "                       'the model is learning a function that is capable of '\n",
      "                       'memorization, but it\\n'\n",
      "                       'is not overfitting to the training data, resulting in '\n",
      "                       'a better generalization at test time.\\n'\n",
      "                       'Learning Process and Surprise Metric. The key idea to '\n",
      "                       'train a long-term memory is to treat its training as '\n",
      "                       'an online\\n'\n",
      "                       'learning problem, in which we aim to compress the past '\n",
      "                       'information 𝑥1,...,𝑥 𝑡−1 into the parameters of our '\n",
      "                       'long-term\\n'\n",
      "                       'neural memory module M𝑡. As discussed earlier, an '\n",
      "                       'event that violates the expectations (i.e., is '\n",
      "                       'surprising) is more\\n'\n",
      "                       'memorable for humans (Mandler 2014). Inspired by this, '\n",
      "                       'a simple definition of surprise for a model can be its '\n",
      "                       'gradient with\\n'\n",
      "                       'respect to the input. The larger the gradient is, the '\n",
      "                       'more different the input data is from the past data. '\n",
      "                       'Accordingly, using\\n'\n",
      "                       'this surprise score, we can update the memory as:\\n'\n",
      "                       'M𝑡 = M𝑡−1 −𝜃𝑡 ∇ℓ(M𝑡−1; 𝑥𝑡)|           {z           }\\n'\n",
      "                       'Surprise\\n'\n",
      "                       '. (8)\\n'\n",
      "                       'This surprise metric, however, can result in missing '\n",
      "                       'important information that comes after a big '\n",
      "                       'surprising moment.\\n'\n",
      "                       'That is, the gradient can become extremely small after '\n",
      "                       'several surprising steps, leading to stocking in a '\n",
      "                       'flat area (i.e., local\\n'\n",
      "                       'minima), and missing information about some parts of '\n",
      "                       'the sequence. From the human memory perspective, an '\n",
      "                       'event might\\n'\n",
      "                       'not consistently surprise us through a long-period of '\n",
      "                       'time although it is memorable. The reason is that the '\n",
      "                       'initial moment\\n'\n",
      "                       'is surprising enough to get our attention through a '\n",
      "                       'long time frame, leading to memorizing the entire time '\n",
      "                       'frame. To\\n'\n",
      "                       'improve the above surprise metric (Equation 8), we '\n",
      "                       'break the surprise metric into (1) past surprise , '\n",
      "                       'which measures the\\n'\n",
      "                       'surprise amount of a very recent past; and (2) '\n",
      "                       'momentary surprise , which measures the surprise of '\n",
      "                       'incoming data:\\n'\n",
      "                       'M𝑡 = M𝑡−1 +𝑆𝑡, (9)\\n'\n",
      "                       '𝑆𝑡 = 𝜂𝑡 𝑆𝑡−1\\n'\n",
      "                       '|{z}\\n'\n",
      "                       'Past Surprise\\n'\n",
      "                       '−𝜃𝑡 ∇ℓ(𝑀𝑡−1; 𝑥𝑡)|          {z          }\\n'\n",
      "                       'Momentary Surprise\\n'\n",
      "                       '. (10)\\n'\n",
      "                       'Interestingly, this formulation is similar to gradient '\n",
      "                       'descent with momentum, where𝑆𝑡 is the momentum '\n",
      "                       'element. Therefore,\\n'\n",
      "                       'the momentum here act as a memory of surprise across '\n",
      "                       'time (sequence length). In this formulation, the term '\n",
      "                       '𝜂𝑡 is a\\n'\n",
      "                       'data-dependent surprise decay (a function of 𝑥𝑡), '\n",
      "                       'controlling how surprise decays over time, and the '\n",
      "                       'term 𝜃𝑡 is controlling\\n'\n",
      "                       'how much of momentary surprise should be incorporated '\n",
      "                       'into the final surprise metric in a data-dependent '\n",
      "                       'manner. This\\n'\n",
      "                       'data-dependency is particularly important in this '\n",
      "                       'design: While surprise of previous tokens might be '\n",
      "                       'needed to affect\\n'\n",
      "                       'the surprise of the next token, it is mostly valid if '\n",
      "                       'all tokens are relevant and are in the same context. '\n",
      "                       'Accordingly, a\\n'\n",
      "                       'data-dependent 𝜂can control if memory needs to: (1) '\n",
      "                       'ignore the last surprise by setting 𝜂𝑡 →0 (possibly '\n",
      "                       'due to the change\\n'\n",
      "                       'of context), or (2) fully incorporate the last '\n",
      "                       'surprise by setting 𝜂𝑡 →1 (possibly as the token is '\n",
      "                       'highly relevant to its recent\\n'\n",
      "                       'past tokens).\\n'\n",
      "                       'Objective. Our above surprise metric is based on a '\n",
      "                       'loss function ℓ(.; .), which is the objective that our '\n",
      "                       'memory is learning\\n'\n",
      "                       'to act as it at test time. That is, our memory module '\n",
      "                       'is a meta model that learns a function based on the '\n",
      "                       'loss function ℓ(.; .).\\n'\n",
      "                       '5\\n'\n",
      "                       'In this work, we focus on associative memory , in '\n",
      "                       'which we aim to store the past data as the pairs of '\n",
      "                       'keys and values. Given\\n'\n",
      "                       '𝑥𝑡, similar to Transformers (Vaswani et al. 2017), we '\n",
      "                       'use two linear layers to project𝑥𝑡 into a key and '\n",
      "                       'value:\\n'\n",
      "                       'k𝑡 = 𝑥𝑡𝑊𝐾, v𝑡 = 𝑥𝑡𝑊𝑉, (11)\\n'\n",
      "                       'where 𝑊𝐾 and 𝑊𝑉 ∈R𝑑in ×𝑑in . Next, we expect our '\n",
      "                       'memory module to learn the associations between keys '\n",
      "                       'and values. To\\n'\n",
      "                       'this end, we define the loss as follows:\\n'\n",
      "                       'ℓ(M𝑡−1; 𝑥𝑡)= ∥M𝑡−1 (k𝑡)−v𝑡∥2\\n'\n",
      "                       '2 (12)\\n'\n",
      "                       'By optimizing the above loss function in the '\n",
      "                       'inner-loop of our meta model (memory), the model '\n",
      "                       'learns how to memorize\\n'\n",
      "                       'the mapping between keys and values at test time. Note '\n",
      "                       'that, similar to meta-learning models (Nichol 2018; '\n",
      "                       'Zintgraf et al.\\n'\n",
      "                       '2019), training of the memory is in the inner-loop, '\n",
      "                       'and so parameters 𝑊𝐾 and 𝑊𝑉 are hyperparameters in the '\n",
      "                       'above loss\\n'\n",
      "                       'function. Accordingly, in the inner loop, we optimize '\n",
      "                       'M’s weights, while in the outer-loop, we optimize '\n",
      "                       'other parameters\\n'\n",
      "                       'of the entire architecture.\\n'\n",
      "                       'Forgetting Mechanism. When dealing with very large '\n",
      "                       'sequences (e.g., millions of tokens), it is crucial to '\n",
      "                       'manage which\\n'\n",
      "                       'past information should be forgotten–even with a deep '\n",
      "                       'or a very large matrix-valued memory. To this end, we '\n",
      "                       'use an\\n'\n",
      "                       'adaptive forgetting mechanism that allows the memory '\n",
      "                       'to forget the information that is not needed anymore, '\n",
      "                       'resulting in\\n'\n",
      "                       'better managing the memory’s limited capacity. That '\n",
      "                       'is, given the next token 𝑥𝑡, we modify the update rule '\n",
      "                       'as:\\n'\n",
      "                       'M𝑡 = (1 −𝛼𝑡)M𝑡−1 +𝑆𝑡, (13)\\n'\n",
      "                       '𝑆𝑡 = 𝜂𝑡𝑆𝑡−1 −𝜃𝑡 ∇ℓ(𝑀𝑡−1; 𝑥𝑡), (14)\\n'\n",
      "                       'where 𝛼𝑡 ∈[0,1]is the gating mechanism that flexibly '\n",
      "                       'controls the memory; i.e., decides how much '\n",
      "                       'information should be\\n'\n",
      "                       'forgotten. For example, it can update the memory '\n",
      "                       'without affecting the past abstraction by letting 𝛼𝑡 '\n",
      "                       '→0, and can clear\\n'\n",
      "                       'the entire memory by letting 𝛼𝑡 →1. Later in this '\n",
      "                       'section, we show that this weight decay mechanism is '\n",
      "                       'closely related to\\n'\n",
      "                       'the gating mechanism in modern RNNs (Dao and Gu 2024; '\n",
      "                       'Orvieto et al. 2023).\\n'\n",
      "                       'Memory Architecture. In this paper, we focus on simple '\n",
      "                       'MLPs with 𝐿M ≥1 layers as the architecture of our '\n",
      "                       'long-term\\n'\n",
      "                       'memory. The main reason behind this choice is that we '\n",
      "                       'want to focus on better motivating the design of the '\n",
      "                       'long-term\\n'\n",
      "                       'memory and ways that it can be incorporated into an '\n",
      "                       'architecture. However, our formulation and '\n",
      "                       'architectural design\\n'\n",
      "                       'opens a new research direction to design neural '\n",
      "                       'architectures that are more effective and efficient in '\n",
      "                       'memorization of data.\\n'\n",
      "                       'Recently, there has been a promising line of work to '\n",
      "                       'design such architectures (Berges et al. 2024; Cetin '\n",
      "                       'et al. 2024; J. Zhang\\n'\n",
      "                       'et al. 2024), which incorporating them into our '\n",
      "                       'framework (i.e., replacing simple MLPs with such '\n",
      "                       'architectures) can be an\\n'\n",
      "                       'interesting future work.\\n'\n",
      "                       'When using vector-valued or matrix-valued memory (De '\n",
      "                       'et al. 2024; Orvieto et al. 2023; S. Yang, B. Wang, '\n",
      "                       'Shen, et\\n'\n",
      "                       'al. 2024), the memory module is compressing the past '\n",
      "                       'data and fit it into a line. That is, from the meta '\n",
      "                       'learning or\\n'\n",
      "                       'online learning perspective (Yu Sun et al. 2024), '\n",
      "                       'using a matrix-valued memory M = 𝑊 ∈R𝑑in ×𝑑in is '\n",
      "                       'equivalent to\\n'\n",
      "                       'optimize ℓ(𝑊𝑡−1; 𝑥𝑡)= ∥𝑊𝑡−1k𝑡 −v𝑡∥2\\n'\n",
      "                       '2, which is an online linear regression objective and '\n",
      "                       'so the optimal solution assumes\\n'\n",
      "                       'the underlying dependency of historical data is '\n",
      "                       'linear. On the other hand, we argue that deep memory '\n",
      "                       'modules (i.e.,\\n'\n",
      "                       '𝐿M ≥2) . Aligning with the theoretical results that '\n",
      "                       'MLPs with at least two layers are strictly more '\n",
      "                       'expressive than linear\\n'\n",
      "                       'models (Hornik, Stinchcombe, and White 1989), in '\n",
      "                       'Section 5.5, we show that deep memory modules are more '\n",
      "                       'effective in\\n'\n",
      "                       'practice.\\n'\n",
      "                       'Retrieving a Memory. In the above, we discuss how one '\n",
      "                       'can design and train a long-term memory module that '\n",
      "                       'learns to\\n'\n",
      "                       'memorize at test time. A key remaining question is: '\n",
      "                       'How one can retrieve information from the memory? We '\n",
      "                       'simply use the\\n'\n",
      "                       'forward pass without weight update (i.e., inference) '\n",
      "                       'to retrieve a memory correspond to a query. Formally, '\n",
      "                       'given an input\\n'\n",
      "                       '𝑥𝑡, we use a linear layer 𝑊𝑄 to project the input, '\n",
      "                       'i.e., q𝑡 = 𝑥𝑡𝑊𝑄 and retrieve the corresponding (or '\n",
      "                       'useful) information\\n'\n",
      "                       'from the memory 𝑦𝑡 by:\\n'\n",
      "                       '𝑦𝑡 = M∗(q𝑡). (15)\\n'\n",
      "                       '6\\n'\n",
      "                       'Figure 1: The illustration of how the training of '\n",
      "                       'neural memory can be done in parallel and using '\n",
      "                       'matmuls.\\n'\n",
      "                       '3.2 How to Parallelize the Long-term Memory Training\\n'\n",
      "                       'As discussed above, the design of our long-term memory '\n",
      "                       'module is equivalent to training a meta model by '\n",
      "                       'optimizing\\n'\n",
      "                       'associative memory loss function ℓ(M𝑡−1; 𝑥𝑡)= ∥M𝑡−1 '\n",
      "                       '(k𝑡)−v𝑡∥2\\n'\n",
      "                       '2 using gradient descent with momentum and weight\\n'\n",
      "                       'decay. Therefore, in theory, the training of long-term '\n",
      "                       'memory module requires O(𝑁)FLOPs, where 𝑁 is the '\n",
      "                       'sequence\\n'\n",
      "                       'length. However, in practice, we need to parallelize '\n",
      "                       'the training process and to fully take advantage of '\n",
      "                       'hardware accelerators\\n'\n",
      "                       '(e.g., TPUs, GPUs), we need to tensorize the process '\n",
      "                       'and use more matmuls.\\n'\n",
      "                       'Next, we show that calculating the weights in the '\n",
      "                       'inner loop with mini-batch gradient descent, '\n",
      "                       'data-dependent learning\\n'\n",
      "                       'rate, and weight decay can be reformulated so that it '\n",
      "                       'uses only matmuls and sum. We build upon the work of '\n",
      "                       'Yu Sun et al.\\n'\n",
      "                       '(2024) that shows forward pass of a model optimizing '\n",
      "                       'with the mini-batch gradient descent (with constant '\n",
      "                       'learning rate)\\n'\n",
      "                       'can be calculated using matmuls. We can split the '\n",
      "                       'sequence into chunks of size 𝑏 ≥1, and write the '\n",
      "                       'mini-batch gradient\\n'\n",
      "                       'descent as:\\n'\n",
      "                       'M𝑡 = (1 −𝛼𝑡)M𝑡−1 −𝜃𝑡∇ℓ(M𝑡−1; 𝑥𝑡)= 𝛽𝑡M0 −\\n'\n",
      "                       '𝑡∑︁\\n'\n",
      "                       '𝑖=1\\n'\n",
      "                       '𝜃𝑖\\n'\n",
      "                       '𝛽𝑡\\n'\n",
      "                       '𝛽𝑖\\n'\n",
      "                       '∇ℓ(M𝑡′; 𝑥𝑖), (16)\\n'\n",
      "                       'where 𝑡′= 𝑡 −mod(𝑡,𝑏), and 𝛽𝑖 = Î𝑖\\n'\n",
      "                       '𝑗=1 (1 −𝛼𝑗). For the sake of simplicity, we focus on '\n",
      "                       'the first chunk, i.e., 𝑡 = 𝑏and so\\n'\n",
      "                       '𝑡′= 0. Also, we explain the process for the case that '\n",
      "                       'M𝑡 = 𝑊𝑡 is linear. The process for MLPs with 𝑁𝑝 ≥2 is '\n",
      "                       'similar. Using\\n'\n",
      "                       'our loss function, we have:\\n'\n",
      "                       '∇ℓ(𝑊0; 𝑥𝑡)= (𝑊0𝑥𝑡 −𝑥𝑡)𝑥⊤\\n'\n",
      "                       '𝑡 ⇒\\n'\n",
      "                       '𝑏∑︁\\n'\n",
      "                       '𝑖=1\\n'\n",
      "                       '𝜃𝑖\\n'\n",
      "                       '𝛽𝑏\\n'\n",
      "                       '𝛽𝑖\\n'\n",
      "                       '∇ℓ(𝑊0; 𝑥𝑖)= Θ𝑏B𝑏(𝑊0𝑋 −𝑋)𝑋⊤, (17)\\n'\n",
      "                       'where Θ𝑏 = diag \\x00\\x02𝜃1 𝜃2 ... 𝜃 𝑏\\n'\n",
      "                       '\\x03\\x01 and B𝑏 is defined analogously on 𝛽𝑏\\n'\n",
      "                       '𝛽𝑖\\n'\n",
      "                       's. Note that, we do not need to store allΘ𝑘𝑏 and\\n'\n",
      "                       'B𝑘𝑏 for 𝑘 = 1,...,𝑁 /𝑏, instead, we store these '\n",
      "                       'matrices for each chunk, resulting in using less '\n",
      "                       'memory. Next, we extend\\n'\n",
      "                       'this representation so we can also incorporate the '\n",
      "                       'momentum term. In a chunk wise gradient descent with '\n",
      "                       'momentum, if\\n'\n",
      "                       'we look at the momentum term, we have:\\n'\n",
      "                       '𝑆𝑡 = 𝜂𝑡𝑆𝑡−1 −𝜃𝑡 𝑢𝑡, (18)\\n'\n",
      "                       'where 𝑢𝑡 = ∇ℓ(𝑀𝑡′; 𝑥𝑡). Note that, we can compute all '\n",
      "                       '𝑢𝑡 at the same time, and so Equation 18 is a linear '\n",
      "                       'recurrence\\n'\n",
      "                       'with 𝑢𝑡 as an input, 𝑆𝑡 as the hidden state, and 𝜂𝑡 as '\n",
      "                       'input-dependent transition value. Accordingly, we can '\n",
      "                       'use parallel\\n'\n",
      "                       'associative scan (J. T. Smith, Warrington, and '\n",
      "                       'Linderman 2023) to calculate𝑆𝑡s in this chunk.\\n'\n",
      "                       'Parameters as the Function of Chunks. Instead of '\n",
      "                       'making parameters like𝛼𝑡,𝜃𝑡, and 𝜂𝑡 input-dependent '\n",
      "                       '(i.e., a function\\n'\n",
      "                       'of token 𝑥𝑡), we can make them functions of their '\n",
      "                       'chunk. Despite losing expressive power, this '\n",
      "                       'formulation can help to\\n'\n",
      "                       'make the training even faster. In this case, we are '\n",
      "                       'using the same value for each of 𝛼, 𝜃, and 𝜂in each '\n",
      "                       'chunk. Accordingly,\\n'\n",
      "                       'in Equation 17, we can store Θ using a single scaler. '\n",
      "                       'Similarly we can make Equation 18 faster. That is, '\n",
      "                       'when 𝜂and 𝜃 are\\n'\n",
      "                       'learnable but time-invariant inside each chunk, this '\n",
      "                       'equation becomes a linear time-invariant system (LTI), '\n",
      "                       'which can be\\n'\n",
      "                       'computed by a global convolution (Gu, Goel, and Re '\n",
      "                       '2022). In our experiments, we make these parameters as '\n",
      "                       'the functions\\n'\n",
      "                       'of tokens. However, such simplifications (i.e., as the '\n",
      "                       'function of chunks) can be the interest of future work '\n",
      "                       'to training\\n'\n",
      "                       'larger models in more efficient manner.\\n'\n",
      "                       '7\\n'\n",
      "                       'Figure 2: Memory as a Context (MAC) Architecture. This '\n",
      "                       'architecture includes three branches of (1) core, (2) '\n",
      "                       'contextual\\n'\n",
      "                       '(long-term) memory, and (3) persistent memory. The '\n",
      "                       'core branch concatenates the corresponding long-term '\n",
      "                       'and persistent\\n'\n",
      "                       'memories with the input sequence. Next, attention '\n",
      "                       'performs on the sequence and decides what part of the '\n",
      "                       'information\\n'\n",
      "                       'should store in the long-term memory. At the test '\n",
      "                       'time, parameters corresponds to contextual memory are '\n",
      "                       'still learning,\\n'\n",
      "                       'parameters corresponds to the core branch are '\n",
      "                       'responsible for in-context learning, and parameters of '\n",
      "                       'persistent memory\\n'\n",
      "                       'are responsible to store the knowledge about tasks and '\n",
      "                       'so are fixed.\\n'\n",
      "                       '3.3 Persistent Memory\\n'\n",
      "                       'Our long-term memory can also be seen as a contextual '\n",
      "                       'memory, meaning that the output is fully depend on the '\n",
      "                       'context.\\n'\n",
      "                       'Therefore, in addition to our long-term memory, we '\n",
      "                       'also use a set of learnable but input-independent '\n",
      "                       'parameters to act as\\n'\n",
      "                       'task-related memory. This type of memory has been '\n",
      "                       'referred to as persistent or meta-memory in the '\n",
      "                       'literature (X. Dong\\n'\n",
      "                       'et al. 2024; Sukhbaatar, Grave, et al. 2019). Given 𝑁𝑝 '\n",
      "                       '≥1, we use learnable parameters 𝑃 =\\n'\n",
      "                       '\\x02𝑝1 𝑝2 ... 𝑝 𝑁𝑝\\n'\n",
      "                       '\\x03\\n'\n",
      "                       'and\\n'\n",
      "                       'append it to the start of our sequence: i.e., given a '\n",
      "                       'context window size of 𝑁, we modify the input as:\\n'\n",
      "                       '𝑥new =\\n'\n",
      "                       '\\x02𝑝1 𝑝2 ... 𝑝 𝑁𝑝\\n'\n",
      "                       '\\x03\\n'\n",
      "                       '|| 𝑥, (19)\\n'\n",
      "                       'where ||is concatenation. Next, we discuss the '\n",
      "                       'motivation of persistent memory from three '\n",
      "                       'perspective:\\n'\n",
      "                       'Memory Perspective. As discussed earlier, our neural '\n",
      "                       'long-term memory is a contextual memory, in which all '\n",
      "                       'parameters\\n'\n",
      "                       'are input-dependent. An effective memory system, '\n",
      "                       'however, also needs input-independent parameters to '\n",
      "                       'store the\\n'\n",
      "                       'abstraction of the task knowledge. That is, mastering '\n",
      "                       'a task requires the memorization of the knowledge that '\n",
      "                       'how the task\\n'\n",
      "                       'can be done, and these parameters are responsible for '\n",
      "                       'storing such knowledge.\\n'\n",
      "                       'Feedforward Network Perspective. In the Transformer '\n",
      "                       'architectures, there are fully connected layers after '\n",
      "                       'the attention\\n'\n",
      "                       'module, which are shown to be similar to attention '\n",
      "                       'weights but with data-independent parameters. That is, '\n",
      "                       'Sukhbaatar,\\n'\n",
      "                       'Grave, et al. (2019) showed that replacing the ReLU in '\n",
      "                       'fully connected layers with Softmax can results in an '\n",
      "                       'attention-like\\n'\n",
      "                       'weights, in which weights are data-independent:\\n'\n",
      "                       '𝐹𝐹𝑁 (𝑥)= 𝑊𝑉 Softmax (𝑊𝐾𝑥). (20)\\n'\n",
      "                       'In fact, 𝑊𝐾 and 𝑊𝑉 are acting similar to 𝐾 and 𝑉 '\n",
      "                       'matrices in attention module when they are '\n",
      "                       'input-independent. The\\n'\n",
      "                       'persistent memory weights are expected to have the '\n",
      "                       'same functionality, meaning that using them in the '\n",
      "                       'first part of the\\n'\n",
      "                       'sequence leads to having input-independent attention '\n",
      "                       'weights (Sukhbaatar, Grave, et al. 2019).\\n'\n",
      "                       'Technical Perspective. Attention with causal mask has '\n",
      "                       'implicit bias toward initial tokens in the sequence, '\n",
      "                       'and so attention\\n'\n",
      "                       'weights are almost always highly active for initial '\n",
      "                       'tokens, resulting in performance damage. From the '\n",
      "                       'technical perspective,\\n'\n",
      "                       'these learnable parameters at the start of the '\n",
      "                       'sequence can mitigate such effect by redistributing '\n",
      "                       'the attention weights\\n'\n",
      "                       'more effectively (Han et al. 2024; Xiao et al. 2024).\\n'\n",
      "                       '8\\n'\n",
      "                       '(a) Memory as a Context (MAC). We segment the '\n",
      "                       'sequence\\n'\n",
      "                       'and use full causal attention in each window. Again, '\n",
      "                       'the first\\n'\n",
      "                       '𝑁𝑝 tokens are persistent memory and the next 𝑁𝑙 are '\n",
      "                       'long-term\\n'\n",
      "                       'memory tokens\\n'\n",
      "                       '(b) Memory as Gating (MAG). We use sliding window '\n",
      "                       'attention\\n'\n",
      "                       '(SWA) as a short-term memory and our neural memory '\n",
      "                       'module\\n'\n",
      "                       'as a long-term memory, combining by a gating.\\n'\n",
      "                       'Figure 3: Attention masks for different variants of '\n",
      "                       'Titans.\\n'\n",
      "                       '4 How to Incorporate Memory?\\n'\n",
      "                       'A\\n'\n",
      "                       'n important question that remained unanswered is: How '\n",
      "                       'one can effectively and efficiently incorporate the\\n'\n",
      "                       'designed neural memory into a deep learning '\n",
      "                       'architecture? As discussed earlier, from a memory '\n",
      "                       'perspective,\\n'\n",
      "                       'the pair of K and V matrices in transformers can be '\n",
      "                       'interpreted as an associative memory block. Due to '\n",
      "                       'their\\n'\n",
      "                       'accurate modeling of dependencies and so their limited '\n",
      "                       'context window, we interpret them as short-term memory '\n",
      "                       'modules,\\n'\n",
      "                       'attending to the current context window size. On the '\n",
      "                       'other hand, our neural memory with the ability to '\n",
      "                       'continuously\\n'\n",
      "                       'learn from data and store it in its weights can play '\n",
      "                       'the role of a a long-term memory. In this section, we '\n",
      "                       'aim to answer\\n'\n",
      "                       'the above question by proposing three different '\n",
      "                       'variants of Titans. Later in our experiments, we show '\n",
      "                       'that each of these\\n'\n",
      "                       'variants has its own advantages/disadvantages and also '\n",
      "                       'can show a trade-off between the efficiency and '\n",
      "                       'effectiveness in\\n'\n",
      "                       'very long-contexts.\\n'\n",
      "                       '4.1 Memory as a Context\\n'\n",
      "                       'In the first architecture design (see Figure 2), we '\n",
      "                       'treat the memory as a context to the current '\n",
      "                       'information. That is, given\\n'\n",
      "                       'a long sequence 𝑥 ∈R𝑁×𝑑in , we first chunk the '\n",
      "                       'sequence into fixed-size segments S(𝑖) for 𝑖 = 1,...,𝑁 '\n",
      "                       '/𝐶. Given the\\n'\n",
      "                       'incoming segment S(𝑡), we consider it as the current '\n",
      "                       'context and its past segment as the historical '\n",
      "                       'information. Therefore,\\n'\n",
      "                       'let M𝑡−1 be the state of long-term memory before '\n",
      "                       'segment S(𝑡), we use the input context as the query to '\n",
      "                       'the memory\\n'\n",
      "                       'M𝑡−1 to retrieve the corresponding information from '\n",
      "                       'the long-term memory. That is, we retrieve the past '\n",
      "                       'information that\\n'\n",
      "                       'corresponds to S(𝑡)as:\\n'\n",
      "                       'ℎ𝑡 = M∗\\n'\n",
      "                       '𝑡−1 (q𝑡), (21)\\n'\n",
      "                       'where q𝑡 = S(𝑡)𝑊𝑄. Next, we use this historical '\n",
      "                       'information along with our persistent memory '\n",
      "                       'parameters as the input\\n'\n",
      "                       'sequence to the attention module:\\n'\n",
      "                       '˜S\\n'\n",
      "                       '(𝑡)\\n'\n",
      "                       '=\\n'\n",
      "                       '\\x02𝑝1 𝑝2 ... 𝑝 𝑁𝑝\\n'\n",
      "                       '\\x03\\n'\n",
      "                       '|| ℎ𝑡 || S(𝑡), (22)\\n'\n",
      "                       '𝑦𝑡 = Attn\\n'\n",
      "                       '\\x10\\n'\n",
      "                       '˜S\\n'\n",
      "                       '(𝑡)\\x11\\n'\n",
      "                       '. (23)\\n'\n",
      "                       'The structure of the attention map over the entire '\n",
      "                       'sequence is shown in Figure 3a. We then use 𝑦𝑡 to '\n",
      "                       'update the long-term\\n'\n",
      "                       'memory module for the next segment and the final '\n",
      "                       'output:\\n'\n",
      "                       'M𝑡 = M𝑡−1 (𝑦𝑡), (24)\\n'\n",
      "                       '𝑜𝑡 = 𝑦𝑡 ⊗M∗\\n'\n",
      "                       '𝑡 (𝑦𝑡). (25)\\n'\n",
      "                       'Note that, in the above, we are updating the weight of '\n",
      "                       'M𝑡−1 through forward pass.\\n'\n",
      "                       'This architecture has two key advantages: (1) '\n",
      "                       'Attention by having both historical and current '\n",
      "                       'context, has the ability to\\n'\n",
      "                       'decides whether given the current data, the long-term '\n",
      "                       'memory information is needed. (2) The attention module '\n",
      "                       'helps\\n'\n",
      "                       '9\\n'\n",
      "                       'Figure 4: Memory as a Gate (MAG) Architecture. This '\n",
      "                       'architecture, similarly, has the three branches of (1) '\n",
      "                       'core, (2)\\n'\n",
      "                       'contextual memory, and (3) persistent memory. It, '\n",
      "                       'however, incorporates only persistent memory into the '\n",
      "                       'context and\\n'\n",
      "                       'combine memory with the core branch using a gating '\n",
      "                       'mechanism. At test time, the behavior is the same as '\n",
      "                       'Figure 2.\\n'\n",
      "                       'the long-term memory to store only useful information '\n",
      "                       'from the current context. That is, not all tokens in '\n",
      "                       'each segment\\n'\n",
      "                       'are useful and memorizing all of them can result in '\n",
      "                       'memory overflow. Therefore, attention is helping the '\n",
      "                       'memory to\\n'\n",
      "                       'understand which information is useful, better '\n",
      "                       'managing the memory capacity. (3) At test time: (i) '\n",
      "                       'persistent memory\\n'\n",
      "                       'parameters are fixed as they encodes the knowledge '\n",
      "                       'about the task, which should not be changed; (ii) the '\n",
      "                       'attention module\\n'\n",
      "                       'weights are in-context learner; and (iii) the '\n",
      "                       'long-term memory module is still learning (memorizing) '\n",
      "                       'the information at test\\n'\n",
      "                       'time. That is, we update the weights of the neural '\n",
      "                       'memory even at test time as weights are encoding the '\n",
      "                       'abstraction of\\n'\n",
      "                       'long past.\\n'\n",
      "                       '4.2 Gated Memory\\n'\n",
      "                       'In the next variant (see Figure 4), in one branch, we '\n",
      "                       'directly use the input data to update the long-term '\n",
      "                       'memory, and in the\\n'\n",
      "                       'second branch, we use a sliding window attention '\n",
      "                       '(SWA):\\n'\n",
      "                       '˜𝑥 =\\n'\n",
      "                       '\\x02𝑝1 𝑝2 ... 𝑝 𝑁𝑝\\n'\n",
      "                       '\\x03\\n'\n",
      "                       '|| 𝑥, (26)\\n'\n",
      "                       '𝑦 = SW-Attn∗(˜𝑥), (27)\\n'\n",
      "                       '𝑜 = 𝑦⊗M( ˜𝑥), (28)\\n'\n",
      "                       'where SW-Attn∗is sliding window attention with prefix '\n",
      "                       '(see Figure 3b). Note that, contrary to the previous '\n",
      "                       'design, we are\\n'\n",
      "                       'not segmenting the input data. Also, we abuse the '\n",
      "                       'notation and use M(𝑥)to refer to the final output of '\n",
      "                       'the memory after\\n'\n",
      "                       'all recursion over the tokens of the sequence. In the '\n",
      "                       'above equation, ⊗can be any non-linear gating. In our '\n",
      "                       'experiments,\\n'\n",
      "                       'we normalize the outputs 𝑦and M(˜𝑥)using learnable '\n",
      "                       'vector-valued weights, followed by a non-linearity '\n",
      "                       '𝜎(.).\\n'\n",
      "                       'The overall attention mask of this design is shown in '\n",
      "                       'Figure 3b. In this design, sliding window attention is '\n",
      "                       'act as a precise\\n'\n",
      "                       'short-term memory, while the neural memory module is '\n",
      "                       'acting as a fading memory for the model. This '\n",
      "                       'architecture design\\n'\n",
      "                       'can also be seen as a multi-head architecture where '\n",
      "                       'the structure of heads are different (X. Dong et al. '\n",
      "                       '2024).\\n'\n",
      "                       '4.3 Memory as a Layer\\n'\n",
      "                       'The last variant uses the neural Memory As a Layer '\n",
      "                       '(MAL) of a deep neural network (see Figure 5). This '\n",
      "                       'architecture\\n'\n",
      "                       'design is more common in the literature, where the '\n",
      "                       'hybrid models stack recurrent models with full or '\n",
      "                       'sliding window\\n'\n",
      "                       'attentions. Given input 𝑥, we have:\\n'\n",
      "                       '˜𝑥 =\\n'\n",
      "                       '\\x02𝑝1 𝑝2 ... 𝑝 𝑁𝑝\\n'\n",
      "                       '\\x03\\n'\n",
      "                       '|| 𝑥, (29)\\n'\n",
      "                       '𝑦 = M(˜𝑥), (30)\\n'\n",
      "                       '𝑜 = SW-Attn (𝑦), (31)\\n'\n",
      "                       '10\\n'\n",
      "                       'Figure 5: Memory as a Layer (MAL) Architecture. In '\n",
      "                       'this architecture, the memory layer is responsible to '\n",
      "                       'compress the\\n'\n",
      "                       'past and current context before the attention module.\\n'\n",
      "                       'where SW-Attn is sliding window attention. The main '\n",
      "                       'drawback of this design is that the power of the model '\n",
      "                       'is limited by\\n'\n",
      "                       'each of the layers and so it cannot take advantage of '\n",
      "                       'the complementary data processing of attention and '\n",
      "                       'neural memory\\n'\n",
      "                       'module. In our experiments, for evaluating memory in '\n",
      "                       'this design, we use a similar architecture as H3 (D. '\n",
      "                       'Y. Fu et al. 2023),\\n'\n",
      "                       'where we replace the the sequence model with our '\n",
      "                       'neural memory module (LMM).\\n'\n",
      "                       'Memory Without Attention. Although in the above, we '\n",
      "                       'discussed MAL as the combination of LMMs and attention '\n",
      "                       'in\\n'\n",
      "                       'a sequential manner, one simple variant of MAL is to '\n",
      "                       'treat LMM as a sequence model without any attention. '\n",
      "                       'From the\\n'\n",
      "                       'memory perspective, as discussed in Section 1, we '\n",
      "                       'expect each part of the memory system to work '\n",
      "                       'independently, even if\\n'\n",
      "                       'other components are disturbed. Therefore, a long-term '\n",
      "                       'memory module should still be a powerful model even '\n",
      "                       'without\\n'\n",
      "                       'short-term memory (i.e., attention). We refer to this '\n",
      "                       'variant as LMM or Titans (LMM) in our experiments. We '\n",
      "                       'provide\\n'\n",
      "                       'additional discussions on the connection of Titans and '\n",
      "                       'other modern recurrent models in Appendix C.\\n'\n",
      "                       '4.4 Architectural Details\\n'\n",
      "                       'For the sake of simplicity and presentation, we avoid '\n",
      "                       'discussing the implementation details like using '\n",
      "                       'residual connection,\\n'\n",
      "                       'gating with linear layer, and normalization. In all '\n",
      "                       'blocks, we use residual connections. In our '\n",
      "                       'implementation, we use\\n'\n",
      "                       'SiLU(.) activation (Elfwing, Uchibe, and Doya 2018) as '\n",
      "                       'the non-linear activation for computing query, key, '\n",
      "                       'and values and\\n'\n",
      "                       'normalize queries and keys using ℓ2-norm.\\n'\n",
      "                       'Convolution. Following the recent modern linear '\n",
      "                       'recurrent models (Gu and Dao 2024; S. Yang, Kautz, and '\n",
      "                       'Hatamizadeh\\n'\n",
      "                       '2024), we incorporate a 1D depthwise-separable '\n",
      "                       'convolution layer after each of the query, key, and '\n",
      "                       'value projections.\\n'\n",
      "                       'While not significantly affect the performance, these '\n",
      "                       '1D convolutions have shown performance improvement and '\n",
      "                       'are also\\n'\n",
      "                       'computationally efficient.\\n'\n",
      "                       'Gating. We also follow the recent architectures that '\n",
      "                       'use normalization and gating with a linear layer '\n",
      "                       'before the final\\n'\n",
      "                       'output projection (Mehta et al. 2023).\\n'\n",
      "                       'Theorem 4.1. Contrary to Transformers, diagonal linear '\n",
      "                       'recurrent models, and DeltaNet, all of which are '\n",
      "                       'limited to TC0 (Merrill,\\n'\n",
      "                       'Petty, and Sabharwal 2024), Titans are capable of '\n",
      "                       'solving problems beyond TC 0, meaning that Titans are '\n",
      "                       'theoretically more\\n'\n",
      "                       'expressive than Transformers and most modern linear '\n",
      "                       'recurrent models in state tracking tasks.\\n'\n",
      "                       '5 Experiments\\n'\n",
      "                       'N\\n'\n",
      "                       'ext, we evaluate the performance of Titans and its '\n",
      "                       'variants in language modeling, commonsense reasoning, '\n",
      "                       'needle\\n'\n",
      "                       'in haystack, DNA modeling, and time series forecasting '\n",
      "                       'tasks1. In more details, in this section, we answer '\n",
      "                       'the\\n'\n",
      "                       'following empirical questions: (1) How do Titans '\n",
      "                       'perform compared to baselines in downstream tasks? '\n",
      "                       '(see §5.2,\\n'\n",
      "                       '1In the first version of the work, we aim to provide '\n",
      "                       'insights/evidences about why the learning paradigms of '\n",
      "                       'Titans are effective. We are working on\\n'\n",
      "                       'finalizing the results of larger models and will '\n",
      "                       'report them in the next version.\\n'\n",
      "                       '11\\n'\n",
      "                       '§5.6, and §5.7); (2) What is the actual context length '\n",
      "                       'of Titans? (see §5.3 and §5.4); (3) How do Titans '\n",
      "                       'scale with respect to\\n'\n",
      "                       'context length? (see §5.8); (4) How the depth of '\n",
      "                       'memory can affect both performance and efficiency? '\n",
      "                       '(see §5.5); and (5)\\n'\n",
      "                       'What is the contribution of each Titans’ component in '\n",
      "                       'its performance? (see §5.9).\\n'\n",
      "                       '5.1 Experimental Setup\\n'\n",
      "                       'Models. In our experiments, we focus on the three '\n",
      "                       'variants of Titans, which we refer to as: Titans with '\n",
      "                       '(1) Memory as a\\n'\n",
      "                       'Context (MAC), (2) Memory as a Gate (MAG), and (3) '\n",
      "                       'Memory as a Layer (MAL) as well as (4) neural memory '\n",
      "                       'module\\n'\n",
      "                       'alone. The reason behind using our long-term memory as '\n",
      "                       'a separate module is based on our definition of '\n",
      "                       'learning. As\\n'\n",
      "                       'discussed in Section 1, we define learning a process '\n",
      "                       'for acquiring effective and useful memory. '\n",
      "                       'Accordingly, we expect our\\n'\n",
      "                       'long-term memory to effectively learn from data, even '\n",
      "                       'without attention. For each of these models, we '\n",
      "                       'consider four scales\\n'\n",
      "                       'with: (i) 170M, (ii) 340M, (iii) 400M, and (iv) 760M '\n",
      "                       'parameters. While the first three are trained on 15B '\n",
      "                       'tokens sampled\\n'\n",
      "                       'from FineWeb-Edu dataset (Penedo et al. 2024), the '\n",
      "                       'last one is trained on 30B tokens from the same '\n",
      "                       'dataset.\\n'\n",
      "                       'Baselines. We compare our models with the '\n",
      "                       'state-of-the-art linear recurrent models, '\n",
      "                       'Transformers, and hybrid models\\n'\n",
      "                       '(recurrent + attention). More specifically in language '\n",
      "                       'tasks, we compare with Transformer++ (Touvron et al. '\n",
      "                       '2023),\\n'\n",
      "                       'RetNet (Yutao Sun et al. 2023), Gated Linear Attention '\n",
      "                       '(GLA) (S. Yang, B. Wang, Shen, et al. 2024), Mamba (Gu '\n",
      "                       'and Dao\\n'\n",
      "                       '2024), Mamba2 (Dao and Gu 2024), DeltaNet (S. Yang, B. '\n",
      "                       'Wang, Yu Zhang, et al. 2024), TTT (Yu Sun et al. '\n",
      "                       '2024), and Gated\\n'\n",
      "                       'DeltaNet (S. Yang, Kautz, and Hatamizadeh 2024). In '\n",
      "                       'needle in haystack tasks, we also compare with GPT4 '\n",
      "                       '(Achiam et al.\\n'\n",
      "                       '2023), Llama3 with RAG (Touvron et al. 2023), '\n",
      "                       'RecurrentGemma2-9B (Botev et al. 2024), and Mistral '\n",
      "                       '(Jiang et al. 2023)\\n'\n",
      "                       'models, all of which are provided in the benchmark '\n",
      "                       '(Yuri Kuratov et al. 2024). In time series tasks, we '\n",
      "                       'compare with\\n'\n",
      "                       'Mamba-based (Behrouz, Santacatterina, and Zabih 2024), '\n",
      "                       'Transformer-based (Y. Liu et al. 2023; Nie et al. '\n",
      "                       '2022; Yunhao\\n'\n",
      "                       'Zhang and Yan 2023), and linear models (Das et al. '\n",
      "                       '2023; Z. Li et al. 2023; H. Wu et al. 2023; Zeng et '\n",
      "                       'al. 2023).\\n'\n",
      "                       'Training. In the training, we follow the training '\n",
      "                       'procedure of S. Yang, Kautz, and Hatamizadeh (2024), '\n",
      "                       'and use LLama 2\\n'\n",
      "                       'tokenizer with a vocabulary size of 32K and use '\n",
      "                       'training length of 4K tokens. We employ AdamW '\n",
      "                       'optimizer with learning\\n'\n",
      "                       'rate of 4𝑒-4 with cosine annealing schedule with batch '\n",
      "                       'size of 0.5M tokens, and weight decay of 0.1.\\n'\n",
      "                       '5.2 Language Modeling\\n'\n",
      "                       'We first focus on the perplexity in language modeling '\n",
      "                       'and also commonsense reasoning tasks. The results for '\n",
      "                       'Titans’\\n'\n",
      "                       'variants and also baselines with three different sizes '\n",
      "                       'of 340M, 400M, and 760M are reported in Table 1. Among '\n",
      "                       'non-hybrid\\n'\n",
      "                       'models, including Transformer++, our neural memory '\n",
      "                       'module achieves the best performance in both '\n",
      "                       'perplexity and\\n'\n",
      "                       'accuracy measures. Comparing our neural memory module '\n",
      "                       'and TTT, which is also a gradient-based recurrent '\n",
      "                       'model can\\n'\n",
      "                       'show us the importance of our weight decay as well as '\n",
      "                       'the momentum. As discussed earlier, the weight decay '\n",
      "                       'can be\\n'\n",
      "                       'interpreted as a gating mechanism to forget the past '\n",
      "                       'data, when it is needed. Also, momentum can help us '\n",
      "                       'better manage\\n'\n",
      "                       'the memory by providing additional memory for the '\n",
      "                       'surprise metric. While some baselines also take '\n",
      "                       'advantage of gating\\n'\n",
      "                       'mechanism, e.g., Mamba, Mamba2, and Gated DeltaNet, '\n",
      "                       'the superior performance of our neural memory module '\n",
      "                       'shows\\n'\n",
      "                       'the importance of both our surprise mechanism and '\n",
      "                       'having deep and non-linear memory. We further discuss '\n",
      "                       'the later in\\n'\n",
      "                       'Section 5.5.\\n'\n",
      "                       'Comparing the hybrid models, we found that all three '\n",
      "                       'variants of Titans (MAC, MAG, and MAL) outperform both '\n",
      "                       'Samba\\n'\n",
      "                       '(Mamba + attention) and Gated DeltaNet-H2 (Gated '\n",
      "                       'DeltaNet + atttention). We attribute the superior '\n",
      "                       'performance of Titans\\n'\n",
      "                       '(MAL) to the power of neural memory module as the '\n",
      "                       'architecture design and used attention are all the '\n",
      "                       'same. Comparing\\n'\n",
      "                       'Titans (MAG) and (MAC), we find that while their '\n",
      "                       'performance are close, MAC performs better when '\n",
      "                       'dealing with longer\\n'\n",
      "                       'dependencies in the data. Interestingly, both MAG and '\n",
      "                       'MAC outperform MAL variant, which due to using the '\n",
      "                       'same\\n'\n",
      "                       'modules, we attribute this to the architecture design '\n",
      "                       'of these models. This finding is particularly '\n",
      "                       'important as the current\\n'\n",
      "                       'hybrid models (except Hymba (X. Dong et al. 2024)) in '\n",
      "                       'the literature are using MAL-style combination of '\n",
      "                       'recurrent models\\n'\n",
      "                       'and attention.\\n'\n",
      "                       '5.3 Needle in a Haystack\\n'\n",
      "                       'Scaling a model to longer context window is not always '\n",
      "                       'equivalent to being effective for very long sequences '\n",
      "                       '(Hsieh\\n'\n",
      "                       'et al. 2024). The needle-in-a-haystack (NIAH) task is '\n",
      "                       'designed to measure the actual effective context '\n",
      "                       'length of models.\\n'\n",
      "                       'In this task, we evaluate the model on retrieving a '\n",
      "                       'piece of information (i.e., the “needle”) from long '\n",
      "                       'distractor texts (i.e.,\\n'\n",
      "                       '12\\n'\n",
      "                       'Table 1: Performance of Titans and recurrent- and '\n",
      "                       'Transformer-based baselines on language modeling and '\n",
      "                       'common-sense\\n'\n",
      "                       'reasoning tasks. Hybrid models are marked with ∗. The '\n",
      "                       'best results among simple and hybrid models are '\n",
      "                       'highlighted.\\n'\n",
      "                       'Model Wiki. LMB. LMB. PIQA Hella. Wino. ARC-e ARC-c '\n",
      "                       'SIQA BoolQ Avg.\\n'\n",
      "                       'ppl↓ ppl↓ acc↑ acc↑ acc_n↑ acc↑ acc↑ acc_n↑ acc↑ acc↑ '\n",
      "                       '↑\\n'\n",
      "                       '340M params / 15B tokens\\n'\n",
      "                       'Transformer++ 31.52 41.08 30.76 62.98 34.76 50.53 '\n",
      "                       '45.21 24.05 36.81 58.24 42.92\\n'\n",
      "                       'RetNet 32.50 49.73 28.24 62.61 34.15 50.91 44.27 23.62 '\n",
      "                       '36.79 59.72 42.54\\n'\n",
      "                       'GLA 28.51 43.02 28.73 64.05 35.96 50.00 54.19 24.29 '\n",
      "                       '37.13 58.39 44.09\\n'\n",
      "                       'Mamba 30.83 40.21 29.94 63.79 35.88 49.82 49.24 24.56 '\n",
      "                       '35.41 60.07 43.59\\n'\n",
      "                       'DeltaNet 28.65 47.30 28.43 63.52 35.95 49.63 52.68 '\n",
      "                       '25.37 37.96 58.79 44.04\\n'\n",
      "                       'TTT 27.44 34.19 30.06 63.97 35.71 50.08 53.01 26.11 '\n",
      "                       '37.32 59.83 44.51\\n'\n",
      "                       'Gated DeltaNet 27.01 30.94 34.11 63.08 38.12 51.60 '\n",
      "                       '55.28 26.77 34.89 59.54 45.42\\n'\n",
      "                       'Titans (LMM) 26.18 29.97 34.98 64.73 39.61 51.85 55.60 '\n",
      "                       '28.14 34.52 59.99 46.17\\n'\n",
      "                       'Titans (MAC)∗ 25.43 28.13 36.00 65.32 40.35 51.21 '\n",
      "                       '58.17 29.00 38.63 60.18 47.36\\n'\n",
      "                       'Titans (MAG)∗ 25.07 28.72 36.71 64.88 40.56 52.49 '\n",
      "                       '57.72 28.16 39.75 60.01 47.54\\n'\n",
      "                       'Titans (MAL)∗ 24.69 28.80 35.74 64.97 39.44 51.97 '\n",
      "                       '56.58 28.21 38.14 57.32 46.55\\n'\n",
      "                       '400M params / 15B tokens\\n'\n",
      "                       'Transformer++ 30.63 37.37 29.64 64.27 37.72 51.53 '\n",
      "                       '54.95 27.36 38.07 61.59 45.64\\n'\n",
      "                       'RetNet 29.92 46.83 29.16 65.23 36.97 51.85 56.01 27.55 '\n",
      "                       '37.30 59.66 45.47\\n'\n",
      "                       'HGRN2 32.33 47.14 26.12 64.52 35.45 52.24 55.97 25.51 '\n",
      "                       '37.35 59.02 44.52\\n'\n",
      "                       'GLA 27.96 36.66 27.86 65.94 37.41 49.56 56.01 26.36 '\n",
      "                       '38.94 59.84 45.24\\n'\n",
      "                       'Mamba 29.22 39.88 29.82 65.72 37.93 50.11 58.37 26.70 '\n",
      "                       '37.76 61.13 45.94\\n'\n",
      "                       'Mamba2 26.34 33.19 32.03 65.77 39.73 52.48 59.00 27.64 '\n",
      "                       '37.92 60.72 46.91\\n'\n",
      "                       'DeltaNet 27.69 44.04 29.96 64.52 37.03 50.82 56.77 '\n",
      "                       '27.13 38.22 60.09 45.57\\n'\n",
      "                       'TTT 26.11 31.52 33.25 65.70 39.11 51.68 58.04 28.99 '\n",
      "                       '38.26 59.87 46.86\\n'\n",
      "                       'Gated DeltaNet 25.47 29.24 34.40 65.94 40.46 51.46 '\n",
      "                       '59.80 28.58 37.43 60.03 47.26\\n'\n",
      "                       'Samba∗ 25.32 29.47 36.86 66.09 39.24 51.45 60.12 27.20 '\n",
      "                       '38.68 58.22 47.23\\n'\n",
      "                       'Gated DeltaNet-H2∗ 24.19 28.09 36.77 66.43 40.79 52.17 '\n",
      "                       '59.55 29.09 39.04 58.56 47.69\\n'\n",
      "                       'Titans (LMM) 25.03 28.99 35.21 65.85 40.91 52.19 59.97 '\n",
      "                       '29.20 38.74 60.85 47.83\\n'\n",
      "                       'Titans (MAC)∗ 25.61 27.73 36.92 66.39 41.18 52.80 '\n",
      "                       '60.24 29.69 40.07 61.93 48.65\\n'\n",
      "                       'Titans (MAG)∗ 23.59 27.81 37.24 66.80 40.92 53.21 '\n",
      "                       '60.01 29.45 39.91 61.28 48.60\\n'\n",
      "                       'Titans (MAL)∗ 23.93 27.89 36.84 66.29 40.74 52.26 '\n",
      "                       '59.85 29.71 38.92 58.40 47.87\\n'\n",
      "                       '760M params / 30B tokens\\n'\n",
      "                       'Transformer++ 25.21 27.64 35.78 66.92 42.19 51.95 '\n",
      "                       '60.38 32.46 39.51 60.37 48.69\\n'\n",
      "                       'RetNet 26.08 24.45 34.51 67.19 41.63 52.09 63.17 32.78 '\n",
      "                       '38.36 57.92 48.46\\n'\n",
      "                       'Mamba 28.12 23.96 32.80 66.04 39.15 52.38 61.49 30.34 '\n",
      "                       '37.96 57.62 47.22\\n'\n",
      "                       'Mamba2 22.94 28.37 33.54 67.90 42.71 49.77 63.48 31.09 '\n",
      "                       '40.06 58.15 48.34\\n'\n",
      "                       'DeltaNet 24.37 24.60 37.06 66.93 41.98 50.65 64.87 '\n",
      "                       '31.39 39.88 59.02 48.97\\n'\n",
      "                       'TTT 24.17 23.51 34.74 67.25 43.92 50.99 64.53 33.81 '\n",
      "                       '40.16 59.58 47.32\\n'\n",
      "                       'Gated DeltaNet 21.18 22.09 35.54 68.01 44.95 50.73 '\n",
      "                       '66.87 33.09 39.21 59.14 49.69\\n'\n",
      "                       'Samba∗ 20.63 22.71 39.72 69.19 47.35 52.01 66.92 33.20 '\n",
      "                       '38.98 61.24 51.08\\n'\n",
      "                       'Gated DeltaNet-H2∗ 19.88 20.83 39.18 68.95 48.22 52.57 '\n",
      "                       '67.01 35.49 39.39 61.11 51.49\\n'\n",
      "                       'Titans (LMM) 20.04 21.96 37.40 69.28 48.46 52.27 66.31 '\n",
      "                       '35.84 40.13 62.76 51.56\\n'\n",
      "                       'Titans (MAC) 19.93 20.12 39.62 70.46 49.01 53.18 67.86 '\n",
      "                       '36.01 41.87 62.05 52.51\\n'\n",
      "                       'Titans (MAG) 18.61 19.86 40.98 70.25 48.94 52.89 68.23 '\n",
      "                       '36.19 40.38 62.11 52.50\\n'\n",
      "                       'Titans (MAL) 19.07 20.33 40.05 69.99 48.82 53.02 67.54 '\n",
      "                       '35.65 30.98 61.72 50.97\\n'\n",
      "                       'the “haystack”). In this part, we use Single NIAH '\n",
      "                       '(S-NIAH) task from RULER benchmark (Hsieh et al. 2024) '\n",
      "                       'and evaluate\\n'\n",
      "                       'Titans and baselines on sequences with length 2K, 4K, '\n",
      "                       '8K, and 16K. The results are reported in Table 2. '\n",
      "                       'Neural Memory\\n'\n",
      "                       'module achieves the best results compare to baselines '\n",
      "                       'in all three tasks. We attribute this superior '\n",
      "                       'performance to three\\n'\n",
      "                       'key differences of Titans with existing sequence '\n",
      "                       'models: (1) Compared to TTT, our Neural Memory can '\n",
      "                       'better handle the\\n'\n",
      "                       'memory capacity by using momentum and also the '\n",
      "                       'forgetting mechanism (i.e., weight decay). Therefore, '\n",
      "                       'with increasing\\n'\n",
      "                       'the sequence length, the performance of Neural Memory '\n",
      "                       'does not drop and show a consistent trend; (2) '\n",
      "                       'Compared to\\n'\n",
      "                       'Mamba2, which has the gating (forgetting) mechanism, '\n",
      "                       'Titans have deep non-linear memory, resulting in '\n",
      "                       'better memory\\n'\n",
      "                       'management. Also, contrary to our neural memory and '\n",
      "                       'DeltaNet, Mamba2 is not capable of removing a memory '\n",
      "                       'and so\\n'\n",
      "                       '13\\n'\n",
      "                       'Table 2: Performance of Titans and baselines on S-NIAH '\n",
      "                       'task from RULER benchmark. The best results among '\n",
      "                       'simple\\n'\n",
      "                       'and hybrid models are highlighted.\\n'\n",
      "                       'Model S-NIAH-PK S-NIAH-N S-NIAH-W\\n'\n",
      "                       '2K 4K 8K 16K 2K 4K 8K 16K 2K 4K 8K 16K\\n'\n",
      "                       'TTT 98.4 98.8 98.0 88.4 60.2 36.6 10.2 4.4 78.8 28.0 '\n",
      "                       '4.4 0.0\\n'\n",
      "                       'Mamba2 98.6 61.4 31.0 5.4 98.4 55.8 14.2 0.0 42.2 4.2 '\n",
      "                       '0.0 0.0\\n'\n",
      "                       'DeltaNet 96.8 98.8 98.6 71.4 47.2 15.4 12.8 5.4 46.2 '\n",
      "                       '20.0 1.6 0.0\\n'\n",
      "                       'Titans (LMM)99.8 98.4 98.2 96.2 100.0 99.8 93.4 80.2 '\n",
      "                       '90.4 89.4 85.8 80.6\\n'\n",
      "                       'Titans (MAC) 99.2 98.8 99.0 98.4 99.6 98.2 97.6 97.4 '\n",
      "                       '98.2 98.2 95.6 95.2\\n'\n",
      "                       'Titans (MAG)99.4 98.0 97.4 97.4 99.2 98.8 97.2 98.6 '\n",
      "                       '98.0 98.0 90.2 88.2\\n'\n",
      "                       'Titans (MAL) 98.8 98.6 98.8 97.8 99.8 98.1 96.8 96.4 '\n",
      "                       '98.0 97.4 92.0 90.4\\n'\n",
      "                       '(a) Few-shot Setup\\n'\n",
      "                       ' (b) Fine-Tuning Setup\\n'\n",
      "                       'Figure 6: Performance of Titans and baselines on '\n",
      "                       'BABILong benchmark. Titans (MAC) outperforms all '\n",
      "                       'baselines, including\\n'\n",
      "                       'extremely large models, e.g., GPT4.\\n'\n",
      "                       'we can see a significant drop in performance when '\n",
      "                       'increasing the sequence length; (3) Compared to '\n",
      "                       'DeltaNet, although it\\n'\n",
      "                       'is capable of removing memory using delta rule, it '\n",
      "                       'cannot erase the memory, lacking forgetting mechanism. '\n",
      "                       'Finally, As\\n'\n",
      "                       'expected we can see on par or better results when '\n",
      "                       'using Titans variants, where the best results '\n",
      "                       'correspond to MAC.\\n'\n",
      "                       '5.4 BABILong Benchmark\\n'\n",
      "                       'In the previous section we discussed the results on a '\n",
      "                       'simple NIAH tasks where a single needle needs to be '\n",
      "                       'retrieved.\\n'\n",
      "                       'Although Titans showed better performance compared to '\n",
      "                       'baselines, their true advantage over very long '\n",
      "                       'sequences is still\\n'\n",
      "                       'hidden. To this end, in this section, we use a harder '\n",
      "                       'task from BABILong benchmark (Yuri Kuratov et al. '\n",
      "                       '2024), in which\\n'\n",
      "                       'the model needs to reason across facts distributed in '\n",
      "                       'extremely long documents. We follow the original '\n",
      "                       'experimental setup\\n'\n",
      "                       'and training process in the benchmark. There are two '\n",
      "                       'settings: (1) Few-shot setting, in which we use large '\n",
      "                       'pre-trained\\n'\n",
      "                       'models, and (2) fine-tuning setting, where we '\n",
      "                       'fine-tune the MAC variant of Titans to compare it with '\n",
      "                       'other fine-tuned\\n'\n",
      "                       'baselines. The results for few-shot setting are '\n",
      "                       'reported in Figure 6a. In this setup, we can see '\n",
      "                       'Titans outperform all\\n'\n",
      "                       'baselines–i.e., Mamba2.8B (Gu and Dao 2024), RWKV-6-7B '\n",
      "                       '(Peng, Goldstein, et al. 2024), RecurrentGemma-9B '\n",
      "                       '(Botev et al.\\n'\n",
      "                       '2024), Gemma-9B (Team et al. 2024), Llama3.1-8B '\n",
      "                       '(Touvron et al. 2023), GPT-4, and GPT4o-mini (Achiam '\n",
      "                       'et al. 2023). These\\n'\n",
      "                       'results are achieved while Titans (MAC) is having much '\n",
      "                       'less number of parameters than baselines.\\n'\n",
      "                       'In the fine-tuning setup, we compare the small '\n",
      "                       'fine-tuned version of Titans (MAC) with: (i) the '\n",
      "                       'fine-tuned version of small\\n'\n",
      "                       'models (almost the same number of parameters as '\n",
      "                       'Titans) such as Mamba (Gu and Dao 2024), RMT (Bulatov, '\n",
      "                       'Yury Kuratov,\\n'\n",
      "                       'and Burtsev 2022), (ii) large models with '\n",
      "                       'Retrieval-Augmented Generation (RAG) (P. Lewis et al. '\n",
      "                       '2020) such as Llama3.1-\\n'\n",
      "                       '8B (Touvron et al. 2023), and (iii) extremely large '\n",
      "                       'models such as GPT-4 (Achiam et al. 2023), GPT4o-mini, '\n",
      "                       'Qwen2.5-72B (A.\\n'\n",
      "                       'Yang et al. 2024), and Llama3.1-70B (Touvron et al. '\n",
      "                       '2023). Baseline results are reported by (Yuri Kuratov '\n",
      "                       'et al. 2024). The\\n'\n",
      "                       'results of Titans and baselines are reported in Figure '\n",
      "                       '6b. Titans outperform all models even extremely large '\n",
      "                       'models like\\n'\n",
      "                       'GPT4. Also, compared to Transformer-based with memory '\n",
      "                       'models like RMT, Titans show better performance mainly '\n",
      "                       'due\\n'\n",
      "                       'to their powerful memory. That is, RMT compress the '\n",
      "                       'historical data into 16 size vector-valued memory, '\n",
      "                       'while Titans with\\n'\n",
      "                       'in-context online memory learner are capable of '\n",
      "                       'encoding the past into the parameters of the model. '\n",
      "                       'Interestingly, even\\n'\n",
      "                       '14\\n'\n",
      "                       '(a) 170M Parameters\\n'\n",
      "                       ' (b) 360M Parameters\\n'\n",
      "                       ' (c) 760M Parameters\\n'\n",
      "                       'Figure 7: The effect of memory depth on the '\n",
      "                       'perplexity. Deeper long-term memory results in better '\n",
      "                       'scaling in longer\\n'\n",
      "                       'sequences.\\n'\n",
      "                       'Table 3: Performance on long-term forecasting. The '\n",
      "                       'best results are highlighted .\\n'\n",
      "                       'Neural MemorySimba iTransformerRLinear '\n",
      "                       'PatchTSTCrossformerTiDE TimesNet DLinear\\n'\n",
      "                       'MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE '\n",
      "                       'MAE MSE MAE MSE MAE\\n'\n",
      "                       'ETTm1 0.358 0.387 0.3830.3960.407 0.410 '\n",
      "                       '0.4140.4070.3870.4000.5130.496 '\n",
      "                       '0.4190.4190.4000.4060.4030.407\\n'\n",
      "                       'ETTm2 0.261 0.309 0.2710.3270.288 0.332 '\n",
      "                       '0.2860.3270.2810.3260.7570.610 '\n",
      "                       '0.3580.4040.2910.3330.3500.401\\n'\n",
      "                       'ETTh1 0.420 0.421 0.4410.4320.454 0.447 '\n",
      "                       '0.4460.4340.4690.4540.5290.522 '\n",
      "                       '0.5410.5070.4580.4500.4560.452\\n'\n",
      "                       'ETTh2 0.336 0.382 0.3610.3910.383 0.407 '\n",
      "                       '0.3740.3980.3870.4070.9420.684 '\n",
      "                       '0.6110.5500.4140.4270.5590.515\\n'\n",
      "                       'ECL 0.162 0.261 0.1690.2740.178 0.270 '\n",
      "                       '0.2190.2980.2050.2900.2440.334 '\n",
      "                       '0.2510.3440.1920.2950.2120.300\\n'\n",
      "                       'Traffic 0.415 0.289 0.4930.2910.428 0.282 '\n",
      "                       '0.6260.3780.4810.3040.5500.304 '\n",
      "                       '0.7600.4730.6200.3360.6250.383\\n'\n",
      "                       'Weather0.231 0.265 0.2550.2800.258 0.278 '\n",
      "                       '0.2720.2910.2590.2810.2590.315 '\n",
      "                       '0.2710.3200.2590.2870.2650.317\\n'\n",
      "                       'augmenting Llama3.1-8B model with RAG performs worse '\n",
      "                       'than Titans with about ×70 less parameters.\\n'\n",
      "                       '5.5 The Effect of Deep Memory\\n'\n",
      "                       'In this section, we evaluate the effect of deep memory '\n",
      "                       'in both wall-clock training time and model '\n",
      "                       'performance2. To this\\n'\n",
      "                       'end, we focus on different variants of our neural '\n",
      "                       'memory module, where 𝐿M= 1,2,3,4. We also use Mamba as '\n",
      "                       'a baseline\\n'\n",
      "                       'for the model performance. For a fair comparison, we '\n",
      "                       'use the same training process for all models and train '\n",
      "                       'them on a\\n'\n",
      "                       'subset of the Pile dataset (L. Gao et al. 2020).\\n'\n",
      "                       'We report the perplexity of our models and baselines '\n",
      "                       'as the function of the sequence length in Figure 7. '\n",
      "                       'Interestingly, with\\n'\n",
      "                       'the increase of memory depth, 𝐿M, the model can '\n",
      "                       'achieve better perplexity over all sequence length. '\n",
      "                       'Also, deeper memory\\n'\n",
      "                       'modules are more robust to the sequence length when '\n",
      "                       'the model has less number of parameters. With the '\n",
      "                       'increase of the\\n'\n",
      "                       'number of parameters, all models show better '\n",
      "                       'performance on longer sequences.\\n'\n",
      "                       'Figure 8: The effect of memory depth on\\n'\n",
      "                       'training throughput\\n'\n",
      "                       'We also evaluate the effect of memory depth ( 𝐿M = '\n",
      "                       '1,2,3,4) on the training\\n'\n",
      "                       'throughput. We report the training throughput (the '\n",
      "                       'number of tokens per\\n'\n",
      "                       'second) as the function of sequence length in Figure '\n",
      "                       '8. All models scale linearly\\n'\n",
      "                       'with respect to the context length (i.e., constant '\n",
      "                       'trend in the number of tokens\\n'\n",
      "                       'per second with respect to sequence length). Also, by '\n",
      "                       'increasing the memory\\n'\n",
      "                       'depth, as expected, we can see a linear trend that a '\n",
      "                       'deeper memory results in\\n'\n",
      "                       'a slower training. Therefore, it is not always '\n",
      "                       'efficient to use deeper memory\\n'\n",
      "                       'modules, showing a trade-off between effectiveness and '\n",
      "                       'efficiency.\\n'\n",
      "                       '5.6 Time Series Forecasting\\n'\n",
      "                       'To show the effectiveness of our memory module in a '\n",
      "                       'broader tasks, we also evaluate its performance in '\n",
      "                       'time series\\n'\n",
      "                       'forecasting tasks. To this end, we use Simba framework '\n",
      "                       '(Patro and Agneeswaran 2024) for time series '\n",
      "                       'forecasting, and\\n'\n",
      "                       '2Note that, in this experiment, we only focus on the '\n",
      "                       'neural memory module to evaluate the effect of memory '\n",
      "                       'depth in the memorization process.\\n'\n",
      "                       'Combining neural memory with attention as we do in '\n",
      "                       'Titans variants, can additionally enhance the '\n",
      "                       'performance of the model over long sequences.\\n'\n",
      "                       '15\\n'\n",
      "                       'Table 4: Downstream evaluation of pre-trained DNA '\n",
      "                       'models on GenomicsBenchmarks (Grešová et al. 2023). We '\n",
      "                       'report\\n'\n",
      "                       'top-1 classification accuracy (%).\\n'\n",
      "                       'Model Enhancer Cohn Enhancer Ens Human Reg. Non-TATA '\n",
      "                       'Promoters Human OCR Ens.\\n'\n",
      "                       'CNN 69.5 68.9 93.3 84.6 68.0\\n'\n",
      "                       'DNABERT 74.0 85.7 88.1 85.6 75.1\\n'\n",
      "                       'GPT 70.5 83.5 91.5 87.7 73.0\\n'\n",
      "                       'HyenaDNA 74.2 89.2 93.8 96.6 80.9\\n'\n",
      "                       'Transformer++ 73.4 89.5 89.9 94.4 79.5\\n'\n",
      "                       'Mamba 73.0 - - 96.6 -\\n'\n",
      "                       'Based 74.6 89.5 89.5 96.8 79.0\\n'\n",
      "                       'Neural Memory Module 75.2 89.6 89.3 96.6 79.9\\n'\n",
      "                       'replace its Mamba module with our neural memory. We '\n",
      "                       'report the results on common time series forecasting '\n",
      "                       'benchmark\\n'\n",
      "                       'datasets–ETT, ECL, Traffic, and Weather (H. Zhou et '\n",
      "                       'al. 2021). The results are reported in Table 3. Our '\n",
      "                       'neural memory\\n'\n",
      "                       'module is outperforming all baselines, including '\n",
      "                       'Mamba-based, linear-based, and Transformer-based '\n",
      "                       'architectures.\\n'\n",
      "                       '5.7 DNA Modeling\\n'\n",
      "                       'In order to understand the capability of Titans beyond '\n",
      "                       'natural language, we further evaluate the performance '\n",
      "                       'of our\\n'\n",
      "                       'neural memory module on DNA modeling tasks. To this '\n",
      "                       'end, we evaluate pre-trained models on the downstream '\n",
      "                       'tasks\\n'\n",
      "                       'in GenomicsBenchmarks (Grešová et al. 2023). We follow '\n",
      "                       'the same experimental setups from Nguyen et al. '\n",
      "                       '(2024), and\\n'\n",
      "                       're-use the reported results of baselines by Arora et '\n",
      "                       'al. (2024). The performance of Titans (LMM) and '\n",
      "                       'baselines are reported\\n'\n",
      "                       'in Table 4. We find that LMM is competitive with '\n",
      "                       'state-of-the-art architectures across different '\n",
      "                       'downstream genomics\\n'\n",
      "                       'tasks.\\n'\n",
      "                       '5.8 Efficiency\\n'\n",
      "                       'Figure 9: Training throughput compari-\\n'\n",
      "                       'son of Titans and baselines.\\n'\n",
      "                       'In this part, we compare the efficiency of our neural '\n",
      "                       'memory as well as Titans\\n'\n",
      "                       'with state-of-the-art sequence models. The training '\n",
      "                       'throughput of models for\\n'\n",
      "                       'different sequence length ×batch size are reported in '\n",
      "                       'Figure 9. Comparing\\n'\n",
      "                       'recurrent models, including our neural memory module, '\n",
      "                       'we can see our memory\\n'\n",
      "                       'module is slightly slower than Mamba2 and Gated '\n",
      "                       'DeltaNet, mainly due to: (1)\\n'\n",
      "                       'having deep memory and more expressive transition '\n",
      "                       'process (memory update),\\n'\n",
      "                       'and (2) highly optimized kernel in the implementation '\n",
      "                       'of Mamba2. Interestingly,\\n'\n",
      "                       'Titans (MAL) are faster than baselines as well as the '\n",
      "                       'memory module. The\\n'\n",
      "                       'main reason for this better throughput is the highly '\n",
      "                       'optimized kernel of Flash-\\n'\n",
      "                       'Attention (Dao 2024), which is used for implementing '\n",
      "                       'SWA and full attention\\n'\n",
      "                       'module in Titans.\\n'\n",
      "                       '5.9 Ablation Study\\n'\n",
      "                       'Finally, we perform ablation studies on the different '\n",
      "                       'architectural choices in Titans. We consider our '\n",
      "                       'neural memory\\n'\n",
      "                       'module as a base model and then changing one component '\n",
      "                       'at a time: (1) replacing deep memory with linear '\n",
      "                       'memory,\\n'\n",
      "                       'removing (2) convolution, (3) momentum in the surprise '\n",
      "                       'measure, (4) weight decay (or forgot mechanism), and '\n",
      "                       '(5) persistent\\n'\n",
      "                       'memory. The results are reported in Table 5. All '\n",
      "                       'components of neural memory design are positively '\n",
      "                       'contributing to its\\n'\n",
      "                       'performance, where the greatest contribution comes '\n",
      "                       'from weight decay, momentum, convolution, and '\n",
      "                       'persistent memory,\\n'\n",
      "                       'respectively.\\n'\n",
      "                       'The Effect of Architectural Design. To evaluate the '\n",
      "                       'effect of architecture design, we compare the '\n",
      "                       'performance of three\\n'\n",
      "                       'represented variants of Titans in three aspects of (i) '\n",
      "                       'language modeling, (ii) commen-sense reasoning, and '\n",
      "                       '(iii) long context\\n'\n",
      "                       'NIAH (BABILong) tasks. The results are reported in '\n",
      "                       'Table 5. We find that MAC and MAG have close '\n",
      "                       'performance in\\n'\n",
      "                       'language modeling and common-sense reasoning tasks, '\n",
      "                       'while MAC achieve significantly better performance in '\n",
      "                       'long-context\\n'\n",
      "                       'NIAH. Both of these models achieve better performance '\n",
      "                       'than MAL. These results along with Figure 9, show a '\n",
      "                       'trade-off\\n'\n",
      "                       'between fast training and more expressive design.\\n'\n",
      "                       '16\\n'\n",
      "                       'Table 5: Ablation Study on Titans. All components of '\n",
      "                       'Titans are positively contributing to its '\n",
      "                       'performance.\\n'\n",
      "                       'Model Language Modeling Reasoning Long Context\\n'\n",
      "                       'ppl↓ acc↑ acc↑\\n'\n",
      "                       'LMM 27.01 47.83 92.68\\n'\n",
      "                       '+Attn(MAC) 26.67 48.65 97.95\\n'\n",
      "                       '+Attn(MAG) 25.70 48.60 96.70\\n'\n",
      "                       '+Attn(MAL) 25.91 47.87 96.91\\n'\n",
      "                       'Linear Memory 28.49 46.97 85.34\\n'\n",
      "                       'w/o Convolution 28.73 45.82 90.28\\n'\n",
      "                       'w/o Momentum 28.98 45.49 87.12\\n'\n",
      "                       'w/o Weight Decay 29.04 45.11 85.60\\n'\n",
      "                       'w/o Persistent Memory 27.63 46.35 92.49\\n'\n",
      "                       '6 Conclusion\\n'\n",
      "                       'In this paper, we present a neural long-term memory '\n",
      "                       'that, as a meta in-context learner, learns to memorize '\n",
      "                       'at test time.\\n'\n",
      "                       'The neural memory module is a recurrent model in '\n",
      "                       'nature, and is adaptively memorizing tokens that are '\n",
      "                       'more surprising\\n'\n",
      "                       'or are close to surprising tokens. Comparing to modern '\n",
      "                       'recurrent models, it has more expressive memory update '\n",
      "                       'and\\n'\n",
      "                       'storing mechanism. Using this memory, we present '\n",
      "                       'Titans architectures, and its three variants, in which '\n",
      "                       'we suggest to\\n'\n",
      "                       'incorporate the memory module as (1) a context, (2) '\n",
      "                       'gating, and (3) a layer. Our experimental evaluation '\n",
      "                       'on diverse tasks\\n'\n",
      "                       'tasks validate that Titans are more effective than '\n",
      "                       'Transformers and recent modern linear recurrent '\n",
      "                       'models, specifically for\\n'\n",
      "                       'long context. That is, Titans can scale to larger than '\n",
      "                       '2M context window size with better accuracy than '\n",
      "                       'baselines.\\n'\n",
      "                       'Titans are implemented in Pytorch and JAX and we '\n",
      "                       'intend to make the code we used to train and evaluate '\n",
      "                       'our models\\n'\n",
      "                       'available soon.\\n'\n",
      "                       '17\\n'\n",
      "                       'References\\n'\n",
      "                       '[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama '\n",
      "                       'Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo\\n'\n",
      "                       'Almeida, Janko Altenschmidt, Sam Altman, Shyamal '\n",
      "                       'Anadkat, et al. “Gpt-4 technical report”. In: arXiv '\n",
      "                       'preprint\\n'\n",
      "                       'arXiv:2303.08774 (2023).\\n'\n",
      "                       '[2] Yaroslav Aksenov, Nikita Balagansky, Sofia Maria '\n",
      "                       'Lo Cicero Vaina, Boris Shaposhnikov, Alexey '\n",
      "                       'Gorbatovski, and\\n'\n",
      "                       'Daniil Gavrilov. “Linear Transformers with Learnable '\n",
      "                       'Kernel Functions are Better In-Context Models”. In: '\n",
      "                       'arXiv\\n'\n",
      "                       'preprint arXiv:2402.10644 (2024).\\n'\n",
      "                       '[3] Marcin Andrychowicz, Misha Denil, Sergio Gomez, '\n",
      "                       'Matthew W Hoffman, David Pfau, Tom Schaul, Brendan\\n'\n",
      "                       'Shillingford, and Nando De Freitas. “Learning to learn '\n",
      "                       'by gradient descent by gradient descent”. In: Advances '\n",
      "                       'in\\n'\n",
      "                       'neural information processing systems 29 (2016).\\n'\n",
      "                       '[4] Cem Anil, Yuhuai Wu, Anders Andreassen, Aitor '\n",
      "                       'Lewkowycz, Vedant Misra, Vinay Ramasesh, Ambrose '\n",
      "                       'Slone,\\n'\n",
      "                       'Guy Gur-Ari, Ethan Dyer, and Behnam Neyshabur. '\n",
      "                       '“Exploring length generalization in large language '\n",
      "                       'models”. In:\\n'\n",
      "                       'Advances in Neural Information Processing Systems 35 '\n",
      "                       '(2022), pp. 38546–38556.\\n'\n",
      "                       '[5] Simran Arora, Sabri Eyuboglu, Michael Zhang, Aman '\n",
      "                       'Timalsina, Silas Alberti, James Zou, Atri Rudra, and '\n",
      "                       'Christo-\\n'\n",
      "                       'pher Re. “Simple linear attention language models '\n",
      "                       'balance the recall-throughput tradeoff”. '\n",
      "                       'In:Forty-first International\\n'\n",
      "                       'Conference on Machine Learning . 2024. url: '\n",
      "                       'https://openreview.net/forum?id=e93ffDcpH3.\\n'\n",
      "                       '[6] Dzmitry Bahdanau. “Neural machine translation by '\n",
      "                       'jointly learning to align and translate”. In: arXiv '\n",
      "                       'preprint\\n'\n",
      "                       'arXiv:1409.0473 (2014).\\n'\n",
      "                       '[7] Reza Bayat, Mohammad Pezeshki, Elvis Dohmatob, '\n",
      "                       'David Lopez-Paz, and Pascal Vincent. “The Pitfalls of '\n",
      "                       'Memo-\\n'\n",
      "                       'rization: When Memorization Hurts Generalization”. In: '\n",
      "                       'arXiv preprint arXiv:2412.07684 (2024).\\n'\n",
      "                       '[8] Maximilian Beck, Korbinian Pöppel, Markus '\n",
      "                       'Spanring, Andreas Auer, Oleksandra Prudnikova, Michael '\n",
      "                       'Kopp,\\n'\n",
      "                       'Günter Klambauer, Johannes Brandstetter, and Sepp '\n",
      "                       'Hochreiter. “xLSTM: Extended Long Short-Term Memory”. '\n",
      "                       'In:\\n'\n",
      "                       'arXiv preprint arXiv:2405.04517 (2024).\\n'\n",
      "                       '[9] Ali Behrouz, Michele Santacatterina, and Ramin '\n",
      "                       'Zabih. “Mambamixer: Efficient selective state space '\n",
      "                       'models with\\n'\n",
      "                       'dual token and channel selection”. In: arXiv preprint '\n",
      "                       'arXiv:2403.19888 (2024).\\n'\n",
      "                       '[10] Vincent-Pierre Berges, Barlas Oğuz, Daniel '\n",
      "                       'Haziza, Wen-tau Yih, Luke Zettlemoyer, and Gargi Gosh. '\n",
      "                       '“Memory\\n'\n",
      "                       'Layers at Scale”. In: arXiv preprint arXiv:2412.09764 '\n",
      "                       '(2024).\\n'\n",
      "                       '[11] Alberto Bietti, Vivien Cabannes, Diane '\n",
      "                       'Bouchacourt, Herve Jegou, and Leon Bottou. “Birth of a '\n",
      "                       'transformer: A\\n'\n",
      "                       'memory viewpoint”. In: Advances in Neural Information '\n",
      "                       'Processing Systems 36 (2024).\\n'\n",
      "                       '[12] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin '\n",
      "                       'Choi, et al. “Piqa: Reasoning about physical '\n",
      "                       'commonsense in\\n'\n",
      "                       'natural language”. In: Proceedings of the AAAI '\n",
      "                       'conference on artificial intelligence . Vol. 34. 05. '\n",
      "                       '2020, pp. 7432–7439.\\n'\n",
      "                       '[13] Aleksandar Botev, Soham De, Samuel L Smith, '\n",
      "                       'Anushan Fernando, George-Cristian Muraru, Ruba Haroun, '\n",
      "                       'Leonard\\n'\n",
      "                       'Berrada, Razvan Pascanu, Pier Giuseppe Sessa, Robert '\n",
      "                       'Dadashi, et al. “RecurrentGemma: Moving Past '\n",
      "                       'Transformers\\n'\n",
      "                       'for Efficient Open Language Models”. In: arXiv '\n",
      "                       'preprint arXiv:2404.07839 (2024).\\n'\n",
      "                       '[14] Léon Bottou and Vladimir Vapnik. “Local learning '\n",
      "                       'algorithms”. In: Neural computation 4.6 (1992), pp. '\n",
      "                       '888–900.\\n'\n",
      "                       '[15] Aydar Bulatov, Yuri Kuratov, Yermek Kapushev, and '\n",
      "                       'Mikhail S Burtsev. “Scaling transformer to 1m tokens '\n",
      "                       'and\\n'\n",
      "                       'beyond with rmt”. In: arXiv preprint arXiv:2304.11062 '\n",
      "                       '(2023).\\n'\n",
      "                       '[16] Aydar Bulatov, Yury Kuratov, and Mikhail Burtsev. '\n",
      "                       '“Recurrent memory transformer”. In: Advances in '\n",
      "                       'Neural\\n'\n",
      "                       'Information Processing Systems 35 (2022), pp. '\n",
      "                       '11079–11091.\\n'\n",
      "                       '[17] Edoardo Cetin, Qi Sun, Tianyu Zhao, and Yujin '\n",
      "                       'Tang. “An Evolved Universal Transformer Memory”. In: '\n",
      "                       'arXiv\\n'\n",
      "                       'preprint arXiv:2410.13166 (2024).\\n'\n",
      "                       '[18] Beidi Chen, Tri Dao, Eric Winsor, Zhao Song, Atri '\n",
      "                       'Rudra, and Christopher Ré. “Scatterbrain: Unifying '\n",
      "                       'sparse and\\n'\n",
      "                       'low-rank attention”. In: Advances in Neural '\n",
      "                       'Information Processing Systems 34 (2021), pp. '\n",
      "                       '17413–17426.\\n'\n",
      "                       '[19] Krzysztof Marcin Choromanski, Valerii '\n",
      "                       'Likhosherstov, David Dohan, Xingyou Song, Andreea '\n",
      "                       'Gane, Tamas Sarlos,\\n'\n",
      "                       'Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, '\n",
      "                       'Lukasz Kaiser, David Benjamin Belanger, Lucy J '\n",
      "                       'Colwell, and\\n'\n",
      "                       'Adrian Weller. “Rethinking Attention with Performers”. '\n",
      "                       'In: International Conference on Learning '\n",
      "                       'Representations .\\n'\n",
      "                       '2021. url: '\n",
      "                       'https://openreview.net/forum?id=Ua6zuk0WRH.\\n'\n",
      "                       '[20] Christopher Clark, Kenton Lee, Ming-Wei Chang, '\n",
      "                       'Tom Kwiatkowski, Michael Collins, and Kristina '\n",
      "                       'Toutanova.\\n'\n",
      "                       '“BoolQ: Exploring the Surprising Difficulty of Natural '\n",
      "                       'Yes/No Questions”. In: Proceedings of the 2019 '\n",
      "                       'Conference\\n'\n",
      "                       'of the North American Chapter of the Association for '\n",
      "                       'Computational Linguistics: Human Language '\n",
      "                       'Technologies,\\n'\n",
      "                       'Volume 1 (Long and Short Papers) . Ed. by Jill '\n",
      "                       'Burstein, Christy Doran, and Thamar Solorio. '\n",
      "                       'Minneapolis, Minnesota:\\n'\n",
      "                       'Association for Computational Linguistics, June 2019, '\n",
      "                       'pp. 2924–2936. doi: 10.18653/v1/N19-1300. url: https:\\n'\n",
      "                       '//aclanthology.org/N19-1300/.\\n'\n",
      "                       '18\\n'\n",
      "                       '[21] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar '\n",
      "                       'Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind '\n",
      "                       'Tafjord.\\n'\n",
      "                       '“Think you have solved question answering? try arc, '\n",
      "                       'the ai2 reasoning challenge”. In:arXiv preprint '\n",
      "                       'arXiv:1803.05457\\n'\n",
      "                       '(2018).\\n'\n",
      "                       '[22] Nelson Cowan. “What are the differences between '\n",
      "                       'long-term, short-term, and working memory?” In: '\n",
      "                       'Progress in\\n'\n",
      "                       'brain research 169 (2008), pp. 323–338.\\n'\n",
      "                       '[23] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G. '\n",
      "                       'Carbonell, Quoc Viet Le, and Ruslan Salakhutdinov. '\n",
      "                       '“Transformer-\\n'\n",
      "                       'XL: Attentive Language Models beyond a Fixed-Length '\n",
      "                       'Context”. In: ACL (1). Ed. by Anna Korhonen, David R.\\n'\n",
      "                       'Traum, and Lluís Màrquez. Association for '\n",
      "                       'Computational Linguistics, 2019, pp. 2978–2988.isbn: '\n",
      "                       '978-1-950737-48-2.\\n'\n",
      "                       '[24] Tri Dao. “FlashAttention-2: Faster Attention with '\n",
      "                       'Better Parallelism and Work Partitioning”. In: The '\n",
      "                       'Twelfth Inter-\\n'\n",
      "                       'national Conference on Learning Representations . '\n",
      "                       '2024. url: '\n",
      "                       'https://openreview.net/forum?id=mZn2Xyh9Ec.\\n'\n",
      "                       '[25] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and '\n",
      "                       'Christopher Ré. “FlashAttention: Fast and '\n",
      "                       'Memory-Efficient\\n'\n",
      "                       'Exact Attention with IO-Awareness”. In:Advances in '\n",
      "                       'Neural Information Processing Systems . Ed. by S. '\n",
      "                       'Koyejo, S.\\n'\n",
      "                       'Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh. '\n",
      "                       'Vol. 35. Curran Associates, Inc., 2022, pp. '\n",
      "                       '16344–16359. url:\\n'\n",
      "                       'https://proceedings.neurips.cc/paper_files/paper/2022/file/67d57c32e20fd0a7a302cb81d36e40d5-\\n'\n",
      "                       'Paper-Conference.pdf.\\n'\n",
      "                       '[26] Tri Dao and Albert Gu. “Transformers are SSMs: '\n",
      "                       'Generalized models and efficient algorithms through '\n",
      "                       'structured\\n'\n",
      "                       'state space duality”. In: arXiv preprint '\n",
      "                       'arXiv:2405.21060 (2024).\\n'\n",
      "                       '[27] Abhimanyu Das, Weihao Kong, Andrew Leach, Shaan K '\n",
      "                       'Mathur, Rajat Sen, and Rose Yu. “Long-term '\n",
      "                       'Forecasting\\n'\n",
      "                       'with TiDE: Time-series Dense Encoder”. In: '\n",
      "                       'Transactions on Machine Learning Research (2023). '\n",
      "                       'issn: 2835-8856. url:\\n'\n",
      "                       'https://openreview.net/forum?id=pCbC3aQB5W.\\n'\n",
      "                       '[28] Soham De, Samuel L Smith, Anushan Fernando, '\n",
      "                       'Aleksandar Botev, George Cristian-Muraru, Albert Gu, '\n",
      "                       'Ruba\\n'\n",
      "                       'Haroun, Leonard Berrada, Yutian Chen, Srivatsan '\n",
      "                       'Srinivasan, et al. “Griffin: Mixing gated linear '\n",
      "                       'recurrences with\\n'\n",
      "                       'local attention for efficient language models”. In: '\n",
      "                       'arXiv preprint arXiv:2402.19427 (2024).\\n'\n",
      "                       '[29] Juechu Dong, Boyuan Feng, Driss Guessous, Yanbo '\n",
      "                       'Liang, and Horace He. “Flex Attention: A Programming '\n",
      "                       'Model\\n'\n",
      "                       'for Generating Optimized Attention Kernels”. In: arXiv '\n",
      "                       'preprint arXiv:2412.05496 (2024).\\n'\n",
      "                       '[30] Xin Dong, Yonggan Fu, Shizhe Diao, Wonmin Byeon, '\n",
      "                       'Zijia Chen, Ameya Sunil Mahabaleshwarkar, Shih-Yang '\n",
      "                       'Liu,\\n'\n",
      "                       'Matthijs Van Keirsbilck, Min-Hung Chen, Yoshi Suhara, '\n",
      "                       'et al. “Hymba: A Hybrid-head Architecture for Small\\n'\n",
      "                       'Language Models”. In: arXiv preprint arXiv:2411.13676 '\n",
      "                       '(2024).\\n'\n",
      "                       '[31] Stefan Elfwing, Eiji Uchibe, and Kenji Doya. '\n",
      "                       '“Sigmoid-weighted linear units for neural network '\n",
      "                       'function approxi-\\n'\n",
      "                       'mation in reinforcement learning”. In: Neural networks '\n",
      "                       '107 (2018), pp. 3–11.\\n'\n",
      "                       '[32] Yukun Feng, Feng Li, Ziang Song, Boyuan Zheng, '\n",
      "                       'and Philipp Koehn. “Learn to remember: Transformer '\n",
      "                       'with\\n'\n",
      "                       'recurrent memory for document-level machine '\n",
      "                       'translation”. In: arXiv preprint arXiv:2205.01546 '\n",
      "                       '(2022).\\n'\n",
      "                       '[33] Daniel Y Fu, Tri Dao, Khaled Kamal Saab, Armin W '\n",
      "                       'Thomas, Atri Rudra, and Christopher Re. “Hungry '\n",
      "                       'Hungry\\n'\n",
      "                       'Hippos: Towards Language Modeling with State Space '\n",
      "                       'Models”. In:The Eleventh International Conference on '\n",
      "                       'Learning\\n'\n",
      "                       'Representations. 2023. url: '\n",
      "                       'https://openreview.net/forum?id=COZDy0WYGg.\\n'\n",
      "                       '[34] Yossi Gandelsman, Yu Sun, Xinlei Chen, and Alexei '\n",
      "                       'Efros. “Test-time training with masked autoencoders”. '\n",
      "                       'In:\\n'\n",
      "                       'Advances in Neural Information Processing Systems 35 '\n",
      "                       '(2022), pp. 29374–29385.\\n'\n",
      "                       '[35] Leo Gao, Stella Biderman, Sid Black, Laurence '\n",
      "                       'Golding, Travis Hoppe, Charles Foster, Jason Phang, '\n",
      "                       'Horace He,\\n'\n",
      "                       'Anish Thite, Noa Nabeshima, et al. “The pile: An 800gb '\n",
      "                       'dataset of diverse text for language modeling”. In: '\n",
      "                       'arXiv\\n'\n",
      "                       'preprint arXiv:2101.00027 (2020).\\n'\n",
      "                       '[36] Felix A Gers, Jürgen Schmidhuber, and Fred '\n",
      "                       'Cummins. “Learning to forget: Continual prediction '\n",
      "                       'with LSTM”. In:\\n'\n",
      "                       'Neural computation 12.10 (2000), pp. 2451–2471.\\n'\n",
      "                       '[37] Alex Graves, Greg Wayne, and Ivo Danihelka. '\n",
      "                       'Neural Turing Machines . 2014. arXiv: 1410.5401 '\n",
      "                       '[cs.NE]. url:\\n'\n",
      "                       'https://arxiv.org/abs/1410.5401.\\n'\n",
      "                       '[38] Klaus Greff, Rupesh K Srivastava, Jan Koutník, '\n",
      "                       'Bas R Steunebrink, and Jürgen Schmidhuber. “LSTM: A '\n",
      "                       'search space\\n'\n",
      "                       'odyssey”. In: IEEE transactions on neural networks and '\n",
      "                       'learning systems 28.10 (2016), pp. 2222–2232.\\n'\n",
      "                       '[39] Katarína Grešová, Vlastimil Martinek, David '\n",
      "                       'Čechák, Petr Šimeček, and Panagiotis Alexiou. “Genomic '\n",
      "                       'benchmarks:\\n'\n",
      "                       'a collection of datasets for genomic sequence '\n",
      "                       'classification”. In: BMC Genomic Data 24.1 (2023), p. '\n",
      "                       '25.\\n'\n",
      "                       '[40] Albert Gu and Tri Dao. “Mamba: Linear-Time '\n",
      "                       'Sequence Modeling with Selective State Spaces”. In: '\n",
      "                       'First Conference\\n'\n",
      "                       'on Language Modeling . 2024. url: '\n",
      "                       'https://openreview.net/forum?id=tEYskw1VY2.\\n'\n",
      "                       '[41] Albert Gu, Karan Goel, and Christopher Re. '\n",
      "                       '“Efficiently Modeling Long Sequences with Structured '\n",
      "                       'State Spaces”.\\n'\n",
      "                       'In: International Conference on Learning '\n",
      "                       'Representations . 2022. url: https : / / openreview . '\n",
      "                       'net / forum ? id =\\n'\n",
      "                       'uYLFoz1vlAC.\\n'\n",
      "                       '19\\n'\n",
      "                       '[42] Chi Han, Qifan Wang, Hao Peng, Wenhan Xiong, Yu '\n",
      "                       'Chen, Heng Ji, and Sinong Wang. “LM-Infinite: '\n",
      "                       'Zero-Shot\\n'\n",
      "                       'Extreme Length Generalization for Large Language '\n",
      "                       'Models”. In: Proceedings of the 2024 Conference of the '\n",
      "                       'North\\n'\n",
      "                       'American Chapter of the Association for Computational '\n",
      "                       'Linguistics: Human Language Technologies (Volume 1: '\n",
      "                       'Long\\n'\n",
      "                       'Papers). Ed. by Kevin Duh, Helena Gomez, and Steven '\n",
      "                       'Bethard. Mexico City, Mexico: Association for '\n",
      "                       'Computational\\n'\n",
      "                       'Linguistics, June 2024, pp. 3991–4008. doi: '\n",
      "                       '10.18653/v1/2024.naacl-long.222. url: '\n",
      "                       'https://aclanthology.\\n'\n",
      "                       'org/2024.naacl-long.222.\\n'\n",
      "                       '[43] Ramin Hasani, Mathias Lechner, Tsun-Hsuan Wang, '\n",
      "                       'Makram Chahine, Alexander Amini, and Daniela Rus. '\n",
      "                       '“Liquid\\n'\n",
      "                       'Structural State-Space Models”. In: The Eleventh '\n",
      "                       'International Conference on Learning Representations . '\n",
      "                       '2023. url:\\n'\n",
      "                       'https://openreview.net/forum?id=g4OTKRKfS7R.\\n'\n",
      "                       '[44] Zexue He, Leonid Karlinsky, Donghyun Kim, Julian '\n",
      "                       'McAuley, Dmitry Krotov, and Rogerio Feris. “CAMELoT:\\n'\n",
      "                       'Towards Large Language Models with Training-Free '\n",
      "                       'Consolidated Associative Memory”. In: arXiv preprint\\n'\n",
      "                       'arXiv:2402.13449 (2024).\\n'\n",
      "                       '[45] Donald Olding Hebb. The organization of behavior: '\n",
      "                       'A neuropsychological theory . Psychology press, 2005.\\n'\n",
      "                       '[46] John J Hopfield. “Neural networks and physical '\n",
      "                       'systems with emergent collective computational '\n",
      "                       'abilities.” In:\\n'\n",
      "                       'Proceedings of the national academy of sciences 79.8 '\n",
      "                       '(1982), pp. 2554–2558.\\n'\n",
      "                       '[47] Kurt Hornik, Maxwell Stinchcombe, and Halbert '\n",
      "                       'White. “Multilayer feedforward networks are universal '\n",
      "                       'approxi-\\n'\n",
      "                       'mators”. In: Neural networks 2.5 (1989), pp. 359–366.\\n'\n",
      "                       '[48] Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, '\n",
      "                       'Shantanu Acharya, Dima Rekesh, Fei Jia, and Boris '\n",
      "                       'Ginsburg.\\n'\n",
      "                       '“RULER: What’s the Real Context Size of Your '\n",
      "                       'Long-Context Language Models?” In: First Conference on '\n",
      "                       'Language\\n'\n",
      "                       'Modeling. 2024. url: '\n",
      "                       'https://openreview.net/forum?id=kIoBbc76Sy.\\n'\n",
      "                       '[49] DeLesley Hutchins, Imanol Schlag, Yuhuai Wu, '\n",
      "                       'Ethan Dyer, and Behnam Neyshabur. “Block-recurrent '\n",
      "                       'transformers”.\\n'\n",
      "                       'In: Advances in neural information processing systems '\n",
      "                       '35 (2022), pp. 33248–33261.\\n'\n",
      "                       '[50] Kazuki Irie, Róbert Csordás, and Jürgen '\n",
      "                       'Schmidhuber. “The dual form of neural networks '\n",
      "                       'revisited: Connecting test\\n'\n",
      "                       'time predictions to training patterns via spotlights '\n",
      "                       'of attention”. In: International Conference on Machine '\n",
      "                       'Learning .\\n'\n",
      "                       'PMLR. 2022, pp. 9639–9659.\\n'\n",
      "                       '[51] Kazuki Irie, Imanol Schlag, Róbert Csordás, and '\n",
      "                       'Jürgen Schmidhuber. “Going beyond linear transformers '\n",
      "                       'with\\n'\n",
      "                       'recurrent fast weight programmers”. In: Advances in '\n",
      "                       'neural information processing systems 34 (2021), pp. '\n",
      "                       '7703–7717.\\n'\n",
      "                       '[52] Vidit Jain and Erik Learned-Miller. “Online '\n",
      "                       'domain adaptation of a pre-trained cascade of '\n",
      "                       'classifiers”. In: CVPR\\n'\n",
      "                       '2011. IEEE. 2011, pp. 577–584.\\n'\n",
      "                       '[53] Albert Q Jiang, Alexandre Sablayrolles, Arthur '\n",
      "                       'Mensch, Chris Bamford, Devendra Singh Chaplot, Diego '\n",
      "                       'de las\\n'\n",
      "                       'Casas, Florian Bressand, Gianna Lengyel, Guillaume '\n",
      "                       'Lample, Lucile Saulnier, et al. “Mistral 7B”. In: '\n",
      "                       'arXiv preprint\\n'\n",
      "                       'arXiv:2310.06825 (2023).\\n'\n",
      "                       '[54] Praneeth Kacham, Vahab Mirrokni, and Peilin '\n",
      "                       'Zhong. “PolySketchFormer: Fast Transformers via '\n",
      "                       'Sketching Polyno-\\n'\n",
      "                       'mial Kernels”. In: Forty-first International '\n",
      "                       'Conference on Machine Learning . 2024. url: '\n",
      "                       'https://openreview.net/\\n'\n",
      "                       'forum?id=ghYrfdJfjK.\\n'\n",
      "                       '[55] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B '\n",
      "                       'Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec\\n'\n",
      "                       'Radford, Jeffrey Wu, and Dario Amodei. “Scaling laws '\n",
      "                       'for neural language models”. In:arXiv preprint '\n",
      "                       'arXiv:2001.08361\\n'\n",
      "                       '(2020).\\n'\n",
      "                       '[56] Angelos Katharopoulos, Apoorv Vyas, Nikolaos '\n",
      "                       'Pappas, and François Fleuret. “Transformers are rnns: '\n",
      "                       'Fast au-\\n'\n",
      "                       'toregressive transformers with linear attention”. In: '\n",
      "                       'International conference on machine learning . PMLR. '\n",
      "                       '2020,\\n'\n",
      "                       'pp. 5156–5165.\\n'\n",
      "                       '[57] Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke '\n",
      "                       'Zettlemoyer, and Mike Lewis. “Generalization through\\n'\n",
      "                       'Memorization: Nearest Neighbor Language Models”. In: '\n",
      "                       'International Conference on Learning Representations . '\n",
      "                       '2020.\\n'\n",
      "                       'url: https://openreview.net/forum?id=HklBjCEKvH.\\n'\n",
      "                       '[58] Yuri Kuratov, Aydar Bulatov, Petr Anokhin, Ivan '\n",
      "                       'Rodkin, Dmitry Igorevich Sorokin, Artyom Sorokin, and '\n",
      "                       'Mikhail\\n'\n",
      "                       'Burtsev. “BABILong: Testing the Limits of LLMs with '\n",
      "                       'Long Context Reasoning-in-a-Haystack”. In: The '\n",
      "                       'Thirty-\\n'\n",
      "                       'eight Conference on Neural Information Processing '\n",
      "                       'Systems Datasets and Benchmarks Track . 2024. url: '\n",
      "                       'https:\\n'\n",
      "                       '//openreview.net/forum?id=u7m2CG84BQ.\\n'\n",
      "                       '[59] Hung Le, Truyen Tran, and Svetha Venkatesh. '\n",
      "                       '“Self-attentive associative memory”. In:International '\n",
      "                       'conference on\\n'\n",
      "                       'machine learning . PMLR. 2020, pp. 5682–5691.\\n'\n",
      "                       '[60] Patrick Lewis, Ethan Perez, Aleksandra Piktus, '\n",
      "                       'Fabio Petroni, Vladimir Karpukhin, Naman Goyal, '\n",
      "                       'Heinrich Küttler,\\n'\n",
      "                       'Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. '\n",
      "                       '“Retrieval-augmented generation for '\n",
      "                       'knowledge-intensive nlp\\n'\n",
      "                       'tasks”. In: Advances in Neural Information Processing '\n",
      "                       'Systems 33 (2020), pp. 9459–9474.\\n'\n",
      "                       '20\\n'\n",
      "                       '[61] Danny Leybzon and Corentin Kervadec. “Learning, '\n",
      "                       'Forgetting, Remembering: Insights From Tracking LLM '\n",
      "                       'Mem-\\n'\n",
      "                       'orization During Training”. In: Proceedings of the 7th '\n",
      "                       'BlackboxNLP Workshop: Analyzing and Interpreting '\n",
      "                       'Neural\\n'\n",
      "                       'Networks for NLP . 2024, pp. 43–57.\\n'\n",
      "                       '[62] Zhe Li, Shiyi Qi, Yiduo Li, and Zenglin Xu. '\n",
      "                       '“Revisiting long-term time series forecasting: An '\n",
      "                       'investigation on linear\\n'\n",
      "                       'mapping”. In: arXiv preprint arXiv:2305.10721 (2023).\\n'\n",
      "                       '[63] Bo Liu, Rui Wang, Lemeng Wu, Yihao Feng, Peter '\n",
      "                       'Stone, and Qiang Liu. “Longhorn: State space models '\n",
      "                       'are amortized\\n'\n",
      "                       'online learners”. In: arXiv preprint arXiv:2407.14207 '\n",
      "                       '(2024).\\n'\n",
      "                       '[64] Nelson F Liu, Kevin Lin, John Hewitt, Ashwin '\n",
      "                       'Paranjape, Michele Bevilacqua, Fabio Petroni, and '\n",
      "                       'Percy Liang.\\n'\n",
      "                       '“Lost in the middle: How language models use long '\n",
      "                       'contexts”. In: Transactions of the Association for '\n",
      "                       'Computational\\n'\n",
      "                       'Linguistics 12 (2024), pp. 157–173.\\n'\n",
      "                       '[65] Yong Liu, Tengge Hu, Haoran Zhang, Haixu Wu, '\n",
      "                       'Shiyu Wang, Lintao Ma, and Mingsheng Long. '\n",
      "                       '“itransformer:\\n'\n",
      "                       'Inverted transformers are effective for time series '\n",
      "                       'forecasting”. In: arXiv preprint arXiv:2310.06625 '\n",
      "                       '(2023).\\n'\n",
      "                       '[66] George Mandler. “The structure of value: '\n",
      "                       'Accounting for taste”. In: Affect and cognition . '\n",
      "                       'Psychology Press, 2014,\\n'\n",
      "                       'pp. 3–36.\\n'\n",
      "                       '[67] Harsh Mehta, Ankit Gupta, Ashok Cutkosky, and '\n",
      "                       'Behnam Neyshabur. “Long Range Language Modeling via\\n'\n",
      "                       'Gated State Spaces”. In: The Eleventh International '\n",
      "                       'Conference on Learning Representations . 2023. url: '\n",
      "                       'https:\\n'\n",
      "                       '//openreview.net/forum?id=5MkYIYCbva.\\n'\n",
      "                       '[68] Stephen Merity, Caiming Xiong, James Bradbury, '\n",
      "                       'and Richard Socher. “Pointer Sentinel Mixture Models”. '\n",
      "                       'In:\\n'\n",
      "                       'International Conference on Learning Representations . '\n",
      "                       '2017. url: https://openreview.net/forum?id=Byj72udxe.\\n'\n",
      "                       '[69] William Merrill, Jackson Petty, and Ashish '\n",
      "                       'Sabharwal. “The Illusion of State in State-Space '\n",
      "                       'Models”. In: Forty-first\\n'\n",
      "                       'International Conference on Machine Learning . 2024. '\n",
      "                       'url: https://openreview.net/forum?id=QZgo9JZpLq.\\n'\n",
      "                       '[70] Ravi Teja Mullapudi, Steven Chen, Keyi Zhang, '\n",
      "                       'Deva Ramanan, and Kayvon Fatahalian. “Online model '\n",
      "                       'distillation\\n'\n",
      "                       'for efficient video inference”. In: Proceedings of the '\n",
      "                       'IEEE/CVF International conference on computer vision . '\n",
      "                       '2019,\\n'\n",
      "                       'pp. 3573–3582.\\n'\n",
      "                       '[71] Tsendsuren Munkhdalai, Manaal Faruqui, and '\n",
      "                       'Siddharth Gopal. “Leave no context behind: Efficient '\n",
      "                       'infinite context\\n'\n",
      "                       'transformers with infini-attention”. In: arXiv '\n",
      "                       'preprint arXiv:2404.07143 (2024).\\n'\n",
      "                       '[72] Tsendsuren Munkhdalai, Alessandro Sordoni, Tong '\n",
      "                       'Wang, and Adam Trischler. “Metalearned neural memory”. '\n",
      "                       'In:\\n'\n",
      "                       'Advances in Neural Information Processing Systems 32 '\n",
      "                       '(2019).\\n'\n",
      "                       '[73] Tsendsuren Munkhdalai and Hong Yu. “Neural '\n",
      "                       'semantic encoders”. In: Proceedings of the conference. '\n",
      "                       'Association for\\n'\n",
      "                       'Computational Linguistics. Meeting . Vol. 1. NIH '\n",
      "                       'Public Access. 2017, p. 397.\\n'\n",
      "                       '[74] Eric Nguyen, Michael Poli, Marjan Faizi, Armin '\n",
      "                       'Thomas, Michael Wornow, Callum Birch-Sykes, Stefano '\n",
      "                       'Massaroli,\\n'\n",
      "                       'Aman Patel, Clayton Rabideau, Yoshua Bengio, et al. '\n",
      "                       '“Hyenadna: Long-range genomic sequence modeling at '\n",
      "                       'single\\n'\n",
      "                       'nucleotide resolution”. In: Advances in neural '\n",
      "                       'information processing systems 36 (2024).\\n'\n",
      "                       '[75] A Nichol. “On first-order meta-learning '\n",
      "                       'algorithms”. In: arXiv preprint arXiv:1803.02999 '\n",
      "                       '(2018).\\n'\n",
      "                       '[76] Yuqi Nie, Nam H Nguyen, Phanwadee Sinthong, and '\n",
      "                       'Jayant Kalagnanam. “A time series is worth 64 words:\\n'\n",
      "                       'Long-term forecasting with transformers”. In: arXiv '\n",
      "                       'preprint arXiv:2211.14730 (2022).\\n'\n",
      "                       '[77] Hideyuki Okano, Tomoo Hirano, and Evan Balaban. '\n",
      "                       '“Learning and memory”. In:Proceedings of the National '\n",
      "                       'Academy\\n'\n",
      "                       'of Sciences 97.23 (2000), pp. 12403–12404.\\n'\n",
      "                       '[78] Antonio Orvieto, Samuel L Smith, Albert Gu, '\n",
      "                       'Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and '\n",
      "                       'Soham De.\\n'\n",
      "                       '“Resurrecting recurrent neural networks for long '\n",
      "                       'sequences”. In: International Conference on Machine '\n",
      "                       'Learning .\\n'\n",
      "                       'PMLR. 2023, pp. 26670–26698.\\n'\n",
      "                       '[79] Denis Paperno, Germán Kruszewski, Angeliki '\n",
      "                       'Lazaridou, Ngoc Quan Pham, Raffaella Bernardi, Sandro '\n",
      "                       'Pezzelle,\\n'\n",
      "                       'Marco Baroni, Gemma Boleda, and Raquel Fernández. “The '\n",
      "                       'LAMBADA dataset: Word prediction requiring a broad\\n'\n",
      "                       'discourse context”. In:Proceedings of the 54th Annual '\n",
      "                       'Meeting of the Association for Computational '\n",
      "                       'Linguistics (Volume\\n'\n",
      "                       '1: Long Papers) . Ed. by Katrin Erk and Noah A. Smith. '\n",
      "                       'Berlin, Germany: Association for Computational '\n",
      "                       'Linguistics,\\n'\n",
      "                       'Aug. 2016, pp. 1525–1534. doi: 10.18653/v1/P16-1144. '\n",
      "                       'url: https://aclanthology.org/P16-1144/.\\n'\n",
      "                       '[80] Badri N. Patro and Vijay S. Agneeswaran. SiMBA: '\n",
      "                       'Simplified Mamba-Based Architecture for Vision and '\n",
      "                       'Multivariate\\n'\n",
      "                       'Time series . 2024. arXiv: 2403.15360 [cs.CV].\\n'\n",
      "                       '[81] Guilherme Penedo, Hynek Kydlíček, Loubna Ben '\n",
      "                       'allal, Anton Lozhkov, Margaret Mitchell, Colin Raffel, '\n",
      "                       'Leandro\\n'\n",
      "                       'Von Werra, and Thomas Wolf. “The FineWeb Datasets: '\n",
      "                       'Decanting the Web for the Finest Text Data at Scale”. '\n",
      "                       'In:\\n'\n",
      "                       'The Thirty-eight Conference on Neural Information '\n",
      "                       'Processing Systems Datasets and Benchmarks Track . '\n",
      "                       '2024. url:\\n'\n",
      "                       'https://openreview.net/forum?id=n6SCkn2QaG.\\n'\n",
      "                       '[82] Bo Peng. RWKV-LM. Version 1.0.0. Aug. 2021. doi: '\n",
      "                       '10.5281/zenodo.5196577 . url: https://github.com/\\n'\n",
      "                       'BlinkDL/RWKV-LM.\\n'\n",
      "                       '21\\n'\n",
      "                       '[83] Bo Peng, Eric Alcaide, Quentin Gregory Anthony, '\n",
      "                       'Alon Albalak, Samuel Arcadinho, Stella Biderman, '\n",
      "                       'Huanqi Cao,\\n'\n",
      "                       'Xin Cheng, Michael Nguyen Chung, Leon Derczynski, '\n",
      "                       'Xingjian Du, Matteo Grella, Kranthi Kiran GV, Xuzheng '\n",
      "                       'He,\\n'\n",
      "                       'Haowen Hou, Przemyslaw Kazienko, Jan Kocon, Jiaming '\n",
      "                       'Kong, Bartłomiej Koptyra, Hayden Lau, Jiaju Lin, '\n",
      "                       'Krishna\\n'\n",
      "                       'Sri Ipsit Mantri, Ferdinand Mom, Atsushi Saito, '\n",
      "                       'Guangyu Song, Xiangru Tang, Johan S. Wind, Stanisław '\n",
      "                       'Woźniak,\\n'\n",
      "                       'Zhenyuan Zhang, Qinghua Zhou, Jian Zhu, and Rui-Jie '\n",
      "                       'Zhu. “RWKV: Reinventing RNNs for the Transformer '\n",
      "                       'Era”.\\n'\n",
      "                       'In: The 2023 Conference on Empirical Methods in '\n",
      "                       'Natural Language Processing . 2023. url: '\n",
      "                       'https://openreview.\\n'\n",
      "                       'net/forum?id=7SaXczaBpG.\\n'\n",
      "                       '[84] Bo Peng, Daniel Goldstein, Quentin Anthony, Alon '\n",
      "                       'Albalak, Eric Alcaide, Stella Biderman, Eugene Cheah, '\n",
      "                       'Xingjian\\n'\n",
      "                       'Du, Teddy Ferdinan, Haowen Hou, et al. “Eagle and '\n",
      "                       'finch: Rwkv with matrix-valued states and dynamic '\n",
      "                       'recurrence”.\\n'\n",
      "                       'In: arXiv preprint arXiv:2404.05892 (2024).\\n'\n",
      "                       '[85] DL Prados and SC Kak. “Neural network capacity '\n",
      "                       'using delta rule”. In: Electronics Letters 25.3 '\n",
      "                       '(1989), pp. 197–199.\\n'\n",
      "                       '[86] Zhen Qin, Yiran Zhong, and Hui Deng. “Exploring '\n",
      "                       'Transformer Extrapolation”. In: Proceedings of the '\n",
      "                       'AAAI\\n'\n",
      "                       'Conference on Artificial Intelligence . Vol. 38. 17. '\n",
      "                       '2024, pp. 18897–18905.\\n'\n",
      "                       '[87] Liliang Ren, Yang Liu, Yadong Lu, Yelong Shen, '\n",
      "                       'Chen Liang, and Weizhu Chen. “Samba: Simple Hybrid '\n",
      "                       'State Space\\n'\n",
      "                       'Models for Efficient Unlimited Context Language '\n",
      "                       'Modeling”. In: arXiv preprint arXiv:2406.07522 '\n",
      "                       '(2024).\\n'\n",
      "                       '[88] Ivan Rodkin, Yuri Kuratov, Aydar Bulatov, and '\n",
      "                       'Mikhail Burtsev. “Associative recurrent memory '\n",
      "                       'transformer”. In:\\n'\n",
      "                       'arXiv preprint arXiv:2407.04841 (2024).\\n'\n",
      "                       '[89] Aurko Roy, Mohammad Saffar, Ashish Vaswani, and '\n",
      "                       'David Grangier. “Efficient content-based sparse '\n",
      "                       'attention with\\n'\n",
      "                       'routing transformers”. In: Transactions of the '\n",
      "                       'Association for Computational Linguistics 9 (2021), '\n",
      "                       'pp. 53–68.\\n'\n",
      "                       '[90] Keisuke Sakaguchi, Ronan Le Bras, Chandra '\n",
      "                       'Bhagavatula, and Yejin Choi. “Winogrande: An '\n",
      "                       'adversarial winograd\\n'\n",
      "                       'schema challenge at scale”. In: Communications of the '\n",
      "                       'ACM 64.9 (2021), pp. 99–106.\\n'\n",
      "                       '[91] Maarten Sap, Hannah Rashkin, Derek Chen, Ronan Le '\n",
      "                       'Bras, and Yejin Choi. “Social IQa: Commonsense '\n",
      "                       'Reasoning\\n'\n",
      "                       'about Social Interactions”. In:Proceedings of the 2019 '\n",
      "                       'Conference on Empirical Methods in Natural Language '\n",
      "                       'Processing\\n'\n",
      "                       'and the 9th International Joint Conference on Natural '\n",
      "                       'Language Processing (EMNLP-IJCNLP) . Ed. by Kentaro '\n",
      "                       'Inui,\\n'\n",
      "                       'Jing Jiang, Vincent Ng, and Xiaojun Wan. Hong Kong, '\n",
      "                       'China: Association for Computational Linguistics, Nov. '\n",
      "                       '2019,\\n'\n",
      "                       'pp. 4463–4473. doi: 10.18653/v1/D19-1454. url: '\n",
      "                       'https://aclanthology.org/D19-1454/.\\n'\n",
      "                       '[92] Imanol Schlag, Kazuki Irie, and Jürgen '\n",
      "                       'Schmidhuber. “Linear transformers are secretly fast '\n",
      "                       'weight programmers”.\\n'\n",
      "                       'In: International Conference on Machine Learning . '\n",
      "                       'PMLR. 2021, pp. 9355–9366.\\n'\n",
      "                       '[93] JH Schmidhuber. “Learning to control fast-weight '\n",
      "                       'memories: An alternative to recurrent nets. Accepted '\n",
      "                       'for\\n'\n",
      "                       'publication in”. In: Neural Computation (1992).\\n'\n",
      "                       '[94] Jürgen Schmidhuber. “Reducing the ratio between '\n",
      "                       'learning complexity and number of time varying '\n",
      "                       'variables\\n'\n",
      "                       'in fully recurrent nets”. In: ICANN’93: Proceedings of '\n",
      "                       'the International Conference on Artificial Neural '\n",
      "                       'Networks\\n'\n",
      "                       'Amsterdam, The Netherlands 13–16 September 1993 3 . '\n",
      "                       'Springer. 1993, pp. 460–463.\\n'\n",
      "                       '[95] Jürgen Schmidhuber and Sepp Hochreiter. “Long '\n",
      "                       'Short-term Memory”. In: Neural Computation MIT-Press '\n",
      "                       '(1997).\\n'\n",
      "                       '[96] Avi Schwarzschild, Zhili Feng, Pratyush Maini, '\n",
      "                       'Zachary C Lipton, and J Zico Kolter. “Rethinking llm '\n",
      "                       'memorization\\n'\n",
      "                       'through the lens of adversarial compression”. In: '\n",
      "                       'arXiv preprint arXiv:2404.15146 (2024).\\n'\n",
      "                       '[97] Jimmy T.H. Smith, Andrew Warrington, and Scott '\n",
      "                       'Linderman. “Simplified State Space Layers for Sequence '\n",
      "                       'Modeling”.\\n'\n",
      "                       'In: The Eleventh International Conference on Learning '\n",
      "                       'Representations . 2023. url: '\n",
      "                       'https://openreview.net/forum?\\n'\n",
      "                       'id=Ai8Hw3AXqks.\\n'\n",
      "                       '[98] Robin Staab, Mark Vero, Mislav Balunovic, and '\n",
      "                       'Martin Vechev. “Beyond Memorization: Violating Privacy '\n",
      "                       'via\\n'\n",
      "                       'Inference with Large Language Models”. In: The Twelfth '\n",
      "                       'International Conference on Learning Representations . '\n",
      "                       '2024.\\n'\n",
      "                       'url: https://openreview.net/forum?id=kmn0BhQk7p.\\n'\n",
      "                       '[99] Sainbayar Sukhbaatar, Edouard Grave, Guillaume '\n",
      "                       'Lample, Herve Jegou, and Armand Joulin. “Augmenting '\n",
      "                       'self-\\n'\n",
      "                       'attention with persistent memory”. In: arXiv preprint '\n",
      "                       'arXiv:1907.01470 (2019).\\n'\n",
      "                       '[100] Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, '\n",
      "                       'et al. “End-to-end memory networks”. In: Advances in '\n",
      "                       'neural\\n'\n",
      "                       'information processing systems 28 (2015).\\n'\n",
      "                       '[101] Yu Sun, Xinhao Li, Karan Dalal, Jiarui Xu, Arjun '\n",
      "                       'Vikram, Genghan Zhang, Yann Dubois, Xinlei Chen, '\n",
      "                       'Xiaolong\\n'\n",
      "                       'Wang, Sanmi Koyejo, et al. “Learning to (learn at test '\n",
      "                       'time): Rnns with expressive hidden states”. In: arXiv '\n",
      "                       'preprint\\n'\n",
      "                       'arXiv:2407.04620 (2024).\\n'\n",
      "                       '[102] Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, '\n",
      "                       'Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei. '\n",
      "                       '“Retentive\\n'\n",
      "                       'network: A successor to transformer for large language '\n",
      "                       'models”. In: arXiv preprint arXiv:2307.08621 (2023).\\n'\n",
      "                       '[103] Gemma Team, Thomas Mesnard, Cassidy Hardin, '\n",
      "                       'Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, '\n",
      "                       'Laurent Sifre,\\n'\n",
      "                       'Morgane Rivière, Mihir Sanjay Kale, Juliette Love, et '\n",
      "                       'al. “Gemma: Open models based on gemini research and\\n'\n",
      "                       'technology”. In: arXiv preprint arXiv:2403.08295 '\n",
      "                       '(2024).\\n'\n",
      "                       '22\\n'\n",
      "                       '[104] W Scott Terry. Learning and memory: Basic '\n",
      "                       'principles, processes, and procedures . Routledge, '\n",
      "                       '2017.\\n'\n",
      "                       '[105] Matteo Tiezzi, Michele Casoni, Alessandro Betti, '\n",
      "                       'Tommaso Guidi, Marco Gori, and Stefano Melacci. “On '\n",
      "                       'the\\n'\n",
      "                       'resurgence of recurrent models for long sequences: '\n",
      "                       'Survey and research opportunities in the transformer '\n",
      "                       'era”. In:\\n'\n",
      "                       'arXiv preprint arXiv:2402.08132 (2024).\\n'\n",
      "                       '[106] Hugo Touvron, Thibaut Lavril, Gautier Izacard, '\n",
      "                       'Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, '\n",
      "                       'Baptiste\\n'\n",
      "                       'Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et '\n",
      "                       'al. “Llama: Open and efficient foundation language '\n",
      "                       'models”.\\n'\n",
      "                       'In: arXiv preprint arXiv:2302.13971 (2023).\\n'\n",
      "                       '[107] Jos Van Der Westhuizen and Joan Lasenby. “The '\n",
      "                       'unreasonable effectiveness of the forget gate”. '\n",
      "                       'In:arXiv preprint\\n'\n",
      "                       'arXiv:1804.04849 (2018).\\n'\n",
      "                       '[108] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob '\n",
      "                       'Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser,\\n'\n",
      "                       'and Illia Polosukhin. “Attention is All you Need”. In: '\n",
      "                       'Advances in Neural Information Processing Systems . '\n",
      "                       'Ed.\\n'\n",
      "                       'by I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. '\n",
      "                       'Fergus, S. Vishwanathan, and R. Garnett. Vol. 30. '\n",
      "                       'Cur-\\n'\n",
      "                       'ran Associates, Inc., 2017. url: https : / / '\n",
      "                       'proceedings . neurips . cc / paper _ files / paper / '\n",
      "                       '2017 / file /\\n'\n",
      "                       '3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.\\n'\n",
      "                       '[109] Shida Wang. “LongSSM: On the Length Extension of '\n",
      "                       'State-space Models in Language Modelling”. In: arXiv '\n",
      "                       'preprint\\n'\n",
      "                       'arXiv:2406.02080 (2024).\\n'\n",
      "                       '[110] Yu Wang, Yifan Gao, Xiusi Chen, Haoming Jiang, '\n",
      "                       'Shiyang Li, Jingfeng Yang, Qingyu Yin, Zheng Li, Xian '\n",
      "                       'Li, Bing Yin,\\n'\n",
      "                       'Jingbo Shang, and Julian McAuley. “MEMORYLLM: Towards '\n",
      "                       'Self-Updatable Large Language Models”. In:Forty-first\\n'\n",
      "                       'International Conference on Machine Learning . 2024. '\n",
      "                       'url: https://openreview.net/forum?id=p0lKWzdikQ.\\n'\n",
      "                       '[111] Yu Wang, Chi Han, Tongtong Wu, Xiaoxin He, '\n",
      "                       'Wangchunshu Zhou, Nafis Sadeq, Xiusi Chen, Zexue He, '\n",
      "                       'Wei Wang,\\n'\n",
      "                       'Gholamreza Haffari, et al. “Towards LifeSpan Cognitive '\n",
      "                       'Systems”. In: arXiv preprint arXiv:2409.13265 (2024).\\n'\n",
      "                       '[112] Zhiwei Wang, Yao Ma, Zitao Liu, and Jiliang '\n",
      "                       'Tang. “R-transformer: Recurrent neural network '\n",
      "                       'enhanced transformer”.\\n'\n",
      "                       'In: arXiv preprint arXiv:1907.05572 (2019).\\n'\n",
      "                       '[113] Jason Weston, Sumit Chopra, and Antoine Bordes. '\n",
      "                       '“Memory networks”. In: arXiv preprint arXiv:1410.3916 '\n",
      "                       '(2014).\\n'\n",
      "                       '[114] Bernard Widrow and Marcian E Hoff. “Adaptive '\n",
      "                       'switching circuits”. In: Neurocomputing: foundations '\n",
      "                       'of research .\\n'\n",
      "                       '1988, pp. 123–134.\\n'\n",
      "                       '[115] Ronald J Williams and David Zipser. “A learning '\n",
      "                       'algorithm for continually running fully recurrent '\n",
      "                       'neural networks”.\\n'\n",
      "                       'In: Neural computation 1.2 (1989), pp. 270–280.\\n'\n",
      "                       '[116] Daniel B Willingham. “Systems of memory in the '\n",
      "                       'human brain”. In: Neuron 18.1 (1997), pp. 5–8.\\n'\n",
      "                       '[117] Chao-Yuan Wu, Christoph Feichtenhofer, Haoqi '\n",
      "                       'Fan, Kaiming He, Philipp Krahenbuhl, and Ross '\n",
      "                       'Girshick. “Long-\\n'\n",
      "                       'term feature banks for detailed video understanding”. '\n",
      "                       'In: Proceedings of the IEEE/CVF conference on computer '\n",
      "                       'vision\\n'\n",
      "                       'and pattern recognition . 2019, pp. 284–293.\\n'\n",
      "                       '[118] Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, '\n",
      "                       'Jianmin Wang, and Mingsheng Long. “TimesNet: Temporal '\n",
      "                       '2D-\\n'\n",
      "                       'Variation Modeling for General Time Series Analysis”. '\n",
      "                       'In: The Eleventh International Conference on Learning\\n'\n",
      "                       'Representations. 2023. url: '\n",
      "                       'https://openreview.net/forum?id=ju_Uqw384Oq.\\n'\n",
      "                       '[119] Qingyang Wu, Zhenzhong Lan, Kun Qian, Jing Gu, '\n",
      "                       'Alborz Geramifard, and Zhou Yu. “Memformer: A memory-\\n'\n",
      "                       'augmented transformer for sequence modeling”. In: '\n",
      "                       'arXiv preprint arXiv:2010.06891 (2020).\\n'\n",
      "                       '[120] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song '\n",
      "                       'Han, and Mike Lewis. “Efficient Streaming Language '\n",
      "                       'Models\\n'\n",
      "                       'with Attention Sinks”. In: The Twelfth International '\n",
      "                       'Conference on Learning Representations . 2024. url: '\n",
      "                       'https:\\n'\n",
      "                       '//openreview.net/forum?id=NG7sS51zVF.\\n'\n",
      "                       '[121] An Yang, Baosong Yang, Beichen Zhang, Binyuan '\n",
      "                       'Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, '\n",
      "                       'Fei\\n'\n",
      "                       'Huang, Haoran Wei, et al. “Qwen2. 5 Technical Report”. '\n",
      "                       'In:arXiv preprint arXiv:2412.15115 (2024).\\n'\n",
      "                       '[122] Songlin Yang, Jan Kautz, and Ali Hatamizadeh. '\n",
      "                       '“Gated Delta Networks: Improving Mamba2 with Delta '\n",
      "                       'Rule”. In:\\n'\n",
      "                       'arXiv preprint arXiv:2412.06464 (2024).\\n'\n",
      "                       '[123] Songlin Yang, Bailin Wang, Yikang Shen, Rameswar '\n",
      "                       'Panda, and Yoon Kim. “Gated Linear Attention '\n",
      "                       'Transformers\\n'\n",
      "                       'with Hardware-Efficient Training”. In: Forty-first '\n",
      "                       'International Conference on Machine Learning . 2024. '\n",
      "                       'url: https:\\n'\n",
      "                       '//openreview.net/forum?id=ia5XvxFUJT.\\n'\n",
      "                       '[124] Songlin Yang, Bailin Wang, Yu Zhang, Yikang '\n",
      "                       'Shen, and Yoon Kim. “Parallelizing Linear Transformers '\n",
      "                       'with the\\n'\n",
      "                       'Delta Rule over Sequence Length”. In:The Thirty-eighth '\n",
      "                       'Annual Conference on Neural Information Processing '\n",
      "                       'Systems .\\n'\n",
      "                       '2024. url: '\n",
      "                       'https://openreview.net/forum?id=y8Rm4VNRPH.\\n'\n",
      "                       '[125] Luca Zancato, Arjun Seshadri, Yonatan Dukler, '\n",
      "                       'Aditya Golatkar, Yantao Shen, Benjamin Bowman, Matthew '\n",
      "                       'Trager,\\n'\n",
      "                       'Alessandro Achille, and Stefano Soatto. “B’MOJO: '\n",
      "                       'Hybrid State Space Realizations of Foundation Models '\n",
      "                       'with\\n'\n",
      "                       'Eidetic and Fading Memory”. In: The Thirty-eighth '\n",
      "                       'Annual Conference on Neural Information Processing '\n",
      "                       'Systems .\\n'\n",
      "                       '2024. url: '\n",
      "                       'https://openreview.net/forum?id=RnQdRY1h5v.\\n'\n",
      "                       '23\\n'\n",
      "                       '[126] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali '\n",
      "                       'Farhadi, and Yejin Choi. “HellaSwag: Can a Machine '\n",
      "                       'Really Finish\\n'\n",
      "                       'Your Sentence?” In: Proceedings of the 57th Annual '\n",
      "                       'Meeting of the Association for Computational '\n",
      "                       'Linguistics . Ed. by\\n'\n",
      "                       'Anna Korhonen, David Traum, and Lluís Màrquez. '\n",
      "                       'Florence, Italy: Association for Computational '\n",
      "                       'Linguistics, July\\n'\n",
      "                       '2019, pp. 4791–4800. doi: 10.18653/v1/P19-1472. url: '\n",
      "                       'https://aclanthology.org/P19-1472/.\\n'\n",
      "                       '[127] Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu. '\n",
      "                       '“Are transformers effective for time series '\n",
      "                       'forecasting?” In:\\n'\n",
      "                       'Proceedings of the AAAI conference on artificial '\n",
      "                       'intelligence . Vol. 37. 2023, pp. 11121–11128.\\n'\n",
      "                       '[128] Hao Zhang, Alexander C Berg, Michael Maire, and '\n",
      "                       'Jitendra Malik. “SVM-KNN: Discriminative nearest '\n",
      "                       'neighbor\\n'\n",
      "                       'classification for visual category recognition”. In: '\n",
      "                       '2006 IEEE Computer Society Conference on Computer '\n",
      "                       'Vision and\\n'\n",
      "                       'Pattern Recognition (CVPR’06) . Vol. 2. IEEE. 2006, '\n",
      "                       'pp. 2126–2136.\\n'\n",
      "                       '[129] Jianyu Zhang, Niklas Nolte, Ranajoy Sadhukhan, '\n",
      "                       'Beidi Chen, and Léon Bottou. “Memory Mosaics”. '\n",
      "                       'In:arXiv preprint\\n'\n",
      "                       'arXiv:2405.06394 (2024).\\n'\n",
      "                       '[130] Yunhao Zhang and Junchi Yan. “Crossformer: '\n",
      "                       'Transformer utilizing cross-dimension dependency for '\n",
      "                       'multivariate\\n'\n",
      "                       'time series forecasting”. In: The eleventh '\n",
      "                       'international conference on learning representations . '\n",
      "                       '2023.\\n'\n",
      "                       '[131] Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai '\n",
      "                       'Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang. '\n",
      "                       '“Informer:\\n'\n",
      "                       'Beyond efficient transformer for long sequence '\n",
      "                       'time-series forecasting”. In: Proceedings of the AAAI '\n",
      "                       'conference on\\n'\n",
      "                       'artificial intelligence . Vol. 35. 12. 2021, pp. '\n",
      "                       '11106–11115.\\n'\n",
      "                       '[132] Luisa Zintgraf, Kyriacos Shiarli, Vitaly Kurin, '\n",
      "                       'Katja Hofmann, and Shimon Whiteson. “Fast context '\n",
      "                       'adaptation via\\n'\n",
      "                       'meta-learning”. In: International Conference on '\n",
      "                       'Machine Learning . PMLR. 2019, pp. 7693–7702.\\n'\n",
      "                       '24\\n'\n",
      "                       'A Related Work\\n'\n",
      "                       'There are diverse perspectives that can independently '\n",
      "                       'lead to the design of Titans or its components. '\n",
      "                       'Accordingly, to\\n'\n",
      "                       'further situate our work in a broader context, we '\n",
      "                       'review three categories of studies:\\n'\n",
      "                       'A.1 Linear Recurrent Models\\n'\n",
      "                       'Recently, to address the computational cost of '\n",
      "                       'Transformers in both training and inference, linear '\n",
      "                       'recurrent models\\n'\n",
      "                       'have attracted much attention (Tiezzi et al. 2024), '\n",
      "                       'mainly due to their fast inference and training. The '\n",
      "                       'first generation\\n'\n",
      "                       'of models–such as RetNet (Yutao Sun et al. 2023), LRU '\n",
      "                       '(Orvieto et al. 2023), RWKV (Peng, Alcaide, et al. '\n",
      "                       '2023), S5 (J. T.\\n'\n",
      "                       'Smith, Warrington, and Linderman 2023), and S4 (Gu, '\n",
      "                       'Goel, and Re 2022)–uses data-independent transition '\n",
      "                       'matrix/decay\\n'\n",
      "                       'mechanism. The second generation of such models '\n",
      "                       'started to incorporate gating mechanism, a widely used '\n",
      "                       'techniques\\n'\n",
      "                       'in traditional RNNs (Gers, Jürgen Schmidhuber, and '\n",
      "                       'Cummins 2000; Greff et al. 2016; Van Der Westhuizen '\n",
      "                       'and Lasenby\\n'\n",
      "                       '2018), into such linear architectures–e.g., Griffin '\n",
      "                       '(De et al. 2024), SSMs (Behrouz, Santacatterina, and '\n",
      "                       'Zabih 2024; Dao\\n'\n",
      "                       'and Gu 2024; Gu and Dao 2024; Hasani et al. 2023), '\n",
      "                       'RWKV6 (Peng, Goldstein, et al. 2024). The third '\n",
      "                       'generation of linear\\n'\n",
      "                       'recurrent models are based on more complex memory '\n",
      "                       'updating rule based on meta-learning, online learning, '\n",
      "                       'and/or\\n'\n",
      "                       'delta-rule, resulting in more expressive and effective '\n",
      "                       'models such as: Longhorn (B. Liu et al. 2024), Gated '\n",
      "                       'DeltaNet (S. Yang,\\n'\n",
      "                       'Kautz, and Hatamizadeh 2024), TTT (Yu Sun et al. '\n",
      "                       '2024), and DeltaNet (S. Yang, B. Wang, Yu Zhang, et '\n",
      "                       'al. 2024). Our\\n'\n",
      "                       'LMM model can be seen as the next generation of such '\n",
      "                       'models, in which we incorporate the token flow into '\n",
      "                       'the memory\\n'\n",
      "                       'updating mechanism, having more powerful memory '\n",
      "                       'updating process. See Appendix C for a detailed '\n",
      "                       'discussion of\\n'\n",
      "                       'different recurrent models and Titans.\\n'\n",
      "                       'A.2 Transformer-based Architectures\\n'\n",
      "                       'Transformers. Transformers (Vaswani et al. 2017) as '\n",
      "                       'the de facto backbone for many deep learning models '\n",
      "                       'are based on\\n'\n",
      "                       'attention mechanism (Bahdanau 2014). They, however, '\n",
      "                       'suffer from quadratic computational cost, limiting '\n",
      "                       'their ability\\n'\n",
      "                       'to scale to long context window. To improve the memory '\n",
      "                       'consumption and throughput of softmax attention for '\n",
      "                       'longer\\n'\n",
      "                       'sequences, various studies focused on I/O aware '\n",
      "                       'implementations of attention (Dao 2024; Dao, D. Fu, et '\n",
      "                       'al. 2022), designing\\n'\n",
      "                       'more efficient attention mechanisms by sparsifying the '\n",
      "                       'attention matrix (B. Chen et al. 2021; Choromanski et '\n",
      "                       'al. 2021; Dai\\n'\n",
      "                       'et al. 2019; J. Dong et al. 2024; Roy et al. 2021), '\n",
      "                       'approximating the softmax (Arora et al. 2024), or '\n",
      "                       'developing kernel-based\\n'\n",
      "                       '(linear) attentions (Aksenov et al. 2024; Kacham, '\n",
      "                       'Mirrokni, and P. Zhong 2024; Schlag, Irie, and Jürgen '\n",
      "                       'Schmidhuber 2021;\\n'\n",
      "                       'S. Yang, B. Wang, Shen, et al. 2024).\\n'\n",
      "                       'Segment-based Transformers. Another line of research '\n",
      "                       'to improve the efficiency of Transformers is '\n",
      "                       'segment-based or\\n'\n",
      "                       'Chunk Transformers (Dai et al. 2019). The main '\n",
      "                       'drawback of chunk Transformers is that segments are '\n",
      "                       'fully separated and\\n'\n",
      "                       'so the context window is limited to the length of the '\n",
      "                       'chunks. To address this issue, various studies discuss '\n",
      "                       'the importance\\n'\n",
      "                       'of a memory so it can help the model to transfer '\n",
      "                       'information across chunks (Bulatov, Yuri Kuratov, et '\n",
      "                       'al. 2023; Bulatov,\\n'\n",
      "                       'Yury Kuratov, and Burtsev 2022; Feng et al. 2022; '\n",
      "                       'Hutchins et al. 2022; Rodkin et al. 2024; Z. Wang et '\n",
      "                       'al. 2019; Q. Wu\\n'\n",
      "                       'et al. 2020; Zancato et al. 2024). The key differences '\n",
      "                       'of Titans with these models are: (1) The memory in '\n",
      "                       'such models are\\n'\n",
      "                       'simple small size vectors, lacking expressive power to '\n",
      "                       'compress complex information; (2) The memory module '\n",
      "                       'lacks forget\\n'\n",
      "                       'mechanism, leading to a fast memory overflow; (3) only '\n",
      "                       'focus on momentary surprise, missing the information '\n",
      "                       'flow. More\\n'\n",
      "                       'specifically, recalling Recurrent Memory Transformers '\n",
      "                       '(RMT) (Bulatov, Yuri Kuratov, et al. 2023; Bulatov, '\n",
      "                       'Yury Kuratov,\\n'\n",
      "                       'and Burtsev 2022; Rodkin et al. 2024), one can treat '\n",
      "                       'Titans (MAC) as the generalization of RMT, where we '\n",
      "                       'use a neural\\n'\n",
      "                       'memory module instead of a vector-valued small size '\n",
      "                       'memory.\\n'\n",
      "                       'Memory for Large Language Models. Another interesting '\n",
      "                       'research direction has been to incorporate external '\n",
      "                       'memory\\n'\n",
      "                       'modules to LLMs after training (Z. He et al. 2024; '\n",
      "                       'Khandelwal et al. 2020; Y. Wang, Y. Gao, et al. 2024). '\n",
      "                       'Such models\\n'\n",
      "                       'are different from our approach as we incorporate the '\n",
      "                       'memory as a part of initial architecture and so we '\n",
      "                       'train it in\\n'\n",
      "                       'an end-to-end manner. Also, most of these explicit '\n",
      "                       'memory modules suffer from the same limitations as '\n",
      "                       'chunk-based\\n'\n",
      "                       'Transformers (mentioned above). For a detailed '\n",
      "                       'discussion of such models, we refer to the recent '\n",
      "                       'study of Y. Wang, Han,\\n'\n",
      "                       'et al. (2024).\\n'\n",
      "                       '25\\n'\n",
      "                       'A.3 Test Time Training and Fast Weight Programs\\n'\n",
      "                       'Memory Design and Augmentation with Memory. In the '\n",
      "                       'literature, a substantial research effort have been '\n",
      "                       'toward\\n'\n",
      "                       'designing memory modules that are capable of either '\n",
      "                       'memorizing the knowledge abstraction (e.g., persistent '\n",
      "                       'mem-\\n'\n",
      "                       'ory) (Sukhbaatar, Grave, et al. 2019), or memorizing '\n",
      "                       'the data-dependent information (also known as '\n",
      "                       'contextual memory),\\n'\n",
      "                       'through recurrence (Bulatov, Yury Kuratov, and Burtsev '\n",
      "                       '2022; Rodkin et al. 2024; Zancato et al. 2024), '\n",
      "                       'Transformers (Berges\\n'\n",
      "                       'et al. 2024; Cetin et al. 2024; Feng et al. 2022; Le, '\n",
      "                       'Tran, and Venkatesh 2020; Munkhdalai, Faruqui, and '\n",
      "                       'Gopal 2024; J. Zhang\\n'\n",
      "                       'et al. 2024), gradient (Irie, Csordás, and Jürgen '\n",
      "                       'Schmidhuber 2022; Munkhdalai, Sordoni, et al. 2019), '\n",
      "                       'or other learning\\n'\n",
      "                       'paradigms (Sukhbaatar, Weston, Fergus, et al. 2015; '\n",
      "                       'Weston, Chopra, and Bordes 2014). These memory models, '\n",
      "                       'however,\\n'\n",
      "                       'either (1) are based on momentary surprise, missing '\n",
      "                       'the data flow and events, (2) lack forget mechanisms '\n",
      "                       'to remove\\n'\n",
      "                       'the memory, leading to a fast memory overflow (3) are '\n",
      "                       'fixed-size shallow (matrix valued) memory, resulting '\n",
      "                       'in poor\\n'\n",
      "                       'performance in long context, and (4) are based on '\n",
      "                       'fixed parameters at test time, lacking test time '\n",
      "                       'adaption.\\n'\n",
      "                       'Fast Weight Programs. The idea of seeing linear layers '\n",
      "                       'as the key-value (associative) memory system backs to '\n",
      "                       'fast\\n'\n",
      "                       'weight programs, in which dynamic fast programs are '\n",
      "                       'incorporated into recurrent neural networks to serve '\n",
      "                       'as writable\\n'\n",
      "                       'memory (Schlag, Irie, and Jürgen Schmidhuber 2021; JH '\n",
      "                       'Schmidhuber 1992; Jürgen Schmidhuber 1993). The two '\n",
      "                       'learning\\n'\n",
      "                       'rules of Hebbian (Hebb 2005) and delta (Prados and Kak '\n",
      "                       '1989) are the most popular learning rules for fast '\n",
      "                       'weight programs,\\n'\n",
      "                       'which have been extensively explored in various '\n",
      "                       'studies (Irie, Schlag, et al. 2021; Munkhdalai, '\n",
      "                       'Sordoni, et al. 2019;\\n'\n",
      "                       'Munkhdalai and H. Yu 2017; Schlag, Irie, and Jürgen '\n",
      "                       'Schmidhuber 2021; JH Schmidhuber 1992; S. Yang, Kautz, '\n",
      "                       'and\\n'\n",
      "                       'Hatamizadeh 2024; S. Yang, B. Wang, Yu Zhang, et al. '\n",
      "                       '2024). All these models, however, are based on '\n",
      "                       'momentary surprise,\\n'\n",
      "                       'missing the token flow in the sequences (see Section '\n",
      "                       '3.1), and most of them lacks a forgetting gate, '\n",
      "                       'resulting in a poor\\n'\n",
      "                       'memory management.\\n'\n",
      "                       'Test Time Training. The key ideas of learning at test '\n",
      "                       'time or learning to learn (i.e., (Andrychowicz et al. '\n",
      "                       '2016)) backs to\\n'\n",
      "                       'very early studies on local learning Bottou and Vapnik '\n",
      "                       '1992, in which each test data sample is trained on its '\n",
      "                       'neighbors\\n'\n",
      "                       'before making a prediction (Gandelsman et al. 2022; H. '\n",
      "                       'Zhang et al. 2006). This approach further has shown '\n",
      "                       'promising\\n'\n",
      "                       'performance in vision tasks (Jain and Learned-Miller '\n",
      "                       '2011; Mullapudi et al. 2019), mostly due to their '\n",
      "                       'ability to mitigate\\n'\n",
      "                       'out-of-distribution samples. The most similar studies '\n",
      "                       'to ours in this direction are MNM (Munkhdalai, '\n",
      "                       'Sordoni, et al. 2019)\\n'\n",
      "                       'and TTT-layer (Yu Sun et al. 2024), which we discussed '\n",
      "                       'the key differences in Appendix C.\\n'\n",
      "                       'B Language Modeling and Common-sense Reasoning '\n",
      "                       'Datasets\\n'\n",
      "                       'Following recent studies on linear recurrent models '\n",
      "                       '(Dao and Gu 2024; S. Yang, Kautz, and Hatamizadeh '\n",
      "                       '2024; S. Yang,\\n'\n",
      "                       'B. Wang, Yu Zhang, et al. 2024), we use Wikitext '\n",
      "                       '(Merity et al. 2017), LMB (Paperno et al. 2016), PIQA '\n",
      "                       '(Bisk et al. 2020),\\n'\n",
      "                       'HellaSwag (Zellers et al. 2019), WinoGrande (Sakaguchi '\n",
      "                       'et al. 2021), ARC-easy (ARC-e) and ARC-challenge '\n",
      "                       '(ARC-c) (P.\\n'\n",
      "                       'Clark et al. 2018), SIQA (Sap et al. 2019), and BoolQ '\n",
      "                       '(C. Clark et al. 2019). Also, the baselines results '\n",
      "                       'for 400M models are\\n'\n",
      "                       'from the reported results by S. Yang, Kautz, and '\n",
      "                       'Hatamizadeh (2024).\\n'\n",
      "                       'C Long-term Memory Module (LMM) as a Sequence Model\\n'\n",
      "                       'In this section, we discuss how LMM as a sequence '\n",
      "                       'model is connected to modern linear recurrent models. '\n",
      "                       'For the sake\\n'\n",
      "                       'of simplicity, we start with a linear memory, where M𝑡 '\n",
      "                       '= 𝑊𝑡 ∈R𝑑in ×𝑑in . In this case, our objective function '\n",
      "                       'becomes\\n'\n",
      "                       'ℓ(M; 𝑥𝑡)= 1\\n'\n",
      "                       '2 ∥M𝑡k𝑡 −v𝑡∥2\\n'\n",
      "                       '2, in which we use gradient descent with momentum and '\n",
      "                       'weight decay for the optimization.\\n'\n",
      "                       'Accordingly, revisiting the recurrent formula in '\n",
      "                       'Equation 13:\\n'\n",
      "                       'M𝑡 = diag (1 −𝛼𝑡)M𝑡 +𝑆𝑡 (32)\\n'\n",
      "                       '𝑆𝑡 = diag (𝜂𝑡)𝑆𝑡−1 −diag (𝜃𝑡)\\x00M𝑡−1k⊤\\n'\n",
      "                       '𝑡 k𝑡 −v⊤\\n'\n",
      "                       '𝑡 k𝑡\\n'\n",
      "                       '\\x01 . (33)\\n'\n",
      "                       'LMM is Generalized Gated DeltaNet. As discussed by S. '\n",
      "                       'Yang, Kautz, and Hatamizadeh (2024), DeltaNet (S. '\n",
      "                       'Yang, B. Wang,\\n'\n",
      "                       'Yu Zhang, et al. 2024) can alternatively be '\n",
      "                       'interpreted as an online learning problem that '\n",
      "                       'optimizes the L= 1\\n'\n",
      "                       '2 ∥S𝑡k𝑡 −v𝑡∥2\\n'\n",
      "                       '2,\\n'\n",
      "                       'resulting in:\\n'\n",
      "                       'S𝑡+1 = S𝑡 −𝜃𝑡∇L= S𝑡\\n'\n",
      "                       '\\x00I −𝜃𝑡k𝑡k⊤\\n'\n",
      "                       '𝑡\\n'\n",
      "                       '\\x01 +𝜃𝑡v𝑡k⊤\\n'\n",
      "                       '𝑡 . (34)\\n'\n",
      "                       '26\\n'\n",
      "                       'In this formulation, Gated DeltaNet is the same as '\n",
      "                       'above but with an additional weight decay term (S. '\n",
      "                       'Yang, Kautz, and\\n'\n",
      "                       'Hatamizadeh 2024). Comparing Equation 32 and Equation '\n",
      "                       '34, we can see that setting 𝜂𝑡 = 0 results in both '\n",
      "                       'formulations to\\n'\n",
      "                       'be equivalent. Accordingly, we can say LMM is '\n",
      "                       'generalizing the very recent study of Gated DeltaNet '\n",
      "                       '(S. Yang, Kautz, and\\n'\n",
      "                       'Hatamizadeh 2024) from three aspects:\\n'\n",
      "                       '• Momentum-based Rule: The Delta Rule is based on '\n",
      "                       'momentary surprise, meaning that the flow of tokens '\n",
      "                       'cannot\\n'\n",
      "                       'affect the memory update rule. LMM, however, is based '\n",
      "                       'on a momentum rule, which consider both past and\\n'\n",
      "                       'momentary surprise.\\n'\n",
      "                       '• Deep Memory: While Gated DeltaNet is limited to a '\n",
      "                       'linear (matrix-valued) memory as it requires finding '\n",
      "                       'the closed\\n'\n",
      "                       'recurrence form, LMM allows using deep memory module '\n",
      "                       'by using a gradient-based formulation, resulting in '\n",
      "                       'higher\\n'\n",
      "                       'expressive power.\\n'\n",
      "                       '• Non-Linear Recurrence: While DeltaNet and Gated '\n",
      "                       'DeltaNet are based on linear recurrence, our LMM is '\n",
      "                       'using\\n'\n",
      "                       'inter-chunk non-linear recurrence and intra-chunk '\n",
      "                       'linear recurrence. This design allows LMM having a '\n",
      "                       'higher\\n'\n",
      "                       'expressive power.\\n'\n",
      "                       'Here, we discussed Gated DeltaNet as a sample of '\n",
      "                       'recent generation of recurrent models. Similar '\n",
      "                       'approaches such\\n'\n",
      "                       'as RWKV-7 (Peng 2021) are also using the same '\n",
      "                       'formulation and loss function, and so LMM is '\n",
      "                       'generalizing all such\\n'\n",
      "                       'models.\\n'\n",
      "                       'LMM is Generalized Longhorn. Similar to DeltaNet, '\n",
      "                       'Longhorn (B. Liu et al. 2024) uses the same loss '\n",
      "                       'function but it\\n'\n",
      "                       'derives the closed form using implicit online '\n",
      "                       'learning:\\n'\n",
      "                       'S𝑡+1 = S𝑡\\n'\n",
      "                       '\\x00I −𝛿𝑡k𝑡k⊤\\n'\n",
      "                       '𝑡\\n'\n",
      "                       '\\x01 +𝛿𝑡v𝑡k⊤\\n'\n",
      "                       '𝑡 , (35)\\n'\n",
      "                       'where 𝛿𝑡 = 𝜃𝑡\\n'\n",
      "                       '1+𝜃𝑡 k𝑡 k⊤\\n'\n",
      "                       '𝑡\\n'\n",
      "                       '. It, however, lacks a forgetting gate, resulting in a '\n",
      "                       'faster memory overflow. Therefore, in addition two\\n'\n",
      "                       'the abovementioned aspects of (1) Momentum-based Rule, '\n",
      "                       '(2) Deep Memory, and (3) Non-Linear Recurrence, LMM '\n",
      "                       'has\\n'\n",
      "                       'the advantage of using an additional (4) Forget Gate, '\n",
      "                       'leading to a better memory management.\\n'\n",
      "                       'LMM is Generalized TTT Layer. To the best of our '\n",
      "                       'knowledge, TTT (Yu Sun et al. 2024), is the only '\n",
      "                       'modern linear\\n'\n",
      "                       'recurrent models with a gradient-based updating rule. '\n",
      "                       'In addition to different architectural designs and '\n",
      "                       'also objective\\n'\n",
      "                       'functions, our LMM has three key differences with '\n",
      "                       'presented TTT layers (Yu Sun et al. 2024):\\n'\n",
      "                       '1. Forgetting Mechanism : TTT layers are updating '\n",
      "                       'memory at each time, without having the chance to '\n",
      "                       'forget the\\n'\n",
      "                       'past data. Accordingly, when fixing the memory size, '\n",
      "                       'the model cannot manage the memory for long sequences. '\n",
      "                       'A\\n'\n",
      "                       'forget mechanism, such as LMM’s, allows clearing the '\n",
      "                       'memory when very past information is not needed '\n",
      "                       'anymore.\\n'\n",
      "                       'We show that in a general case, this forget mechanism '\n",
      "                       'is equivalent to weight decay and provide a fast '\n",
      "                       'method to\\n'\n",
      "                       'incorporate it into the parallel training.\\n'\n",
      "                       '2. Momentum-based Update Rule : TTT layers are based '\n",
      "                       'on momentary surprise, meaning that the flow of '\n",
      "                       'tokens\\n'\n",
      "                       'cannot affect the memory update rule. LMM, however, is '\n",
      "                       'based on a momentum rule, which consider both past '\n",
      "                       'and\\n'\n",
      "                       'momentary surprise. See Section 3.1 for the motivation '\n",
      "                       'of this design.\\n'\n",
      "                       '3. Deep Memory : While TTT-layers allows for deeper '\n",
      "                       'memory, the advantages/disadvantages of such deeper '\n",
      "                       'memory\\n'\n",
      "                       'modules have not been experimentally evaluated.\\n'\n",
      "                       'To the best of our knowledge, our neural long-term '\n",
      "                       'memory module is the first linear recurrent model with '\n",
      "                       'momentum-\\n'\n",
      "                       'based update rule.\\n'\n",
      "                       'Finally, as a key difference with all the above and '\n",
      "                       'other recent linear recurrent studies, note that the '\n",
      "                       'hybrid variants of\\n'\n",
      "                       'modern linear models–such as Griffin (De et al. 2024), '\n",
      "                       'DeltaNet (S. Yang, B. Wang, Yu Zhang, et al. 2024), '\n",
      "                       'Gated DeltaNet (S.\\n'\n",
      "                       'Yang, Kautz, and Hatamizadeh 2024), H3 (D. Y. Fu et '\n",
      "                       'al. 2023), Mamba2 (Dao and Gu 2024), Samba (Ren et al. '\n",
      "                       '2024), etc.–all\\n'\n",
      "                       'are based on sequential layer-wise design. We present '\n",
      "                       'Titans to show how effectively one can incorporate '\n",
      "                       'such memory\\n'\n",
      "                       'modules into an architecture.\\n'\n",
      "                       '27'}}\n",
      "\u001b[36;1m\u001b[1;3m[1:checkpoint]\u001b[0m \u001b[1mState at the end of step 1:\n",
      "\u001b[0m{'extract_information': {'state': {'error': None,\n",
      "                                   'extracted_info': {...},\n",
      "                                   'pdf_text': 'Titans: Learning to Memorize '\n",
      "                                               'at Test Time\\n'\n",
      "                                               'Ali Behrouz\\n'\n",
      "                                               '†\\n'\n",
      "                                               ', Peilin Zhong\\n'\n",
      "                                               '†\\n'\n",
      "                                               ', and Vahab Mirrokni\\n'\n",
      "                                               '†\\n'\n",
      "                                               '†\\n'\n",
      "                                               'Google Research\\n'\n",
      "                                               '{alibehrouz, peilinz, '\n",
      "                                               'mirrokni}@google.com\\n'\n",
      "                                               'Abstract\\n'\n",
      "                                               'Over more than a decade there '\n",
      "                                               'has been an extensive research '\n",
      "                                               'effort of how effectively '\n",
      "                                               'utilize recurrent models and\\n'\n",
      "                                               'attentions. While recurrent '\n",
      "                                               'models aim to compress the '\n",
      "                                               'data into a fixed-size memory '\n",
      "                                               '(called hidden state), '\n",
      "                                               'attention allows\\n'\n",
      "                                               'attending to the entire '\n",
      "                                               'context window, capturing the '\n",
      "                                               'direct dependencies of all '\n",
      "                                               'tokens. This more accurate '\n",
      "                                               'modeling\\n'\n",
      "                                               'of dependencies, however, '\n",
      "                                               'comes with a quadratic cost, '\n",
      "                                               'limiting the model to a '\n",
      "                                               'fixed-length context. We '\n",
      "                                               'present a new\\n'\n",
      "                                               'neural long-term memory module '\n",
      "                                               'that learns to memorize '\n",
      "                                               'historical context and helps '\n",
      "                                               'an attention to attend to the\\n'\n",
      "                                               'current context while '\n",
      "                                               'utilizing long past '\n",
      "                                               'information. We show that this '\n",
      "                                               'neural memory has the '\n",
      "                                               'advantage of a fast\\n'\n",
      "                                               'parallelizable training while '\n",
      "                                               'maintaining a fast inference. '\n",
      "                                               'From a memory perspective, we '\n",
      "                                               'argue that attention due to '\n",
      "                                               'its\\n'\n",
      "                                               'limited context but accurate '\n",
      "                                               'dependency modeling performs '\n",
      "                                               'as a short-term memory, while '\n",
      "                                               'neural memory due to its\\n'\n",
      "                                               'ability to memorize the data, '\n",
      "                                               'acts as a long-term, more '\n",
      "                                               'persistent, memory. Based on '\n",
      "                                               'these two modules, we '\n",
      "                                               'introduce\\n'\n",
      "                                               'a new family of architectures, '\n",
      "                                               'called Titans, and present '\n",
      "                                               'three variants to address how '\n",
      "                                               'one can effectively '\n",
      "                                               'incorporate\\n'\n",
      "                                               'memory into this architecture. '\n",
      "                                               'Our experimental results on '\n",
      "                                               'language modeling, '\n",
      "                                               'common-sense reasoning, '\n",
      "                                               'genomics,\\n'\n",
      "                                               'and time series tasks show '\n",
      "                                               'that Titans are more effective '\n",
      "                                               'than Transformers and recent '\n",
      "                                               'modern linear recurrent '\n",
      "                                               'models.\\n'\n",
      "                                               'They further can effectively '\n",
      "                                               'scale to larger than 2M '\n",
      "                                               'context window size with '\n",
      "                                               'higher accuracy in '\n",
      "                                               'needle-in-haystack tasks\\n'\n",
      "                                               'compared to baselines.\\n'\n",
      "                                               '1 Introduction\\n'\n",
      "                                               '“The true art of memory is the '\n",
      "                                               'art of attention!\"\\n'\n",
      "                                               '— Samuel Johnson, 1787\\n'\n",
      "                                               'T\\n'\n",
      "                                               'ransformers, pure '\n",
      "                                               'attention-based architectures '\n",
      "                                               '(Vaswani et al. 2017), have '\n",
      "                                               'been firmly established as '\n",
      "                                               'state-of-\\n'\n",
      "                                               'the-art models in sequence '\n",
      "                                               'modeling, mainly due to their '\n",
      "                                               'in-context learning and '\n",
      "                                               'ability to learn at scale '\n",
      "                                               '(Kaplan\\n'\n",
      "                                               'et al. 2020). The primary '\n",
      "                                               'building blocks of '\n",
      "                                               'Transformers–attention '\n",
      "                                               'modules—function as '\n",
      "                                               'associative memory\\n'\n",
      "                                               'blocks (Bietti et al. 2024), '\n",
      "                                               'where they learn to store '\n",
      "                                               'key-value associations and '\n",
      "                                               'retrieve them by computing '\n",
      "                                               'pairwise\\n'\n",
      "                                               'similarity between queries '\n",
      "                                               '(i.e., search signals) and '\n",
      "                                               'keys (i.e., contexts). '\n",
      "                                               'Accordingly, by design, the '\n",
      "                                               'output of a Transformer\\n'\n",
      "                                               'is exclusively conditioned on '\n",
      "                                               'the direct dependencies of '\n",
      "                                               'tokens in the current context '\n",
      "                                               'window. This accurate modeling '\n",
      "                                               'of\\n'\n",
      "                                               'dependencies, however, comes '\n",
      "                                               'with quadratic time and memory '\n",
      "                                               'complexity in terms of the '\n",
      "                                               'context length. In complex\\n'\n",
      "                                               'real-world tasks (e.g., '\n",
      "                                               'language modeling (N. F. Liu '\n",
      "                                               'et al. 2024), video '\n",
      "                                               'understanding (C.-Y. Wu et al. '\n",
      "                                               '2019), long-term time\\n'\n",
      "                                               'series forecasting (H. Zhou et '\n",
      "                                               'al. 2021)), the context window '\n",
      "                                               'can become extremely large, '\n",
      "                                               'making the applicability of\\n'\n",
      "                                               'Transformers challenging in '\n",
      "                                               'these downstream tasks.\\n'\n",
      "                                               'To overcome the scalability '\n",
      "                                               'issue of Transformers, recent '\n",
      "                                               'studies aim to design '\n",
      "                                               'different variants of linear '\n",
      "                                               'Transform-\\n'\n",
      "                                               'ers (Kacham, Mirrokni, and P. '\n",
      "                                               'Zhong 2024; Katharopoulos et '\n",
      "                                               'al. 2020; S. Yang, B. Wang, '\n",
      "                                               'Shen, et al. 2024), where '\n",
      "                                               'softmax is\\n'\n",
      "                                               'replaced by a kernel function '\n",
      "                                               'in the attention (see §2.1 for '\n",
      "                                               'details), resulting in a '\n",
      "                                               'significant drop in memory '\n",
      "                                               'consumption.\\n'\n",
      "                                               'Despite efficiency and the '\n",
      "                                               'ability to scale to longer '\n",
      "                                               'context, linear Transformers '\n",
      "                                               'do not show competitive '\n",
      "                                               'performance\\n'\n",
      "                                               'compared to Transformers as '\n",
      "                                               'the kernel trick makes the '\n",
      "                                               'model a linear recurrent '\n",
      "                                               'network, in which the data is '\n",
      "                                               'compressed\\n'\n",
      "                                               'into a matrix-valued states '\n",
      "                                               '(Katharopoulos et al. 2020). '\n",
      "                                               'This, however, brings a '\n",
      "                                               'contradictory fact about '\n",
      "                                               'linear recurrent (or\\n'\n",
      "                                               'linear Transformers) models: '\n",
      "                                               'On one hand, we use these '\n",
      "                                               'linear models to enhance '\n",
      "                                               'scalability and efficiency '\n",
      "                                               '(linear vs.\\n'\n",
      "                                               'quadratic complexity), whose '\n",
      "                                               'advantages is appeared for '\n",
      "                                               'very long context; On the '\n",
      "                                               'other hand, a very long '\n",
      "                                               'context cannot\\n'\n",
      "                                               'be properly compressed in a '\n",
      "                                               'small vector-valued or '\n",
      "                                               'matrix-valued states (S. Wang '\n",
      "                                               '2024).\\n'\n",
      "                                               '1\\n'\n",
      "                                               'arXiv:2501.00663v1  [cs.LG]  '\n",
      "                                               '31 Dec 2024\\n'\n",
      "                                               'Furthermore, beyond '\n",
      "                                               'efficiency, most existing '\n",
      "                                               'architectures–ranging from '\n",
      "                                               'Hopfield Networks (Hopfield '\n",
      "                                               '1982) to LSTMs (Jür-\\n'\n",
      "                                               'gen Schmidhuber and Hochreiter '\n",
      "                                               '1997) and Transformers '\n",
      "                                               '(Vaswani et al. 2017)–face '\n",
      "                                               'challenges when dealing with '\n",
      "                                               'general-\\n'\n",
      "                                               'ization, length extrapolation, '\n",
      "                                               'and/or reasoning (Anil et al. '\n",
      "                                               '2022; Qin, Y. Zhong, and Deng '\n",
      "                                               '2024), all of which are '\n",
      "                                               'inseparable\\n'\n",
      "                                               'parts of many hard real-world '\n",
      "                                               'tasks. Although these '\n",
      "                                               'architectures draw inspiration '\n",
      "                                               'from the human brain, each of '\n",
      "                                               'which\\n'\n",
      "                                               'are missing: (1) a crucial '\n",
      "                                               'component for learning '\n",
      "                                               'process—such as short-term '\n",
      "                                               'memory, long-term memory, '\n",
      "                                               'meta-memory,\\n'\n",
      "                                               'attending to current context, '\n",
      "                                               'etc. (Cowan 2008); (2) how '\n",
      "                                               'these components are '\n",
      "                                               'interconnected systems that '\n",
      "                                               'can operate\\n'\n",
      "                                               'independently; and/or (3) the '\n",
      "                                               'ability to actively learn from '\n",
      "                                               'data and memorize the '\n",
      "                                               'abstraction of past history. '\n",
      "                                               'We argue\\n'\n",
      "                                               'that in an effective learning '\n",
      "                                               'paradigm, similar to human '\n",
      "                                               'brain, there aredistinct yet '\n",
      "                                               'interconnected modules, each '\n",
      "                                               'of which\\n'\n",
      "                                               'is responsible for a component '\n",
      "                                               'crucial to the learning '\n",
      "                                               'process.\\n'\n",
      "                                               'Memory Perspective\\n'\n",
      "                                               'Memory is a fundamental mental '\n",
      "                                               'process and is an inseparable '\n",
      "                                               'component of human learning '\n",
      "                                               '(Terry 2017). Without\\n'\n",
      "                                               'a properly functioning memory '\n",
      "                                               'system, humans and animals '\n",
      "                                               'would be restricted to basic '\n",
      "                                               'reflexes and stereotyped\\n'\n",
      "                                               'behaviors. Accordingly, memory '\n",
      "                                               'has been the inspiration for '\n",
      "                                               'many seminal research in '\n",
      "                                               'machine learning literature; '\n",
      "                                               'e.g.,\\n'\n",
      "                                               'Hopfield Networks (Hopfield '\n",
      "                                               '1982), LSTMs (Jürgen '\n",
      "                                               'Schmidhuber and Hochreiter '\n",
      "                                               '1997), and Transformers '\n",
      "                                               '(Vaswani et al.\\n'\n",
      "                                               '2017).\\n'\n",
      "                                               'Taking inspiration from the '\n",
      "                                               'common definitions of memory '\n",
      "                                               'and learning in '\n",
      "                                               'neuropsychology literature '\n",
      "                                               '(Okano, Hirano,\\n'\n",
      "                                               'and Balaban 2000), most '\n",
      "                                               'existing architectures '\n",
      "                                               'consider memory as a neural '\n",
      "                                               'update caused by an input, and '\n",
      "                                               'define learning\\n'\n",
      "                                               'as a process for acquiring '\n",
      "                                               'effective and useful memory, '\n",
      "                                               'given an objective. In this '\n",
      "                                               'perspective, Recurrent Neural '\n",
      "                                               'Networks\\n'\n",
      "                                               '(RNNs) (Williams and Zipser '\n",
      "                                               '1989) can be defined as models '\n",
      "                                               'with a vector-valued memory '\n",
      "                                               'module M(also called hidden\\n'\n",
      "                                               'state) with two main steps: '\n",
      "                                               'Given a new input 𝑥𝑡 at time '\n",
      "                                               '𝑡, the model (1) updates the '\n",
      "                                               'memory using a function '\n",
      "                                               '𝑓(M𝑡−1,𝑥𝑡)\\n'\n",
      "                                               '(with compression); and (2) '\n",
      "                                               'retrieves the corresponding '\n",
      "                                               'memory of input using a '\n",
      "                                               'function 𝑔(M𝑡,𝑥𝑡)(see §2.1 for '\n",
      "                                               'details).\\n'\n",
      "                                               'Similarly, Transformers can be '\n",
      "                                               'seen as architectures with a '\n",
      "                                               'growing memory and two similar '\n",
      "                                               'steps. That is, the pair of '\n",
      "                                               'key\\n'\n",
      "                                               'and value matrices acts as the '\n",
      "                                               'model’s memory, and the model: '\n",
      "                                               '(1) updates the memory by '\n",
      "                                               'appending the key and value '\n",
      "                                               'to\\n'\n",
      "                                               'the memory (without '\n",
      "                                               'compression), and (2) '\n",
      "                                               'retrieves query vectors’ '\n",
      "                                               'corresponding memory by '\n",
      "                                               'finding the similarity of\\n'\n",
      "                                               'query and key vectors, which '\n",
      "                                               'is then used to weight the '\n",
      "                                               'value vectors for the output.\\n'\n",
      "                                               'This perspective, can help us '\n",
      "                                               'better understand existing '\n",
      "                                               'paradigms, their critical '\n",
      "                                               'differences, and design more '\n",
      "                                               'effective\\n'\n",
      "                                               'architectures. For example, '\n",
      "                                               'the main difference between '\n",
      "                                               'Transformers (Vaswani et al. '\n",
      "                                               '2017) and linear Transform-\\n'\n",
      "                                               'ers (Katharopoulos et al. '\n",
      "                                               '2020) is the memory structure '\n",
      "                                               'as well as the memory updating '\n",
      "                                               'step, in which linear '\n",
      "                                               'Transformers\\n'\n",
      "                                               'compress the historical data '\n",
      "                                               'into a fixed-size '\n",
      "                                               'matrix-valued memory while '\n",
      "                                               'Transformers keep all '\n",
      "                                               'historical data (within\\n'\n",
      "                                               'the context length) without '\n",
      "                                               'any compression. While both '\n",
      "                                               'linear Transformers and linear '\n",
      "                                               'RNNs (including state space\\n'\n",
      "                                               'models) compress the '\n",
      "                                               'information in memory update '\n",
      "                                               'step, the critical difference '\n",
      "                                               'lies in the structure of the '\n",
      "                                               'memory,\\n'\n",
      "                                               'where linear RNNs (vs. linear '\n",
      "                                               'Transformers) use a '\n",
      "                                               'vector-valued memory (vs. '\n",
      "                                               'matrix-valued memory). '\n",
      "                                               'Therefore, this\\n'\n",
      "                                               'perspective motivates us to '\n",
      "                                               'ask: (Q1) What constitute a '\n",
      "                                               'good structure for the memory? '\n",
      "                                               '(Q2) What is a proper memory\\n'\n",
      "                                               'update mechanism? and (Q3) '\n",
      "                                               'What is a good memory '\n",
      "                                               'retrieval process?\\n'\n",
      "                                               'Revisiting our understanding '\n",
      "                                               'of human memory, it is neither '\n",
      "                                               'a unitary process nor it '\n",
      "                                               'serves a single function '\n",
      "                                               '(Cowan\\n'\n",
      "                                               '2008). In fact, memory is a '\n",
      "                                               'confederation of systems–e.g., '\n",
      "                                               'short-term, working, and '\n",
      "                                               'long-term memory–each serving '\n",
      "                                               'a\\n'\n",
      "                                               'different function with '\n",
      "                                               'different neural structures, '\n",
      "                                               'and each capable of operating '\n",
      "                                               'independently (Willingham '\n",
      "                                               '1997). This\\n'\n",
      "                                               'fact motivates us to ask: (Q4) '\n",
      "                                               'How to design an efficient '\n",
      "                                               'architecture that incorporates '\n",
      "                                               'different interconnected '\n",
      "                                               'memory\\n'\n",
      "                                               'modules. Finally, storing a '\n",
      "                                               'memory is a neural process '\n",
      "                                               'that requires to encode and '\n",
      "                                               'store the abstraction of the '\n",
      "                                               'past. It can\\n'\n",
      "                                               'be over-simplification to '\n",
      "                                               'assume a single vector or a '\n",
      "                                               'matrix, whose parameters are '\n",
      "                                               'encoding the data in a linear '\n",
      "                                               'manner,\\n'\n",
      "                                               'are enough for storing '\n",
      "                                               'long-term history. (Q5) Is a '\n",
      "                                               'deep memory module needed to '\n",
      "                                               'effectively store/remember '\n",
      "                                               'long\\n'\n",
      "                                               'past?\\n'\n",
      "                                               'Contributions and Roadmap\\n'\n",
      "                                               'In this paper, we aim to '\n",
      "                                               'answer the above five '\n",
      "                                               'questions by designing a '\n",
      "                                               'long-term neural memory '\n",
      "                                               'module, that can\\n'\n",
      "                                               'efficiently and effectively '\n",
      "                                               'learn to memorize at test '\n",
      "                                               'time. Building upon its '\n",
      "                                               'design, we discuss how it can '\n",
      "                                               'be incorporated\\n'\n",
      "                                               'into an architecture.\\n'\n",
      "                                               'Neural Memory (§3). We present '\n",
      "                                               'a (deep) neural long-term '\n",
      "                                               'memory that (as a meta '\n",
      "                                               'in-context model) learns how '\n",
      "                                               'to\\n'\n",
      "                                               'memorize/store the data into '\n",
      "                                               'its parameters at test time. '\n",
      "                                               'Inspired by human long-term '\n",
      "                                               'memory system (Mandler 2014),\\n'\n",
      "                                               '2\\n'\n",
      "                                               'we design this memory module '\n",
      "                                               'so an event that violates the '\n",
      "                                               'expectations (being '\n",
      "                                               'surprising) is more memorable. '\n",
      "                                               'To this\\n'\n",
      "                                               'end, we measure the surprise '\n",
      "                                               'of an input with the gradient '\n",
      "                                               'of the neural network with '\n",
      "                                               'respect to the input in '\n",
      "                                               'associative\\n'\n",
      "                                               'memory loss (see §3.1 for '\n",
      "                                               'details). To better handle the '\n",
      "                                               'limited memory, we present a '\n",
      "                                               'decaying mechanism that '\n",
      "                                               'consider the\\n'\n",
      "                                               'proportion of memory size and '\n",
      "                                               'the amount of data surprise, '\n",
      "                                               'resulting in better memory '\n",
      "                                               'management. We show that this\\n'\n",
      "                                               'decay mechanism is in fact the '\n",
      "                                               'generalization of forgetting '\n",
      "                                               'mechanism in modern recurrent '\n",
      "                                               'models (Dao and Gu 2024; Gu\\n'\n",
      "                                               'and Dao 2024; S. Yang, Kautz, '\n",
      "                                               'and Hatamizadeh 2024). '\n",
      "                                               'Interestingly, we find that '\n",
      "                                               'this mechanism is equivalent '\n",
      "                                               'to optimizing\\n'\n",
      "                                               'a meta neural network with '\n",
      "                                               'mini-batch gradient descent, '\n",
      "                                               'momentum, and weight decay. '\n",
      "                                               'Building upon tensorizing\\n'\n",
      "                                               'mini-batch gradient descent to '\n",
      "                                               'use more matmul operations (Yu '\n",
      "                                               'Sun et al. 2024), we present a '\n",
      "                                               'fast and parallelizable\\n'\n",
      "                                               'algorithm to train our deep '\n",
      "                                               'neural long-term memory.\\n'\n",
      "                                               'Titans Architectures (§4). '\n",
      "                                               'After designing the long-term '\n",
      "                                               'neural memory, an important '\n",
      "                                               'remaining question is how to\\n'\n",
      "                                               'effectively and efficiently '\n",
      "                                               'incorporate memory into a deep '\n",
      "                                               'learning architecture. We '\n",
      "                                               'present Titans, a family of '\n",
      "                                               'deep models\\n'\n",
      "                                               'that consists of three '\n",
      "                                               'hyper-heads: (1) Core: this '\n",
      "                                               'module consists of the '\n",
      "                                               'short-term memory, and is '\n",
      "                                               'responsible for the main\\n'\n",
      "                                               'flow of processing the data '\n",
      "                                               '(we use attention with limited '\n",
      "                                               'window size); (2) Long-term '\n",
      "                                               'Memory: this branch is our '\n",
      "                                               'neural\\n'\n",
      "                                               'long-term memory module that '\n",
      "                                               'is responsible to '\n",
      "                                               'store/remember long past; (3) '\n",
      "                                               'Persistent Memory: this is a '\n",
      "                                               'set of learnable\\n'\n",
      "                                               'but date-independent '\n",
      "                                               'parameters that encodes the '\n",
      "                                               'knowledge about a task. '\n",
      "                                               'Finally, as a proof of '\n",
      "                                               'concept, we present three\\n'\n",
      "                                               'variants of Titans, in which '\n",
      "                                               'we incorporate memory as: (i) '\n",
      "                                               'a context, (ii) a layer, and '\n",
      "                                               '(iii) a gated branch.\\n'\n",
      "                                               'Experimental Results (§5). We '\n",
      "                                               'perform experimental '\n",
      "                                               'evaluations on language '\n",
      "                                               'modeling, commonsense '\n",
      "                                               'reasoning, recall-\\n'\n",
      "                                               'intensive, needle in haystack, '\n",
      "                                               'time series forecasting, and '\n",
      "                                               'DNA modeling tasks. We observe '\n",
      "                                               'that our Titan architecture\\n'\n",
      "                                               'outperforms all modern '\n",
      "                                               'recurrent models as well as '\n",
      "                                               'their hybrid variants '\n",
      "                                               '(combining with sliding-window '\n",
      "                                               'attention) across\\n'\n",
      "                                               'a comprehensive set of '\n",
      "                                               'benchmarks. Furthermore, '\n",
      "                                               'Titans outperforms '\n",
      "                                               'Transformers with the same '\n",
      "                                               'context window, and\\n'\n",
      "                                               'show competitive performance '\n",
      "                                               'with Transformers that use the '\n",
      "                                               'entire context. This results '\n",
      "                                               'are achieved while, contrary '\n",
      "                                               'to\\n'\n",
      "                                               'Transformers, Titans scale to '\n",
      "                                               'larger than 2M context window '\n",
      "                                               'size.\\n'\n",
      "                                               '2 Preliminaries\\n'\n",
      "                                               'I\\n'\n",
      "                                               'n this section, we discuss the '\n",
      "                                               'notation and some background '\n",
      "                                               'concepts that we use though '\n",
      "                                               'the paper. We let\\n'\n",
      "                                               '𝑥 ∈R𝑁×𝑑in be the input, Mbe a '\n",
      "                                               'neural network (neural memory '\n",
      "                                               'module), Q,K,V be the query, '\n",
      "                                               'key and value\\n'\n",
      "                                               'of the attention mechanism, '\n",
      "                                               'and M be the attention mask. '\n",
      "                                               'When segmenting the sequence, '\n",
      "                                               'we use S(𝑖)to refer to\\n'\n",
      "                                               'the 𝑖-th segment. Through the '\n",
      "                                               'paper, we abuse the notation '\n",
      "                                               'and use subscripts to refer to '\n",
      "                                               'a specific element of a '\n",
      "                                               'matrix,\\n'\n",
      "                                               'vector, or segments. For '\n",
      "                                               'example, we let S(𝑖)\\n'\n",
      "                                               '𝑗 be the 𝑗-th token in the '\n",
      "                                               '𝑖-th segment. The only '\n",
      "                                               'exception is subscripts with '\n",
      "                                               '𝑡,\\n'\n",
      "                                               'which we reserved to index '\n",
      "                                               'recurrence over time, or the '\n",
      "                                               'state of a neural network at '\n",
      "                                               'time𝑡. Given a neural network '\n",
      "                                               'Nand\\n'\n",
      "                                               'a data sample 𝑥, we use '\n",
      "                                               'N(𝑥)(resp. N∗(𝑥)) to refer to '\n",
      "                                               'the forward pass with (resp. '\n",
      "                                               'without) weight adjustment. '\n",
      "                                               'Also, we\\n'\n",
      "                                               'abuse the notation and use '\n",
      "                                               'N(𝑘)to refer to the 𝑘-th layer '\n",
      "                                               'of the neural network. In the '\n",
      "                                               'following, we first, discuss '\n",
      "                                               'the\\n'\n",
      "                                               'backgrounds for attention and '\n",
      "                                               'its efficient variants '\n",
      "                                               'followed by a review of modern '\n",
      "                                               'linear RNNs. Finally, we '\n",
      "                                               'discuss a\\n'\n",
      "                                               'memory perspective of these '\n",
      "                                               'architectures that motivates '\n",
      "                                               'us to design Titans.\\n'\n",
      "                                               '2.1 Backgrounds\\n'\n",
      "                                               'Attention. Transformers '\n",
      "                                               '(Vaswani et al. 2017) as the '\n",
      "                                               'de facto backbone for many '\n",
      "                                               'deep learning models are based '\n",
      "                                               'on\\n'\n",
      "                                               'attention mechanism. Given '\n",
      "                                               'input 𝑥 ∈R𝑁×𝑑in , causal '\n",
      "                                               'attention computes output y '\n",
      "                                               '∈R𝑁×𝑑in based on softmax over '\n",
      "                                               'input\\n'\n",
      "                                               'dependent key, value, and '\n",
      "                                               'query matrices:\\n'\n",
      "                                               'Q = 𝑥WQ, K = 𝑥WK, V = 𝑥WV, '\n",
      "                                               '(1)\\n'\n",
      "                                               'y𝑖 =\\n'\n",
      "                                               '𝑖∑︁\\n'\n",
      "                                               '𝑗=1\\n'\n",
      "                                               'exp\\n'\n",
      "                                               '\\x10\\n'\n",
      "                                               'Q⊤\\n'\n",
      "                                               '𝑖 K𝑗/√𝑑in\\n'\n",
      "                                               '\\x11\\n'\n",
      "                                               'V𝑗\\n'\n",
      "                                               'Í𝑖\\n'\n",
      "                                               'ℓ=1 exp\\n'\n",
      "                                               '\\x10\\n'\n",
      "                                               'Q⊤\\n'\n",
      "                                               '𝑖 Kℓ/√𝑑in\\n'\n",
      "                                               '\\x11, (2)\\n'\n",
      "                                               'where WQ,WK,and WV ∈R𝑑in ×𝑑in '\n",
      "                                               'are learnable parameters. '\n",
      "                                               'Despite the power and '\n",
      "                                               'effectiveness in recall, '\n",
      "                                               'transformers\\n'\n",
      "                                               'need at least 𝑁 ×𝑑 operators '\n",
      "                                               'to calculate the output, '\n",
      "                                               'resulting in larger memory '\n",
      "                                               'consumption and '\n",
      "                                               'lower-throughput for\\n'\n",
      "                                               'longer sequences.\\n'\n",
      "                                               'Efficient Attentions. To '\n",
      "                                               'improve the memory consumption '\n",
      "                                               'and throughput of softmax '\n",
      "                                               'attention for longer '\n",
      "                                               'sequences,\\n'\n",
      "                                               'various studies focused on I/O '\n",
      "                                               'aware implementations of '\n",
      "                                               'attention (Dao 2024; Dao, D. '\n",
      "                                               'Fu, et al. 2022), designing '\n",
      "                                               'more\\n'\n",
      "                                               '3\\n'\n",
      "                                               'efficient attention mechanisms '\n",
      "                                               'by sparsifying the attention '\n",
      "                                               'matrix (B. Chen et al. 2021; '\n",
      "                                               'Choromanski et al. 2021; Dai '\n",
      "                                               'et al.\\n'\n",
      "                                               '2019), approximating the '\n",
      "                                               'softmax (Arora et al. 2024), '\n",
      "                                               'or developing kernel-based '\n",
      "                                               '(linear) attentions (Aksenov '\n",
      "                                               'et al. 2024;\\n'\n",
      "                                               'Kacham, Mirrokni, and P. Zhong '\n",
      "                                               '2024; Schlag, Irie, and Jürgen '\n",
      "                                               'Schmidhuber 2021; S. Yang, B. '\n",
      "                                               'Wang, Shen, et al. 2024). In\\n'\n",
      "                                               'this part, we focus on the '\n",
      "                                               'later, i.e., linear '\n",
      "                                               'attentions, where the softmax '\n",
      "                                               'in standard attention is '\n",
      "                                               'replaced with an alternative\\n'\n",
      "                                               'kernel function 𝜙(.,.), such '\n",
      "                                               'that 𝜙(𝑥,𝑦)= 𝜙(𝑥)𝜙(𝑦). '\n",
      "                                               'Accordingly, the attention can '\n",
      "                                               'be written as:\\n'\n",
      "                                               'y𝑖 =\\n'\n",
      "                                               '𝑖∑︁\\n'\n",
      "                                               '𝑗=1\\n'\n",
      "                                               '𝜙(𝑄⊤\\n'\n",
      "                                               '𝑖 𝐾𝑗)\\n'\n",
      "                                               'Í𝑖\\n'\n",
      "                                               'ℓ=1 𝜙(𝑄⊤\\n'\n",
      "                                               '𝑖 𝐾ℓ)\\n'\n",
      "                                               '𝑉𝑗 =\\n'\n",
      "                                               '𝑖∑︁\\n'\n",
      "                                               '𝑗=1\\n'\n",
      "                                               '𝜙(𝑄𝑖)⊤𝜙(𝐾𝑗)\\n'\n",
      "                                               'Í𝑖\\n'\n",
      "                                               'ℓ=1 𝜙(𝑄𝑖)⊤𝜙(𝐾ℓ)\\n'\n",
      "                                               '𝑉𝑗 =\\n'\n",
      "                                               '𝜙(𝑄𝑖)⊤Í𝑖\\n'\n",
      "                                               '𝑗=1 𝜙(𝐾𝑗)𝑉𝑗\\n'\n",
      "                                               '𝜙(𝑄𝑖)⊤Í𝑖\\n'\n",
      "                                               'ℓ=1 𝜙(𝐾ℓ)\\n'\n",
      "                                               ', (3)\\n'\n",
      "                                               'resulting in a '\n",
      "                                               'higher-throughput as terms Í𝑖\\n'\n",
      "                                               '𝑗=1 𝜙(𝐾𝑗)and Í𝑖\\n'\n",
      "                                               'ℓ=1 𝜙(𝐾ℓ)are re-using in each '\n",
      "                                               'step. When choosing the '\n",
      "                                               'kernel\\n'\n",
      "                                               'as identity matrix (Yutao Sun '\n",
      "                                               'et al. 2023), the above '\n",
      "                                               'formulation can also be '\n",
      "                                               'written in a recurrent '\n",
      "                                               'format:\\n'\n",
      "                                               'M𝑡 = M𝑡−1 +𝐾⊤\\n'\n",
      "                                               '𝑡 𝑉𝑡 , (4)\\n'\n",
      "                                               'y𝑡 = 𝑄𝑡M𝑡 , (5)\\n'\n",
      "                                               'which allows efficient '\n",
      "                                               'inference for linear '\n",
      "                                               'attentions.\\n'\n",
      "                                               'Modern Linear Models and Their '\n",
      "                                               'Memory Perspective. As '\n",
      "                                               'discussed earlier, one can '\n",
      "                                               'define learning as a process '\n",
      "                                               'for\\n'\n",
      "                                               'acquiring effective and useful '\n",
      "                                               'memory. Building upon this, '\n",
      "                                               'one can see the hidden state '\n",
      "                                               'of Recurrent Neural Networks\\n'\n",
      "                                               '(RNNs) as a memory unit, which '\n",
      "                                               'the model aims to compress the '\n",
      "                                               'information into. Accordingly, '\n",
      "                                               'in a general form of\\n'\n",
      "                                               'recurrent neural network, the '\n",
      "                                               'hidden state can be treated as '\n",
      "                                               'a memory unit and the '\n",
      "                                               'recurrence process can be '\n",
      "                                               'split into the\\n'\n",
      "                                               'read and write operations in '\n",
      "                                               'the memory unit. That is, we '\n",
      "                                               'let 𝑥 ∈R𝑁×𝑑in be the input, M∈ '\n",
      "                                               'R𝑑 is the memory unit, and\\n'\n",
      "                                               'y ∈R𝑑in is the output, then '\n",
      "                                               'the general form of the '\n",
      "                                               'recurrent neural network is '\n",
      "                                               'defined as:\\n'\n",
      "                                               'M𝑡 = 𝑓(M𝑡−1,𝑥𝑡), Write '\n",
      "                                               'Operation (6)\\n'\n",
      "                                               'y𝑡 = 𝑔(M𝑡,𝑥𝑡), Read Operation '\n",
      "                                               '(7)\\n'\n",
      "                                               'where 𝑓(.,.)is the read and '\n",
      "                                               '𝑔(.,.)is the write '\n",
      "                                               'corresponding functions. Note '\n",
      "                                               'that here the subscript of M𝑡 '\n",
      "                                               'shows the state\\n'\n",
      "                                               'of the memory at time 𝑡.\\n'\n",
      "                                               'In this perspective, the '\n",
      "                                               'recurrence formula of linear '\n",
      "                                               'Transformers (see Equation 4) '\n",
      "                                               'is equivalent to additively '\n",
      "                                               'compress\\n'\n",
      "                                               'and write keys and values, '\n",
      "                                               '(𝐾𝑡,𝑉𝑡), into a matrix-valued '\n",
      "                                               'memory unit M𝑡. Therefore, '\n",
      "                                               'when dealing with long '\n",
      "                                               'context\\n'\n",
      "                                               'data, this additive nature of '\n",
      "                                               'the process results in memory '\n",
      "                                               'overflow, significantly '\n",
      "                                               'damaging the performance of '\n",
      "                                               'the model.\\n'\n",
      "                                               'To address this, studies have '\n",
      "                                               'focused on two promising '\n",
      "                                               'directions: (1) Adding forget '\n",
      "                                               'mechanism: several studies '\n",
      "                                               'have\\n'\n",
      "                                               'presented adaptive '\n",
      "                                               '(data-dependent) forgetting '\n",
      "                                               'gate mechanisms for linear '\n",
      "                                               'models, where it can erase the '\n",
      "                                               'memory when it\\n'\n",
      "                                               'is needed. As examples of such '\n",
      "                                               'models, we refer to GLA (S. '\n",
      "                                               'Yang, B. Wang, Shen, et al. '\n",
      "                                               '2024), LRU (Orvieto et al. '\n",
      "                                               '2023),\\n'\n",
      "                                               'Griffin (De et al. 2024), '\n",
      "                                               'xLSTM (Beck et al. 2024), and '\n",
      "                                               'Mamba2 (Dao and Gu 2024), '\n",
      "                                               'which the later is also '\n",
      "                                               'connected to the\\n'\n",
      "                                               'discretized version of '\n",
      "                                               'traditional state space models '\n",
      "                                               '(Gu and Dao 2024).(2) '\n",
      "                                               'Improving the write operation: '\n",
      "                                               'To overcome the\\n'\n",
      "                                               'additive nature of memory '\n",
      "                                               'write operation in traditional '\n",
      "                                               'recurrent models, Widrow and '\n",
      "                                               'Hoff (1988) presented Delta '\n",
      "                                               'Rule,\\n'\n",
      "                                               'in which before adding a '\n",
      "                                               'memory (i.e., a pair of key '\n",
      "                                               'and value), the model first '\n",
      "                                               'removes its past value. To '\n",
      "                                               'enhance the\\n'\n",
      "                                               'parallelizable training and '\n",
      "                                               'scaling, S. Yang, B. Wang, Yu '\n",
      "                                               'Zhang, et al. (2024) present a '\n",
      "                                               'fast paralellizable algorithm. '\n",
      "                                               'Finally,\\n'\n",
      "                                               'very recently, S. Yang, Kautz, '\n",
      "                                               'and Hatamizadeh (2024) '\n",
      "                                               'improved the DeltaNets by '\n",
      "                                               'adding a forget gate.\\n'\n",
      "                                               'Memory Modules. Memory has '\n",
      "                                               'always been one of the core '\n",
      "                                               'parts of the neural network '\n",
      "                                               'designs (Graves, Wayne,\\n'\n",
      "                                               'and Danihelka 2014; JH '\n",
      "                                               'Schmidhuber 1992; Jürgen '\n",
      "                                               'Schmidhuber and Hochreiter '\n",
      "                                               '1997; J. Zhang et al. 2024). '\n",
      "                                               'The idea of\\n'\n",
      "                                               'seeing linear layers as the '\n",
      "                                               'key-value (associative) memory '\n",
      "                                               'system backs to fast weight '\n",
      "                                               'programs, in which dynamic '\n",
      "                                               'fast\\n'\n",
      "                                               'programs are incorporated into '\n",
      "                                               'recurrent neural networks to '\n",
      "                                               'serve as writable memory (JH '\n",
      "                                               'Schmidhuber 1992). The two\\n'\n",
      "                                               'learning rules of Hebbian '\n",
      "                                               '(Hebb 2005) and delta (Prados '\n",
      "                                               'and Kak 1989) are the most '\n",
      "                                               'popular learning rules for '\n",
      "                                               'fast weight\\n'\n",
      "                                               'programs, which have been '\n",
      "                                               'extensively explored in '\n",
      "                                               'various studies (Irie, Schlag, '\n",
      "                                               'et al. 2021; Munkhdalai, '\n",
      "                                               'Sordoni, et al.\\n'\n",
      "                                               '2019; Munkhdalai and H. Yu '\n",
      "                                               '2017; Schlag, Irie, and Jürgen '\n",
      "                                               'Schmidhuber 2021; JH '\n",
      "                                               'Schmidhuber 1992; S. Yang, '\n",
      "                                               'Kautz, and\\n'\n",
      "                                               'Hatamizadeh 2024; S. Yang, B. '\n",
      "                                               'Wang, Yu Zhang, et al. 2024). '\n",
      "                                               'All these models, however, are '\n",
      "                                               'based on momentary surprise,\\n'\n",
      "                                               'missing the token flow in the '\n",
      "                                               'sequences (see Section 3.1), '\n",
      "                                               'and most of them lacks a '\n",
      "                                               'forgetting gate, resulting in '\n",
      "                                               'a poor\\n'\n",
      "                                               'memory management.\\n'\n",
      "                                               'We further discuss the '\n",
      "                                               'connection of our '\n",
      "                                               'architectures with recent '\n",
      "                                               'models in Appendix C. '\n",
      "                                               'Additional related work are\\n'\n",
      "                                               'discussed in Appendix A.\\n'\n",
      "                                               '4\\n'\n",
      "                                               '3 Learning to Memorize at Test '\n",
      "                                               'Time\\n'\n",
      "                                               'T\\n'\n",
      "                                               'o overcome the lack of '\n",
      "                                               'long-term memory and to enable '\n",
      "                                               'the model to learn, forget, '\n",
      "                                               'and retrieve information, in\\n'\n",
      "                                               'this section, we present a '\n",
      "                                               'neural long-term memory '\n",
      "                                               'module, which is a meta models '\n",
      "                                               'that learns to memorize at\\n'\n",
      "                                               'test time. In Section 3.1, we '\n",
      "                                               'first discuss the motivation '\n",
      "                                               'and the design of the neural '\n",
      "                                               'memory. In Section 3.2, we\\n'\n",
      "                                               'discuss how our architecture '\n",
      "                                               'design can benefit from a fast '\n",
      "                                               'and parallelizable training. '\n",
      "                                               'Finally, in Section 3.3, we '\n",
      "                                               'augment\\n'\n",
      "                                               'our architecture using '\n",
      "                                               'persistent memory module, in '\n",
      "                                               'which we use learnable but '\n",
      "                                               'data-independent parameters to '\n",
      "                                               'learn\\n'\n",
      "                                               'meta information about the '\n",
      "                                               'task.\\n'\n",
      "                                               '3.1 Long-term Memory\\n'\n",
      "                                               'To design a neural long-term '\n",
      "                                               'memory module, we need a model '\n",
      "                                               'that can encode the '\n",
      "                                               'abstraction of the past '\n",
      "                                               'history into its\\n'\n",
      "                                               'parameters. An example of this '\n",
      "                                               'can be LLMs that are shown to '\n",
      "                                               'be memorizing their training '\n",
      "                                               'data (Leybzon and Kervadec\\n'\n",
      "                                               '2024; Schwarzschild et al. '\n",
      "                                               '2024; Staab et al. 2024). '\n",
      "                                               'Therefore, a simple idea is to '\n",
      "                                               'train a neural network and '\n",
      "                                               'expect it to\\n'\n",
      "                                               'memorize its training data. '\n",
      "                                               'Memorization, however, has '\n",
      "                                               'almost always been known as an '\n",
      "                                               'undesirable phenomena in\\n'\n",
      "                                               'neural networks as it limits '\n",
      "                                               'the model generalization '\n",
      "                                               '(Bayat et al. 2024), causes '\n",
      "                                               'privacy concerns (Staab et al. '\n",
      "                                               '2024), and\\n'\n",
      "                                               'so results in poor performance '\n",
      "                                               'at test time. Moreover, the '\n",
      "                                               'memorization of the training '\n",
      "                                               'data might not be helpful at '\n",
      "                                               'test\\n'\n",
      "                                               'time, in which the data might '\n",
      "                                               'be out-of-distribution. We '\n",
      "                                               'argue that, we need an online '\n",
      "                                               'meta-model that learns how to\\n'\n",
      "                                               'memorize/forget the data at '\n",
      "                                               'test time. In this setup, the '\n",
      "                                               'model is learning a function '\n",
      "                                               'that is capable of '\n",
      "                                               'memorization, but it\\n'\n",
      "                                               'is not overfitting to the '\n",
      "                                               'training data, resulting in a '\n",
      "                                               'better generalization at test '\n",
      "                                               'time.\\n'\n",
      "                                               'Learning Process and Surprise '\n",
      "                                               'Metric. The key idea to train '\n",
      "                                               'a long-term memory is to treat '\n",
      "                                               'its training as an online\\n'\n",
      "                                               'learning problem, in which we '\n",
      "                                               'aim to compress the past '\n",
      "                                               'information 𝑥1,...,𝑥 𝑡−1 into '\n",
      "                                               'the parameters of our '\n",
      "                                               'long-term\\n'\n",
      "                                               'neural memory module M𝑡. As '\n",
      "                                               'discussed earlier, an event '\n",
      "                                               'that violates the expectations '\n",
      "                                               '(i.e., is surprising) is more\\n'\n",
      "                                               'memorable for humans (Mandler '\n",
      "                                               '2014). Inspired by this, a '\n",
      "                                               'simple definition of surprise '\n",
      "                                               'for a model can be its '\n",
      "                                               'gradient with\\n'\n",
      "                                               'respect to the input. The '\n",
      "                                               'larger the gradient is, the '\n",
      "                                               'more different the input data '\n",
      "                                               'is from the past data. '\n",
      "                                               'Accordingly, using\\n'\n",
      "                                               'this surprise score, we can '\n",
      "                                               'update the memory as:\\n'\n",
      "                                               'M𝑡 = M𝑡−1 −𝜃𝑡 ∇ℓ(M𝑡−1; '\n",
      "                                               '𝑥𝑡)|           {z           }\\n'\n",
      "                                               'Surprise\\n'\n",
      "                                               '. (8)\\n'\n",
      "                                               'This surprise metric, however, '\n",
      "                                               'can result in missing '\n",
      "                                               'important information that '\n",
      "                                               'comes after a big surprising '\n",
      "                                               'moment.\\n'\n",
      "                                               'That is, the gradient can '\n",
      "                                               'become extremely small after '\n",
      "                                               'several surprising steps, '\n",
      "                                               'leading to stocking in a flat '\n",
      "                                               'area (i.e., local\\n'\n",
      "                                               'minima), and missing '\n",
      "                                               'information about some parts '\n",
      "                                               'of the sequence. From the '\n",
      "                                               'human memory perspective, an '\n",
      "                                               'event might\\n'\n",
      "                                               'not consistently surprise us '\n",
      "                                               'through a long-period of time '\n",
      "                                               'although it is memorable. The '\n",
      "                                               'reason is that the initial '\n",
      "                                               'moment\\n'\n",
      "                                               'is surprising enough to get '\n",
      "                                               'our attention through a long '\n",
      "                                               'time frame, leading to '\n",
      "                                               'memorizing the entire time '\n",
      "                                               'frame. To\\n'\n",
      "                                               'improve the above surprise '\n",
      "                                               'metric (Equation 8), we break '\n",
      "                                               'the surprise metric into (1) '\n",
      "                                               'past surprise , which measures '\n",
      "                                               'the\\n'\n",
      "                                               'surprise amount of a very '\n",
      "                                               'recent past; and (2) momentary '\n",
      "                                               'surprise , which measures the '\n",
      "                                               'surprise of incoming data:\\n'\n",
      "                                               'M𝑡 = M𝑡−1 +𝑆𝑡, (9)\\n'\n",
      "                                               '𝑆𝑡 = 𝜂𝑡 𝑆𝑡−1\\n'\n",
      "                                               '|{z}\\n'\n",
      "                                               'Past Surprise\\n'\n",
      "                                               '−𝜃𝑡 ∇ℓ(𝑀𝑡−1; 𝑥𝑡)|          '\n",
      "                                               '{z          }\\n'\n",
      "                                               'Momentary Surprise\\n'\n",
      "                                               '. (10)\\n'\n",
      "                                               'Interestingly, this '\n",
      "                                               'formulation is similar to '\n",
      "                                               'gradient descent with '\n",
      "                                               'momentum, where𝑆𝑡 is the '\n",
      "                                               'momentum element. Therefore,\\n'\n",
      "                                               'the momentum here act as a '\n",
      "                                               'memory of surprise across time '\n",
      "                                               '(sequence length). In this '\n",
      "                                               'formulation, the term 𝜂𝑡 is a\\n'\n",
      "                                               'data-dependent surprise decay '\n",
      "                                               '(a function of 𝑥𝑡), '\n",
      "                                               'controlling how surprise '\n",
      "                                               'decays over time, and the term '\n",
      "                                               '𝜃𝑡 is controlling\\n'\n",
      "                                               'how much of momentary surprise '\n",
      "                                               'should be incorporated into '\n",
      "                                               'the final surprise metric in a '\n",
      "                                               'data-dependent manner. This\\n'\n",
      "                                               'data-dependency is '\n",
      "                                               'particularly important in this '\n",
      "                                               'design: While surprise of '\n",
      "                                               'previous tokens might be '\n",
      "                                               'needed to affect\\n'\n",
      "                                               'the surprise of the next '\n",
      "                                               'token, it is mostly valid if '\n",
      "                                               'all tokens are relevant and '\n",
      "                                               'are in the same context. '\n",
      "                                               'Accordingly, a\\n'\n",
      "                                               'data-dependent 𝜂can control if '\n",
      "                                               'memory needs to: (1) ignore '\n",
      "                                               'the last surprise by setting '\n",
      "                                               '𝜂𝑡 →0 (possibly due to the '\n",
      "                                               'change\\n'\n",
      "                                               'of context), or (2) fully '\n",
      "                                               'incorporate the last surprise '\n",
      "                                               'by setting 𝜂𝑡 →1 (possibly as '\n",
      "                                               'the token is highly relevant '\n",
      "                                               'to its recent\\n'\n",
      "                                               'past tokens).\\n'\n",
      "                                               'Objective. Our above surprise '\n",
      "                                               'metric is based on a loss '\n",
      "                                               'function ℓ(.; .), which is the '\n",
      "                                               'objective that our memory is '\n",
      "                                               'learning\\n'\n",
      "                                               'to act as it at test time. '\n",
      "                                               'That is, our memory module is '\n",
      "                                               'a meta model that learns a '\n",
      "                                               'function based on the loss '\n",
      "                                               'function ℓ(.; .).\\n'\n",
      "                                               '5\\n'\n",
      "                                               'In this work, we focus on '\n",
      "                                               'associative memory , in which '\n",
      "                                               'we aim to store the past data '\n",
      "                                               'as the pairs of keys and '\n",
      "                                               'values. Given\\n'\n",
      "                                               '𝑥𝑡, similar to Transformers '\n",
      "                                               '(Vaswani et al. 2017), we use '\n",
      "                                               'two linear layers to project𝑥𝑡 '\n",
      "                                               'into a key and value:\\n'\n",
      "                                               'k𝑡 = 𝑥𝑡𝑊𝐾, v𝑡 = 𝑥𝑡𝑊𝑉, (11)\\n'\n",
      "                                               'where 𝑊𝐾 and 𝑊𝑉 ∈R𝑑in ×𝑑in . '\n",
      "                                               'Next, we expect our memory '\n",
      "                                               'module to learn the '\n",
      "                                               'associations between keys and '\n",
      "                                               'values. To\\n'\n",
      "                                               'this end, we define the loss '\n",
      "                                               'as follows:\\n'\n",
      "                                               'ℓ(M𝑡−1; 𝑥𝑡)= ∥M𝑡−1 (k𝑡)−v𝑡∥2\\n'\n",
      "                                               '2 (12)\\n'\n",
      "                                               'By optimizing the above loss '\n",
      "                                               'function in the inner-loop of '\n",
      "                                               'our meta model (memory), the '\n",
      "                                               'model learns how to memorize\\n'\n",
      "                                               'the mapping between keys and '\n",
      "                                               'values at test time. Note '\n",
      "                                               'that, similar to meta-learning '\n",
      "                                               'models (Nichol 2018; Zintgraf '\n",
      "                                               'et al.\\n'\n",
      "                                               '2019), training of the memory '\n",
      "                                               'is in the inner-loop, and so '\n",
      "                                               'parameters 𝑊𝐾 and 𝑊𝑉 are '\n",
      "                                               'hyperparameters in the above '\n",
      "                                               'loss\\n'\n",
      "                                               'function. Accordingly, in the '\n",
      "                                               'inner loop, we optimize M’s '\n",
      "                                               'weights, while in the '\n",
      "                                               'outer-loop, we optimize other '\n",
      "                                               'parameters\\n'\n",
      "                                               'of the entire architecture.\\n'\n",
      "                                               'Forgetting Mechanism. When '\n",
      "                                               'dealing with very large '\n",
      "                                               'sequences (e.g., millions of '\n",
      "                                               'tokens), it is crucial to '\n",
      "                                               'manage which\\n'\n",
      "                                               'past information should be '\n",
      "                                               'forgotten–even with a deep or '\n",
      "                                               'a very large matrix-valued '\n",
      "                                               'memory. To this end, we use '\n",
      "                                               'an\\n'\n",
      "                                               'adaptive forgetting mechanism '\n",
      "                                               'that allows the memory to '\n",
      "                                               'forget the information that is '\n",
      "                                               'not needed anymore, resulting '\n",
      "                                               'in\\n'\n",
      "                                               'better managing the memory’s '\n",
      "                                               'limited capacity. That is, '\n",
      "                                               'given the next token 𝑥𝑡, we '\n",
      "                                               'modify the update rule as:\\n'\n",
      "                                               'M𝑡 = (1 −𝛼𝑡)M𝑡−1 +𝑆𝑡, (13)\\n'\n",
      "                                               '𝑆𝑡 = 𝜂𝑡𝑆𝑡−1 −𝜃𝑡 ∇ℓ(𝑀𝑡−1; 𝑥𝑡), '\n",
      "                                               '(14)\\n'\n",
      "                                               'where 𝛼𝑡 ∈[0,1]is the gating '\n",
      "                                               'mechanism that flexibly '\n",
      "                                               'controls the memory; i.e., '\n",
      "                                               'decides how much information '\n",
      "                                               'should be\\n'\n",
      "                                               'forgotten. For example, it can '\n",
      "                                               'update the memory without '\n",
      "                                               'affecting the past abstraction '\n",
      "                                               'by letting 𝛼𝑡 →0, and can '\n",
      "                                               'clear\\n'\n",
      "                                               'the entire memory by letting '\n",
      "                                               '𝛼𝑡 →1. Later in this section, '\n",
      "                                               'we show that this weight decay '\n",
      "                                               'mechanism is closely related '\n",
      "                                               'to\\n'\n",
      "                                               'the gating mechanism in modern '\n",
      "                                               'RNNs (Dao and Gu 2024; Orvieto '\n",
      "                                               'et al. 2023).\\n'\n",
      "                                               'Memory Architecture. In this '\n",
      "                                               'paper, we focus on simple MLPs '\n",
      "                                               'with 𝐿M ≥1 layers as the '\n",
      "                                               'architecture of our long-term\\n'\n",
      "                                               'memory. The main reason behind '\n",
      "                                               'this choice is that we want to '\n",
      "                                               'focus on better motivating the '\n",
      "                                               'design of the long-term\\n'\n",
      "                                               'memory and ways that it can be '\n",
      "                                               'incorporated into an '\n",
      "                                               'architecture. However, our '\n",
      "                                               'formulation and architectural '\n",
      "                                               'design\\n'\n",
      "                                               'opens a new research direction '\n",
      "                                               'to design neural architectures '\n",
      "                                               'that are more effective and '\n",
      "                                               'efficient in memorization of '\n",
      "                                               'data.\\n'\n",
      "                                               'Recently, there has been a '\n",
      "                                               'promising line of work to '\n",
      "                                               'design such architectures '\n",
      "                                               '(Berges et al. 2024; Cetin et '\n",
      "                                               'al. 2024; J. Zhang\\n'\n",
      "                                               'et al. 2024), which '\n",
      "                                               'incorporating them into our '\n",
      "                                               'framework (i.e., replacing '\n",
      "                                               'simple MLPs with such '\n",
      "                                               'architectures) can be an\\n'\n",
      "                                               'interesting future work.\\n'\n",
      "                                               'When using vector-valued or '\n",
      "                                               'matrix-valued memory (De et '\n",
      "                                               'al. 2024; Orvieto et al. 2023; '\n",
      "                                               'S. Yang, B. Wang, Shen, et\\n'\n",
      "                                               'al. 2024), the memory module '\n",
      "                                               'is compressing the past data '\n",
      "                                               'and fit it into a line. That '\n",
      "                                               'is, from the meta learning or\\n'\n",
      "                                               'online learning perspective '\n",
      "                                               '(Yu Sun et al. 2024), using a '\n",
      "                                               'matrix-valued memory M = 𝑊 '\n",
      "                                               '∈R𝑑in ×𝑑in is equivalent to\\n'\n",
      "                                               'optimize ℓ(𝑊𝑡−1; 𝑥𝑡)= ∥𝑊𝑡−1k𝑡 '\n",
      "                                               '−v𝑡∥2\\n'\n",
      "                                               '2, which is an online linear '\n",
      "                                               'regression objective and so '\n",
      "                                               'the optimal solution assumes\\n'\n",
      "                                               'the underlying dependency of '\n",
      "                                               'historical data is linear. On '\n",
      "                                               'the other hand, we argue that '\n",
      "                                               'deep memory modules (i.e.,\\n'\n",
      "                                               '𝐿M ≥2) . Aligning with the '\n",
      "                                               'theoretical results that MLPs '\n",
      "                                               'with at least two layers are '\n",
      "                                               'strictly more expressive than '\n",
      "                                               'linear\\n'\n",
      "                                               'models (Hornik, Stinchcombe, '\n",
      "                                               'and White 1989), in Section '\n",
      "                                               '5.5, we show that deep memory '\n",
      "                                               'modules are more effective in\\n'\n",
      "                                               'practice.\\n'\n",
      "                                               'Retrieving a Memory. In the '\n",
      "                                               'above, we discuss how one can '\n",
      "                                               'design and train a long-term '\n",
      "                                               'memory module that learns to\\n'\n",
      "                                               'memorize at test time. A key '\n",
      "                                               'remaining question is: How one '\n",
      "                                               'can retrieve information from '\n",
      "                                               'the memory? We simply use the\\n'\n",
      "                                               'forward pass without weight '\n",
      "                                               'update (i.e., inference) to '\n",
      "                                               'retrieve a memory correspond '\n",
      "                                               'to a query. Formally, given an '\n",
      "                                               'input\\n'\n",
      "                                               '𝑥𝑡, we use a linear layer 𝑊𝑄 '\n",
      "                                               'to project the input, i.e., q𝑡 '\n",
      "                                               '= 𝑥𝑡𝑊𝑄 and retrieve the '\n",
      "                                               'corresponding (or useful) '\n",
      "                                               'information\\n'\n",
      "                                               'from the memory 𝑦𝑡 by:\\n'\n",
      "                                               '𝑦𝑡 = M∗(q𝑡). (15)\\n'\n",
      "                                               '6\\n'\n",
      "                                               'Figure 1: The illustration of '\n",
      "                                               'how the training of neural '\n",
      "                                               'memory can be done in parallel '\n",
      "                                               'and using matmuls.\\n'\n",
      "                                               '3.2 How to Parallelize the '\n",
      "                                               'Long-term Memory Training\\n'\n",
      "                                               'As discussed above, the design '\n",
      "                                               'of our long-term memory module '\n",
      "                                               'is equivalent to training a '\n",
      "                                               'meta model by optimizing\\n'\n",
      "                                               'associative memory loss '\n",
      "                                               'function ℓ(M𝑡−1; 𝑥𝑡)= ∥M𝑡−1 '\n",
      "                                               '(k𝑡)−v𝑡∥2\\n'\n",
      "                                               '2 using gradient descent with '\n",
      "                                               'momentum and weight\\n'\n",
      "                                               'decay. Therefore, in theory, '\n",
      "                                               'the training of long-term '\n",
      "                                               'memory module requires '\n",
      "                                               'O(𝑁)FLOPs, where 𝑁 is the '\n",
      "                                               'sequence\\n'\n",
      "                                               'length. However, in practice, '\n",
      "                                               'we need to parallelize the '\n",
      "                                               'training process and to fully '\n",
      "                                               'take advantage of hardware '\n",
      "                                               'accelerators\\n'\n",
      "                                               '(e.g., TPUs, GPUs), we need to '\n",
      "                                               'tensorize the process and use '\n",
      "                                               'more matmuls.\\n'\n",
      "                                               'Next, we show that calculating '\n",
      "                                               'the weights in the inner loop '\n",
      "                                               'with mini-batch gradient '\n",
      "                                               'descent, data-dependent '\n",
      "                                               'learning\\n'\n",
      "                                               'rate, and weight decay can be '\n",
      "                                               'reformulated so that it uses '\n",
      "                                               'only matmuls and sum. We build '\n",
      "                                               'upon the work of Yu Sun et '\n",
      "                                               'al.\\n'\n",
      "                                               '(2024) that shows forward pass '\n",
      "                                               'of a model optimizing with the '\n",
      "                                               'mini-batch gradient descent '\n",
      "                                               '(with constant learning rate)\\n'\n",
      "                                               'can be calculated using '\n",
      "                                               'matmuls. We can split the '\n",
      "                                               'sequence into chunks of size 𝑏 '\n",
      "                                               '≥1, and write the mini-batch '\n",
      "                                               'gradient\\n'\n",
      "                                               'descent as:\\n'\n",
      "                                               'M𝑡 = (1 −𝛼𝑡)M𝑡−1 −𝜃𝑡∇ℓ(M𝑡−1; '\n",
      "                                               '𝑥𝑡)= 𝛽𝑡M0 −\\n'\n",
      "                                               '𝑡∑︁\\n'\n",
      "                                               '𝑖=1\\n'\n",
      "                                               '𝜃𝑖\\n'\n",
      "                                               '𝛽𝑡\\n'\n",
      "                                               '𝛽𝑖\\n'\n",
      "                                               '∇ℓ(M𝑡′; 𝑥𝑖), (16)\\n'\n",
      "                                               'where 𝑡′= 𝑡 −mod(𝑡,𝑏), and 𝛽𝑖 '\n",
      "                                               '= Î𝑖\\n'\n",
      "                                               '𝑗=1 (1 −𝛼𝑗). For the sake of '\n",
      "                                               'simplicity, we focus on the '\n",
      "                                               'first chunk, i.e., 𝑡 = 𝑏and '\n",
      "                                               'so\\n'\n",
      "                                               '𝑡′= 0. Also, we explain the '\n",
      "                                               'process for the case that M𝑡 = '\n",
      "                                               '𝑊𝑡 is linear. The process for '\n",
      "                                               'MLPs with 𝑁𝑝 ≥2 is similar. '\n",
      "                                               'Using\\n'\n",
      "                                               'our loss function, we have:\\n'\n",
      "                                               '∇ℓ(𝑊0; 𝑥𝑡)= (𝑊0𝑥𝑡 −𝑥𝑡)𝑥⊤\\n'\n",
      "                                               '𝑡 ⇒\\n'\n",
      "                                               '𝑏∑︁\\n'\n",
      "                                               '𝑖=1\\n'\n",
      "                                               '𝜃𝑖\\n'\n",
      "                                               '𝛽𝑏\\n'\n",
      "                                               '𝛽𝑖\\n'\n",
      "                                               '∇ℓ(𝑊0; 𝑥𝑖)= Θ𝑏B𝑏(𝑊0𝑋 −𝑋)𝑋⊤, '\n",
      "                                               '(17)\\n'\n",
      "                                               'where Θ𝑏 = diag \\x00\\x02𝜃1 𝜃2 '\n",
      "                                               '... 𝜃 𝑏\\n'\n",
      "                                               '\\x03\\x01 and B𝑏 is defined '\n",
      "                                               'analogously on 𝛽𝑏\\n'\n",
      "                                               '𝛽𝑖\\n'\n",
      "                                               's. Note that, we do not need '\n",
      "                                               'to store allΘ𝑘𝑏 and\\n'\n",
      "                                               'B𝑘𝑏 for 𝑘 = 1,...,𝑁 /𝑏, '\n",
      "                                               'instead, we store these '\n",
      "                                               'matrices for each chunk, '\n",
      "                                               'resulting in using less '\n",
      "                                               'memory. Next, we extend\\n'\n",
      "                                               'this representation so we can '\n",
      "                                               'also incorporate the momentum '\n",
      "                                               'term. In a chunk wise gradient '\n",
      "                                               'descent with momentum, if\\n'\n",
      "                                               'we look at the momentum term, '\n",
      "                                               'we have:\\n'\n",
      "                                               '𝑆𝑡 = 𝜂𝑡𝑆𝑡−1 −𝜃𝑡 𝑢𝑡, (18)\\n'\n",
      "                                               'where 𝑢𝑡 = ∇ℓ(𝑀𝑡′; 𝑥𝑡). Note '\n",
      "                                               'that, we can compute all 𝑢𝑡 at '\n",
      "                                               'the same time, and so Equation '\n",
      "                                               '18 is a linear recurrence\\n'\n",
      "                                               'with 𝑢𝑡 as an input, 𝑆𝑡 as the '\n",
      "                                               'hidden state, and 𝜂𝑡 as '\n",
      "                                               'input-dependent transition '\n",
      "                                               'value. Accordingly, we can use '\n",
      "                                               'parallel\\n'\n",
      "                                               'associative scan (J. T. Smith, '\n",
      "                                               'Warrington, and Linderman '\n",
      "                                               '2023) to calculate𝑆𝑡s in this '\n",
      "                                               'chunk.\\n'\n",
      "                                               'Parameters as the Function of '\n",
      "                                               'Chunks. Instead of making '\n",
      "                                               'parameters like𝛼𝑡,𝜃𝑡, and 𝜂𝑡 '\n",
      "                                               'input-dependent (i.e., a '\n",
      "                                               'function\\n'\n",
      "                                               'of token 𝑥𝑡), we can make them '\n",
      "                                               'functions of their chunk. '\n",
      "                                               'Despite losing expressive '\n",
      "                                               'power, this formulation can '\n",
      "                                               'help to\\n'\n",
      "                                               'make the training even faster. '\n",
      "                                               'In this case, we are using the '\n",
      "                                               'same value for each of 𝛼, 𝜃, '\n",
      "                                               'and 𝜂in each chunk. '\n",
      "                                               'Accordingly,\\n'\n",
      "                                               'in Equation 17, we can store Θ '\n",
      "                                               'using a single scaler. '\n",
      "                                               'Similarly we can make Equation '\n",
      "                                               '18 faster. That is, when 𝜂and '\n",
      "                                               '𝜃 are\\n'\n",
      "                                               'learnable but time-invariant '\n",
      "                                               'inside each chunk, this '\n",
      "                                               'equation becomes a linear '\n",
      "                                               'time-invariant system (LTI), '\n",
      "                                               'which can be\\n'\n",
      "                                               'computed by a global '\n",
      "                                               'convolution (Gu, Goel, and Re '\n",
      "                                               '2022). In our experiments, we '\n",
      "                                               'make these parameters as the '\n",
      "                                               'functions\\n'\n",
      "                                               'of tokens. However, such '\n",
      "                                               'simplifications (i.e., as the '\n",
      "                                               'function of chunks) can be the '\n",
      "                                               'interest of future work to '\n",
      "                                               'training\\n'\n",
      "                                               'larger models in more '\n",
      "                                               'efficient manner.\\n'\n",
      "                                               '7\\n'\n",
      "                                               'Figure 2: Memory as a Context '\n",
      "                                               '(MAC) Architecture. This '\n",
      "                                               'architecture includes three '\n",
      "                                               'branches of (1) core, (2) '\n",
      "                                               'contextual\\n'\n",
      "                                               '(long-term) memory, and (3) '\n",
      "                                               'persistent memory. The core '\n",
      "                                               'branch concatenates the '\n",
      "                                               'corresponding long-term and '\n",
      "                                               'persistent\\n'\n",
      "                                               'memories with the input '\n",
      "                                               'sequence. Next, attention '\n",
      "                                               'performs on the sequence and '\n",
      "                                               'decides what part of the '\n",
      "                                               'information\\n'\n",
      "                                               'should store in the long-term '\n",
      "                                               'memory. At the test time, '\n",
      "                                               'parameters corresponds to '\n",
      "                                               'contextual memory are still '\n",
      "                                               'learning,\\n'\n",
      "                                               'parameters corresponds to the '\n",
      "                                               'core branch are responsible '\n",
      "                                               'for in-context learning, and '\n",
      "                                               'parameters of persistent '\n",
      "                                               'memory\\n'\n",
      "                                               'are responsible to store the '\n",
      "                                               'knowledge about tasks and so '\n",
      "                                               'are fixed.\\n'\n",
      "                                               '3.3 Persistent Memory\\n'\n",
      "                                               'Our long-term memory can also '\n",
      "                                               'be seen as a contextual '\n",
      "                                               'memory, meaning that the '\n",
      "                                               'output is fully depend on the '\n",
      "                                               'context.\\n'\n",
      "                                               'Therefore, in addition to our '\n",
      "                                               'long-term memory, we also use '\n",
      "                                               'a set of learnable but '\n",
      "                                               'input-independent parameters '\n",
      "                                               'to act as\\n'\n",
      "                                               'task-related memory. This type '\n",
      "                                               'of memory has been referred to '\n",
      "                                               'as persistent or meta-memory '\n",
      "                                               'in the literature (X. Dong\\n'\n",
      "                                               'et al. 2024; Sukhbaatar, '\n",
      "                                               'Grave, et al. 2019). Given 𝑁𝑝 '\n",
      "                                               '≥1, we use learnable '\n",
      "                                               'parameters 𝑃 =\\n'\n",
      "                                               '\\x02𝑝1 𝑝2 ... 𝑝 𝑁𝑝\\n'\n",
      "                                               '\\x03\\n'\n",
      "                                               'and\\n'\n",
      "                                               'append it to the start of our '\n",
      "                                               'sequence: i.e., given a '\n",
      "                                               'context window size of 𝑁, we '\n",
      "                                               'modify the input as:\\n'\n",
      "                                               '𝑥new =\\n'\n",
      "                                               '\\x02𝑝1 𝑝2 ... 𝑝 𝑁𝑝\\n'\n",
      "                                               '\\x03\\n'\n",
      "                                               '|| 𝑥, (19)\\n'\n",
      "                                               'where ||is concatenation. '\n",
      "                                               'Next, we discuss the '\n",
      "                                               'motivation of persistent '\n",
      "                                               'memory from three '\n",
      "                                               'perspective:\\n'\n",
      "                                               'Memory Perspective. As '\n",
      "                                               'discussed earlier, our neural '\n",
      "                                               'long-term memory is a '\n",
      "                                               'contextual memory, in which '\n",
      "                                               'all parameters\\n'\n",
      "                                               'are input-dependent. An '\n",
      "                                               'effective memory system, '\n",
      "                                               'however, also needs '\n",
      "                                               'input-independent parameters '\n",
      "                                               'to store the\\n'\n",
      "                                               'abstraction of the task '\n",
      "                                               'knowledge. That is, mastering '\n",
      "                                               'a task requires the '\n",
      "                                               'memorization of the knowledge '\n",
      "                                               'that how the task\\n'\n",
      "                                               'can be done, and these '\n",
      "                                               'parameters are responsible for '\n",
      "                                               'storing such knowledge.\\n'\n",
      "                                               'Feedforward Network '\n",
      "                                               'Perspective. In the '\n",
      "                                               'Transformer architectures, '\n",
      "                                               'there are fully connected '\n",
      "                                               'layers after the attention\\n'\n",
      "                                               'module, which are shown to be '\n",
      "                                               'similar to attention weights '\n",
      "                                               'but with data-independent '\n",
      "                                               'parameters. That is, '\n",
      "                                               'Sukhbaatar,\\n'\n",
      "                                               'Grave, et al. (2019) showed '\n",
      "                                               'that replacing the ReLU in '\n",
      "                                               'fully connected layers with '\n",
      "                                               'Softmax can results in an '\n",
      "                                               'attention-like\\n'\n",
      "                                               'weights, in which weights are '\n",
      "                                               'data-independent:\\n'\n",
      "                                               '𝐹𝐹𝑁 (𝑥)= 𝑊𝑉 Softmax (𝑊𝐾𝑥). '\n",
      "                                               '(20)\\n'\n",
      "                                               'In fact, 𝑊𝐾 and 𝑊𝑉 are acting '\n",
      "                                               'similar to 𝐾 and 𝑉 matrices in '\n",
      "                                               'attention module when they are '\n",
      "                                               'input-independent. The\\n'\n",
      "                                               'persistent memory weights are '\n",
      "                                               'expected to have the same '\n",
      "                                               'functionality, meaning that '\n",
      "                                               'using them in the first part '\n",
      "                                               'of the\\n'\n",
      "                                               'sequence leads to having '\n",
      "                                               'input-independent attention '\n",
      "                                               'weights (Sukhbaatar, Grave, et '\n",
      "                                               'al. 2019).\\n'\n",
      "                                               'Technical Perspective. '\n",
      "                                               'Attention with causal mask has '\n",
      "                                               'implicit bias toward initial '\n",
      "                                               'tokens in the sequence, and so '\n",
      "                                               'attention\\n'\n",
      "                                               'weights are almost always '\n",
      "                                               'highly active for initial '\n",
      "                                               'tokens, resulting in '\n",
      "                                               'performance damage. From the '\n",
      "                                               'technical perspective,\\n'\n",
      "                                               'these learnable parameters at '\n",
      "                                               'the start of the sequence can '\n",
      "                                               'mitigate such effect by '\n",
      "                                               'redistributing the attention '\n",
      "                                               'weights\\n'\n",
      "                                               'more effectively (Han et al. '\n",
      "                                               '2024; Xiao et al. 2024).\\n'\n",
      "                                               '8\\n'\n",
      "                                               '(a) Memory as a Context (MAC). '\n",
      "                                               'We segment the sequence\\n'\n",
      "                                               'and use full causal attention '\n",
      "                                               'in each window. Again, the '\n",
      "                                               'first\\n'\n",
      "                                               '𝑁𝑝 tokens are persistent '\n",
      "                                               'memory and the next 𝑁𝑙 are '\n",
      "                                               'long-term\\n'\n",
      "                                               'memory tokens\\n'\n",
      "                                               '(b) Memory as Gating (MAG). We '\n",
      "                                               'use sliding window attention\\n'\n",
      "                                               '(SWA) as a short-term memory '\n",
      "                                               'and our neural memory module\\n'\n",
      "                                               'as a long-term memory, '\n",
      "                                               'combining by a gating.\\n'\n",
      "                                               'Figure 3: Attention masks for '\n",
      "                                               'different variants of Titans.\\n'\n",
      "                                               '4 How to Incorporate Memory?\\n'\n",
      "                                               'A\\n'\n",
      "                                               'n important question that '\n",
      "                                               'remained unanswered is: How '\n",
      "                                               'one can effectively and '\n",
      "                                               'efficiently incorporate the\\n'\n",
      "                                               'designed neural memory into a '\n",
      "                                               'deep learning architecture? As '\n",
      "                                               'discussed earlier, from a '\n",
      "                                               'memory perspective,\\n'\n",
      "                                               'the pair of K and V matrices '\n",
      "                                               'in transformers can be '\n",
      "                                               'interpreted as an associative '\n",
      "                                               'memory block. Due to their\\n'\n",
      "                                               'accurate modeling of '\n",
      "                                               'dependencies and so their '\n",
      "                                               'limited context window, we '\n",
      "                                               'interpret them as short-term '\n",
      "                                               'memory modules,\\n'\n",
      "                                               'attending to the current '\n",
      "                                               'context window size. On the '\n",
      "                                               'other hand, our neural memory '\n",
      "                                               'with the ability to '\n",
      "                                               'continuously\\n'\n",
      "                                               'learn from data and store it '\n",
      "                                               'in its weights can play the '\n",
      "                                               'role of a a long-term memory. '\n",
      "                                               'In this section, we aim to '\n",
      "                                               'answer\\n'\n",
      "                                               'the above question by '\n",
      "                                               'proposing three different '\n",
      "                                               'variants of Titans. Later in '\n",
      "                                               'our experiments, we show that '\n",
      "                                               'each of these\\n'\n",
      "                                               'variants has its own '\n",
      "                                               'advantages/disadvantages and '\n",
      "                                               'also can show a trade-off '\n",
      "                                               'between the efficiency and '\n",
      "                                               'effectiveness in\\n'\n",
      "                                               'very long-contexts.\\n'\n",
      "                                               '4.1 Memory as a Context\\n'\n",
      "                                               'In the first architecture '\n",
      "                                               'design (see Figure 2), we '\n",
      "                                               'treat the memory as a context '\n",
      "                                               'to the current information. '\n",
      "                                               'That is, given\\n'\n",
      "                                               'a long sequence 𝑥 ∈R𝑁×𝑑in , we '\n",
      "                                               'first chunk the sequence into '\n",
      "                                               'fixed-size segments S(𝑖) for 𝑖 '\n",
      "                                               '= 1,...,𝑁 /𝐶. Given the\\n'\n",
      "                                               'incoming segment S(𝑡), we '\n",
      "                                               'consider it as the current '\n",
      "                                               'context and its past segment '\n",
      "                                               'as the historical information. '\n",
      "                                               'Therefore,\\n'\n",
      "                                               'let M𝑡−1 be the state of '\n",
      "                                               'long-term memory before '\n",
      "                                               'segment S(𝑡), we use the input '\n",
      "                                               'context as the query to the '\n",
      "                                               'memory\\n'\n",
      "                                               'M𝑡−1 to retrieve the '\n",
      "                                               'corresponding information from '\n",
      "                                               'the long-term memory. That is, '\n",
      "                                               'we retrieve the past '\n",
      "                                               'information that\\n'\n",
      "                                               'corresponds to S(𝑡)as:\\n'\n",
      "                                               'ℎ𝑡 = M∗\\n'\n",
      "                                               '𝑡−1 (q𝑡), (21)\\n'\n",
      "                                               'where q𝑡 = S(𝑡)𝑊𝑄. Next, we '\n",
      "                                               'use this historical '\n",
      "                                               'information along with our '\n",
      "                                               'persistent memory parameters '\n",
      "                                               'as the input\\n'\n",
      "                                               'sequence to the attention '\n",
      "                                               'module:\\n'\n",
      "                                               '˜S\\n'\n",
      "                                               '(𝑡)\\n'\n",
      "                                               '=\\n'\n",
      "                                               '\\x02𝑝1 𝑝2 ... 𝑝 𝑁𝑝\\n'\n",
      "                                               '\\x03\\n'\n",
      "                                               '|| ℎ𝑡 || S(𝑡), (22)\\n'\n",
      "                                               '𝑦𝑡 = Attn\\n'\n",
      "                                               '\\x10\\n'\n",
      "                                               '˜S\\n'\n",
      "                                               '(𝑡)\\x11\\n'\n",
      "                                               '. (23)\\n'\n",
      "                                               'The structure of the attention '\n",
      "                                               'map over the entire sequence '\n",
      "                                               'is shown in Figure 3a. We then '\n",
      "                                               'use 𝑦𝑡 to update the '\n",
      "                                               'long-term\\n'\n",
      "                                               'memory module for the next '\n",
      "                                               'segment and the final output:\\n'\n",
      "                                               'M𝑡 = M𝑡−1 (𝑦𝑡), (24)\\n'\n",
      "                                               '𝑜𝑡 = 𝑦𝑡 ⊗M∗\\n'\n",
      "                                               '𝑡 (𝑦𝑡). (25)\\n'\n",
      "                                               'Note that, in the above, we '\n",
      "                                               'are updating the weight of '\n",
      "                                               'M𝑡−1 through forward pass.\\n'\n",
      "                                               'This architecture has two key '\n",
      "                                               'advantages: (1) Attention by '\n",
      "                                               'having both historical and '\n",
      "                                               'current context, has the '\n",
      "                                               'ability to\\n'\n",
      "                                               'decides whether given the '\n",
      "                                               'current data, the long-term '\n",
      "                                               'memory information is needed. '\n",
      "                                               '(2) The attention module '\n",
      "                                               'helps\\n'\n",
      "                                               '9\\n'\n",
      "                                               'Figure 4: Memory as a Gate '\n",
      "                                               '(MAG) Architecture. This '\n",
      "                                               'architecture, similarly, has '\n",
      "                                               'the three branches of (1) '\n",
      "                                               'core, (2)\\n'\n",
      "                                               'contextual memory, and (3) '\n",
      "                                               'persistent memory. It, '\n",
      "                                               'however, incorporates only '\n",
      "                                               'persistent memory into the '\n",
      "                                               'context and\\n'\n",
      "                                               'combine memory with the core '\n",
      "                                               'branch using a gating '\n",
      "                                               'mechanism. At test time, the '\n",
      "                                               'behavior is the same as Figure '\n",
      "                                               '2.\\n'\n",
      "                                               'the long-term memory to store '\n",
      "                                               'only useful information from '\n",
      "                                               'the current context. That is, '\n",
      "                                               'not all tokens in each '\n",
      "                                               'segment\\n'\n",
      "                                               'are useful and memorizing all '\n",
      "                                               'of them can result in memory '\n",
      "                                               'overflow. Therefore, attention '\n",
      "                                               'is helping the memory to\\n'\n",
      "                                               'understand which information '\n",
      "                                               'is useful, better managing the '\n",
      "                                               'memory capacity. (3) At test '\n",
      "                                               'time: (i) persistent memory\\n'\n",
      "                                               'parameters are fixed as they '\n",
      "                                               'encodes the knowledge about '\n",
      "                                               'the task, which should not be '\n",
      "                                               'changed; (ii) the attention '\n",
      "                                               'module\\n'\n",
      "                                               'weights are in-context '\n",
      "                                               'learner; and (iii) the '\n",
      "                                               'long-term memory module is '\n",
      "                                               'still learning (memorizing) '\n",
      "                                               'the information at test\\n'\n",
      "                                               'time. That is, we update the '\n",
      "                                               'weights of the neural memory '\n",
      "                                               'even at test time as weights '\n",
      "                                               'are encoding the abstraction '\n",
      "                                               'of\\n'\n",
      "                                               'long past.\\n'\n",
      "                                               '4.2 Gated Memory\\n'\n",
      "                                               'In the next variant (see '\n",
      "                                               'Figure 4), in one branch, we '\n",
      "                                               'directly use the input data to '\n",
      "                                               'update the long-term memory, '\n",
      "                                               'and in the\\n'\n",
      "                                               'second branch, we use a '\n",
      "                                               'sliding window attention '\n",
      "                                               '(SWA):\\n'\n",
      "                                               '˜𝑥 =\\n'\n",
      "                                               '\\x02𝑝1 𝑝2 ... 𝑝 𝑁𝑝\\n'\n",
      "                                               '\\x03\\n'\n",
      "                                               '|| 𝑥, (26)\\n'\n",
      "                                               '𝑦 = SW-Attn∗(˜𝑥), (27)\\n'\n",
      "                                               '𝑜 = 𝑦⊗M( ˜𝑥), (28)\\n'\n",
      "                                               'where SW-Attn∗is sliding '\n",
      "                                               'window attention with prefix '\n",
      "                                               '(see Figure 3b). Note that, '\n",
      "                                               'contrary to the previous '\n",
      "                                               'design, we are\\n'\n",
      "                                               'not segmenting the input data. '\n",
      "                                               'Also, we abuse the notation '\n",
      "                                               'and use M(𝑥)to refer to the '\n",
      "                                               'final output of the memory '\n",
      "                                               'after\\n'\n",
      "                                               'all recursion over the tokens '\n",
      "                                               'of the sequence. In the above '\n",
      "                                               'equation, ⊗can be any '\n",
      "                                               'non-linear gating. In our '\n",
      "                                               'experiments,\\n'\n",
      "                                               'we normalize the outputs 𝑦and '\n",
      "                                               'M(˜𝑥)using learnable '\n",
      "                                               'vector-valued weights, '\n",
      "                                               'followed by a non-linearity '\n",
      "                                               '𝜎(.).\\n'\n",
      "                                               'The overall attention mask of '\n",
      "                                               'this design is shown in Figure '\n",
      "                                               '3b. In this design, sliding '\n",
      "                                               'window attention is act as a '\n",
      "                                               'precise\\n'\n",
      "                                               'short-term memory, while the '\n",
      "                                               'neural memory module is acting '\n",
      "                                               'as a fading memory for the '\n",
      "                                               'model. This architecture '\n",
      "                                               'design\\n'\n",
      "                                               'can also be seen as a '\n",
      "                                               'multi-head architecture where '\n",
      "                                               'the structure of heads are '\n",
      "                                               'different (X. Dong et al. '\n",
      "                                               '2024).\\n'\n",
      "                                               '4.3 Memory as a Layer\\n'\n",
      "                                               'The last variant uses the '\n",
      "                                               'neural Memory As a Layer (MAL) '\n",
      "                                               'of a deep neural network (see '\n",
      "                                               'Figure 5). This architecture\\n'\n",
      "                                               'design is more common in the '\n",
      "                                               'literature, where the hybrid '\n",
      "                                               'models stack recurrent models '\n",
      "                                               'with full or sliding window\\n'\n",
      "                                               'attentions. Given input 𝑥, we '\n",
      "                                               'have:\\n'\n",
      "                                               '˜𝑥 =\\n'\n",
      "                                               '\\x02𝑝1 𝑝2 ... 𝑝 𝑁𝑝\\n'\n",
      "                                               '\\x03\\n'\n",
      "                                               '|| 𝑥, (29)\\n'\n",
      "                                               '𝑦 = M(˜𝑥), (30)\\n'\n",
      "                                               '𝑜 = SW-Attn (𝑦), (31)\\n'\n",
      "                                               '10\\n'\n",
      "                                               'Figure 5: Memory as a Layer '\n",
      "                                               '(MAL) Architecture. In this '\n",
      "                                               'architecture, the memory layer '\n",
      "                                               'is responsible to compress '\n",
      "                                               'the\\n'\n",
      "                                               'past and current context '\n",
      "                                               'before the attention module.\\n'\n",
      "                                               'where SW-Attn is sliding '\n",
      "                                               'window attention. The main '\n",
      "                                               'drawback of this design is '\n",
      "                                               'that the power of the model is '\n",
      "                                               'limited by\\n'\n",
      "                                               'each of the layers and so it '\n",
      "                                               'cannot take advantage of the '\n",
      "                                               'complementary data processing '\n",
      "                                               'of attention and neural '\n",
      "                                               'memory\\n'\n",
      "                                               'module. In our experiments, '\n",
      "                                               'for evaluating memory in this '\n",
      "                                               'design, we use a similar '\n",
      "                                               'architecture as H3 (D. Y. Fu '\n",
      "                                               'et al. 2023),\\n'\n",
      "                                               'where we replace the the '\n",
      "                                               'sequence model with our neural '\n",
      "                                               'memory module (LMM).\\n'\n",
      "                                               'Memory Without Attention. '\n",
      "                                               'Although in the above, we '\n",
      "                                               'discussed MAL as the '\n",
      "                                               'combination of LMMs and '\n",
      "                                               'attention in\\n'\n",
      "                                               'a sequential manner, one '\n",
      "                                               'simple variant of MAL is to '\n",
      "                                               'treat LMM as a sequence model '\n",
      "                                               'without any attention. From '\n",
      "                                               'the\\n'\n",
      "                                               'memory perspective, as '\n",
      "                                               'discussed in Section 1, we '\n",
      "                                               'expect each part of the memory '\n",
      "                                               'system to work independently, '\n",
      "                                               'even if\\n'\n",
      "                                               'other components are '\n",
      "                                               'disturbed. Therefore, a '\n",
      "                                               'long-term memory module should '\n",
      "                                               'still be a powerful model even '\n",
      "                                               'without\\n'\n",
      "                                               'short-term memory (i.e., '\n",
      "                                               'attention). We refer to this '\n",
      "                                               'variant as LMM or Titans (LMM) '\n",
      "                                               'in our experiments. We '\n",
      "                                               'provide\\n'\n",
      "                                               'additional discussions on the '\n",
      "                                               'connection of Titans and other '\n",
      "                                               'modern recurrent models in '\n",
      "                                               'Appendix C.\\n'\n",
      "                                               '4.4 Architectural Details\\n'\n",
      "                                               'For the sake of simplicity and '\n",
      "                                               'presentation, we avoid '\n",
      "                                               'discussing the implementation '\n",
      "                                               'details like using residual '\n",
      "                                               'connection,\\n'\n",
      "                                               'gating with linear layer, and '\n",
      "                                               'normalization. In all blocks, '\n",
      "                                               'we use residual connections. '\n",
      "                                               'In our implementation, we use\\n'\n",
      "                                               'SiLU(.) activation (Elfwing, '\n",
      "                                               'Uchibe, and Doya 2018) as the '\n",
      "                                               'non-linear activation for '\n",
      "                                               'computing query, key, and '\n",
      "                                               'values and\\n'\n",
      "                                               'normalize queries and keys '\n",
      "                                               'using ℓ2-norm.\\n'\n",
      "                                               'Convolution. Following the '\n",
      "                                               'recent modern linear recurrent '\n",
      "                                               'models (Gu and Dao 2024; S. '\n",
      "                                               'Yang, Kautz, and Hatamizadeh\\n'\n",
      "                                               '2024), we incorporate a 1D '\n",
      "                                               'depthwise-separable '\n",
      "                                               'convolution layer after each '\n",
      "                                               'of the query, key, and value '\n",
      "                                               'projections.\\n'\n",
      "                                               'While not significantly affect '\n",
      "                                               'the performance, these 1D '\n",
      "                                               'convolutions have shown '\n",
      "                                               'performance improvement and '\n",
      "                                               'are also\\n'\n",
      "                                               'computationally efficient.\\n'\n",
      "                                               'Gating. We also follow the '\n",
      "                                               'recent architectures that use '\n",
      "                                               'normalization and gating with '\n",
      "                                               'a linear layer before the '\n",
      "                                               'final\\n'\n",
      "                                               'output projection (Mehta et '\n",
      "                                               'al. 2023).\\n'\n",
      "                                               'Theorem 4.1. Contrary to '\n",
      "                                               'Transformers, diagonal linear '\n",
      "                                               'recurrent models, and '\n",
      "                                               'DeltaNet, all of which are '\n",
      "                                               'limited to TC0 (Merrill,\\n'\n",
      "                                               'Petty, and Sabharwal 2024), '\n",
      "                                               'Titans are capable of solving '\n",
      "                                               'problems beyond TC 0, meaning '\n",
      "                                               'that Titans are theoretically '\n",
      "                                               'more\\n'\n",
      "                                               'expressive than Transformers '\n",
      "                                               'and most modern linear '\n",
      "                                               'recurrent models in state '\n",
      "                                               'tracking tasks.\\n'\n",
      "                                               '5 Experiments\\n'\n",
      "                                               'N\\n'\n",
      "                                               'ext, we evaluate the '\n",
      "                                               'performance of Titans and its '\n",
      "                                               'variants in language modeling, '\n",
      "                                               'commonsense reasoning, needle\\n'\n",
      "                                               'in haystack, DNA modeling, and '\n",
      "                                               'time series forecasting '\n",
      "                                               'tasks1. In more details, in '\n",
      "                                               'this section, we answer the\\n'\n",
      "                                               'following empirical questions: '\n",
      "                                               '(1) How do Titans perform '\n",
      "                                               'compared to baselines in '\n",
      "                                               'downstream tasks? (see §5.2,\\n'\n",
      "                                               '1In the first version of the '\n",
      "                                               'work, we aim to provide '\n",
      "                                               'insights/evidences about why '\n",
      "                                               'the learning paradigms of '\n",
      "                                               'Titans are effective. We are '\n",
      "                                               'working on\\n'\n",
      "                                               'finalizing the results of '\n",
      "                                               'larger models and will report '\n",
      "                                               'them in the next version.\\n'\n",
      "                                               '11\\n'\n",
      "                                               '§5.6, and §5.7); (2) What is '\n",
      "                                               'the actual context length of '\n",
      "                                               'Titans? (see §5.3 and §5.4); '\n",
      "                                               '(3) How do Titans scale with '\n",
      "                                               'respect to\\n'\n",
      "                                               'context length? (see §5.8); '\n",
      "                                               '(4) How the depth of memory '\n",
      "                                               'can affect both performance '\n",
      "                                               'and efficiency? (see §5.5); '\n",
      "                                               'and (5)\\n'\n",
      "                                               'What is the contribution of '\n",
      "                                               'each Titans’ component in its '\n",
      "                                               'performance? (see §5.9).\\n'\n",
      "                                               '5.1 Experimental Setup\\n'\n",
      "                                               'Models. In our experiments, we '\n",
      "                                               'focus on the three variants of '\n",
      "                                               'Titans, which we refer to as: '\n",
      "                                               'Titans with (1) Memory as a\\n'\n",
      "                                               'Context (MAC), (2) Memory as a '\n",
      "                                               'Gate (MAG), and (3) Memory as '\n",
      "                                               'a Layer (MAL) as well as (4) '\n",
      "                                               'neural memory module\\n'\n",
      "                                               'alone. The reason behind using '\n",
      "                                               'our long-term memory as a '\n",
      "                                               'separate module is based on '\n",
      "                                               'our definition of learning. '\n",
      "                                               'As\\n'\n",
      "                                               'discussed in Section 1, we '\n",
      "                                               'define learning a process for '\n",
      "                                               'acquiring effective and useful '\n",
      "                                               'memory. Accordingly, we expect '\n",
      "                                               'our\\n'\n",
      "                                               'long-term memory to '\n",
      "                                               'effectively learn from data, '\n",
      "                                               'even without attention. For '\n",
      "                                               'each of these models, we '\n",
      "                                               'consider four scales\\n'\n",
      "                                               'with: (i) 170M, (ii) 340M, '\n",
      "                                               '(iii) 400M, and (iv) 760M '\n",
      "                                               'parameters. While the first '\n",
      "                                               'three are trained on 15B '\n",
      "                                               'tokens sampled\\n'\n",
      "                                               'from FineWeb-Edu dataset '\n",
      "                                               '(Penedo et al. 2024), the last '\n",
      "                                               'one is trained on 30B tokens '\n",
      "                                               'from the same dataset.\\n'\n",
      "                                               'Baselines. We compare our '\n",
      "                                               'models with the '\n",
      "                                               'state-of-the-art linear '\n",
      "                                               'recurrent models, '\n",
      "                                               'Transformers, and hybrid '\n",
      "                                               'models\\n'\n",
      "                                               '(recurrent + attention). More '\n",
      "                                               'specifically in language '\n",
      "                                               'tasks, we compare with '\n",
      "                                               'Transformer++ (Touvron et al. '\n",
      "                                               '2023),\\n'\n",
      "                                               'RetNet (Yutao Sun et al. '\n",
      "                                               '2023), Gated Linear Attention '\n",
      "                                               '(GLA) (S. Yang, B. Wang, Shen, '\n",
      "                                               'et al. 2024), Mamba (Gu and '\n",
      "                                               'Dao\\n'\n",
      "                                               '2024), Mamba2 (Dao and Gu '\n",
      "                                               '2024), DeltaNet (S. Yang, B. '\n",
      "                                               'Wang, Yu Zhang, et al. 2024), '\n",
      "                                               'TTT (Yu Sun et al. 2024), and '\n",
      "                                               'Gated\\n'\n",
      "                                               'DeltaNet (S. Yang, Kautz, and '\n",
      "                                               'Hatamizadeh 2024). In needle '\n",
      "                                               'in haystack tasks, we also '\n",
      "                                               'compare with GPT4 (Achiam et '\n",
      "                                               'al.\\n'\n",
      "                                               '2023), Llama3 with RAG '\n",
      "                                               '(Touvron et al. 2023), '\n",
      "                                               'RecurrentGemma2-9B (Botev et '\n",
      "                                               'al. 2024), and Mistral (Jiang '\n",
      "                                               'et al. 2023)\\n'\n",
      "                                               'models, all of which are '\n",
      "                                               'provided in the benchmark '\n",
      "                                               '(Yuri Kuratov et al. 2024). In '\n",
      "                                               'time series tasks, we compare '\n",
      "                                               'with\\n'\n",
      "                                               'Mamba-based (Behrouz, '\n",
      "                                               'Santacatterina, and Zabih '\n",
      "                                               '2024), Transformer-based (Y. '\n",
      "                                               'Liu et al. 2023; Nie et al. '\n",
      "                                               '2022; Yunhao\\n'\n",
      "                                               'Zhang and Yan 2023), and '\n",
      "                                               'linear models (Das et al. '\n",
      "                                               '2023; Z. Li et al. 2023; H. Wu '\n",
      "                                               'et al. 2023; Zeng et al. '\n",
      "                                               '2023).\\n'\n",
      "                                               'Training. In the training, we '\n",
      "                                               'follow the training procedure '\n",
      "                                               'of S. Yang, Kautz, and '\n",
      "                                               'Hatamizadeh (2024), and use '\n",
      "                                               'LLama 2\\n'\n",
      "                                               'tokenizer with a vocabulary '\n",
      "                                               'size of 32K and use training '\n",
      "                                               'length of 4K tokens. We employ '\n",
      "                                               'AdamW optimizer with learning\\n'\n",
      "                                               'rate of 4𝑒-4 with cosine '\n",
      "                                               'annealing schedule with batch '\n",
      "                                               'size of 0.5M tokens, and '\n",
      "                                               'weight decay of 0.1.\\n'\n",
      "                                               '5.2 Language Modeling\\n'\n",
      "                                               'We first focus on the '\n",
      "                                               'perplexity in language '\n",
      "                                               'modeling and also commonsense '\n",
      "                                               'reasoning tasks. The results '\n",
      "                                               'for Titans’\\n'\n",
      "                                               'variants and also baselines '\n",
      "                                               'with three different sizes of '\n",
      "                                               '340M, 400M, and 760M are '\n",
      "                                               'reported in Table 1. Among '\n",
      "                                               'non-hybrid\\n'\n",
      "                                               'models, including '\n",
      "                                               'Transformer++, our neural '\n",
      "                                               'memory module achieves the '\n",
      "                                               'best performance in both '\n",
      "                                               'perplexity and\\n'\n",
      "                                               'accuracy measures. Comparing '\n",
      "                                               'our neural memory module and '\n",
      "                                               'TTT, which is also a '\n",
      "                                               'gradient-based recurrent model '\n",
      "                                               'can\\n'\n",
      "                                               'show us the importance of our '\n",
      "                                               'weight decay as well as the '\n",
      "                                               'momentum. As discussed '\n",
      "                                               'earlier, the weight decay can '\n",
      "                                               'be\\n'\n",
      "                                               'interpreted as a gating '\n",
      "                                               'mechanism to forget the past '\n",
      "                                               'data, when it is needed. Also, '\n",
      "                                               'momentum can help us better '\n",
      "                                               'manage\\n'\n",
      "                                               'the memory by providing '\n",
      "                                               'additional memory for the '\n",
      "                                               'surprise metric. While some '\n",
      "                                               'baselines also take advantage '\n",
      "                                               'of gating\\n'\n",
      "                                               'mechanism, e.g., Mamba, '\n",
      "                                               'Mamba2, and Gated DeltaNet, '\n",
      "                                               'the superior performance of '\n",
      "                                               'our neural memory module '\n",
      "                                               'shows\\n'\n",
      "                                               'the importance of both our '\n",
      "                                               'surprise mechanism and having '\n",
      "                                               'deep and non-linear memory. We '\n",
      "                                               'further discuss the later in\\n'\n",
      "                                               'Section 5.5.\\n'\n",
      "                                               'Comparing the hybrid models, '\n",
      "                                               'we found that all three '\n",
      "                                               'variants of Titans (MAC, MAG, '\n",
      "                                               'and MAL) outperform both '\n",
      "                                               'Samba\\n'\n",
      "                                               '(Mamba + attention) and Gated '\n",
      "                                               'DeltaNet-H2 (Gated DeltaNet + '\n",
      "                                               'atttention). We attribute the '\n",
      "                                               'superior performance of '\n",
      "                                               'Titans\\n'\n",
      "                                               '(MAL) to the power of neural '\n",
      "                                               'memory module as the '\n",
      "                                               'architecture design and used '\n",
      "                                               'attention are all the same. '\n",
      "                                               'Comparing\\n'\n",
      "                                               'Titans (MAG) and (MAC), we '\n",
      "                                               'find that while their '\n",
      "                                               'performance are close, MAC '\n",
      "                                               'performs better when dealing '\n",
      "                                               'with longer\\n'\n",
      "                                               'dependencies in the data. '\n",
      "                                               'Interestingly, both MAG and '\n",
      "                                               'MAC outperform MAL variant, '\n",
      "                                               'which due to using the same\\n'\n",
      "                                               'modules, we attribute this to '\n",
      "                                               'the architecture design of '\n",
      "                                               'these models. This finding is '\n",
      "                                               'particularly important as the '\n",
      "                                               'current\\n'\n",
      "                                               'hybrid models (except Hymba '\n",
      "                                               '(X. Dong et al. 2024)) in the '\n",
      "                                               'literature are using MAL-style '\n",
      "                                               'combination of recurrent '\n",
      "                                               'models\\n'\n",
      "                                               'and attention.\\n'\n",
      "                                               '5.3 Needle in a Haystack\\n'\n",
      "                                               'Scaling a model to longer '\n",
      "                                               'context window is not always '\n",
      "                                               'equivalent to being effective '\n",
      "                                               'for very long sequences '\n",
      "                                               '(Hsieh\\n'\n",
      "                                               'et al. 2024). The '\n",
      "                                               'needle-in-a-haystack (NIAH) '\n",
      "                                               'task is designed to measure '\n",
      "                                               'the actual effective context '\n",
      "                                               'length of models.\\n'\n",
      "                                               'In this task, we evaluate the '\n",
      "                                               'model on retrieving a piece of '\n",
      "                                               'information (i.e., the '\n",
      "                                               '“needle”) from long distractor '\n",
      "                                               'texts (i.e.,\\n'\n",
      "                                               '12\\n'\n",
      "                                               'Table 1: Performance of Titans '\n",
      "                                               'and recurrent- and '\n",
      "                                               'Transformer-based baselines on '\n",
      "                                               'language modeling and '\n",
      "                                               'common-sense\\n'\n",
      "                                               'reasoning tasks. Hybrid models '\n",
      "                                               'are marked with ∗. The best '\n",
      "                                               'results among simple and '\n",
      "                                               'hybrid models are '\n",
      "                                               'highlighted.\\n'\n",
      "                                               'Model Wiki. LMB. LMB. PIQA '\n",
      "                                               'Hella. Wino. ARC-e ARC-c SIQA '\n",
      "                                               'BoolQ Avg.\\n'\n",
      "                                               'ppl↓ ppl↓ acc↑ acc↑ acc_n↑ '\n",
      "                                               'acc↑ acc↑ acc_n↑ acc↑ acc↑ ↑\\n'\n",
      "                                               '340M params / 15B tokens\\n'\n",
      "                                               'Transformer++ 31.52 41.08 '\n",
      "                                               '30.76 62.98 34.76 50.53 45.21 '\n",
      "                                               '24.05 36.81 58.24 42.92\\n'\n",
      "                                               'RetNet 32.50 49.73 28.24 62.61 '\n",
      "                                               '34.15 50.91 44.27 23.62 36.79 '\n",
      "                                               '59.72 42.54\\n'\n",
      "                                               'GLA 28.51 43.02 28.73 64.05 '\n",
      "                                               '35.96 50.00 54.19 24.29 37.13 '\n",
      "                                               '58.39 44.09\\n'\n",
      "                                               'Mamba 30.83 40.21 29.94 63.79 '\n",
      "                                               '35.88 49.82 49.24 24.56 35.41 '\n",
      "                                               '60.07 43.59\\n'\n",
      "                                               'DeltaNet 28.65 47.30 28.43 '\n",
      "                                               '63.52 35.95 49.63 52.68 25.37 '\n",
      "                                               '37.96 58.79 44.04\\n'\n",
      "                                               'TTT 27.44 34.19 30.06 63.97 '\n",
      "                                               '35.71 50.08 53.01 26.11 37.32 '\n",
      "                                               '59.83 44.51\\n'\n",
      "                                               'Gated DeltaNet 27.01 30.94 '\n",
      "                                               '34.11 63.08 38.12 51.60 55.28 '\n",
      "                                               '26.77 34.89 59.54 45.42\\n'\n",
      "                                               'Titans (LMM) 26.18 29.97 34.98 '\n",
      "                                               '64.73 39.61 51.85 55.60 28.14 '\n",
      "                                               '34.52 59.99 46.17\\n'\n",
      "                                               'Titans (MAC)∗ 25.43 28.13 '\n",
      "                                               '36.00 65.32 40.35 51.21 58.17 '\n",
      "                                               '29.00 38.63 60.18 47.36\\n'\n",
      "                                               'Titans (MAG)∗ 25.07 28.72 '\n",
      "                                               '36.71 64.88 40.56 52.49 57.72 '\n",
      "                                               '28.16 39.75 60.01 47.54\\n'\n",
      "                                               'Titans (MAL)∗ 24.69 28.80 '\n",
      "                                               '35.74 64.97 39.44 51.97 56.58 '\n",
      "                                               '28.21 38.14 57.32 46.55\\n'\n",
      "                                               '400M params / 15B tokens\\n'\n",
      "                                               'Transformer++ 30.63 37.37 '\n",
      "                                               '29.64 64.27 37.72 51.53 54.95 '\n",
      "                                               '27.36 38.07 61.59 45.64\\n'\n",
      "                                               'RetNet 29.92 46.83 29.16 65.23 '\n",
      "                                               '36.97 51.85 56.01 27.55 37.30 '\n",
      "                                               '59.66 45.47\\n'\n",
      "                                               'HGRN2 32.33 47.14 26.12 64.52 '\n",
      "                                               '35.45 52.24 55.97 25.51 37.35 '\n",
      "                                               '59.02 44.52\\n'\n",
      "                                               'GLA 27.96 36.66 27.86 65.94 '\n",
      "                                               '37.41 49.56 56.01 26.36 38.94 '\n",
      "                                               '59.84 45.24\\n'\n",
      "                                               'Mamba 29.22 39.88 29.82 65.72 '\n",
      "                                               '37.93 50.11 58.37 26.70 37.76 '\n",
      "                                               '61.13 45.94\\n'\n",
      "                                               'Mamba2 26.34 33.19 32.03 65.77 '\n",
      "                                               '39.73 52.48 59.00 27.64 37.92 '\n",
      "                                               '60.72 46.91\\n'\n",
      "                                               'DeltaNet 27.69 44.04 29.96 '\n",
      "                                               '64.52 37.03 50.82 56.77 27.13 '\n",
      "                                               '38.22 60.09 45.57\\n'\n",
      "                                               'TTT 26.11 31.52 33.25 65.70 '\n",
      "                                               '39.11 51.68 58.04 28.99 38.26 '\n",
      "                                               '59.87 46.86\\n'\n",
      "                                               'Gated DeltaNet 25.47 29.24 '\n",
      "                                               '34.40 65.94 40.46 51.46 59.80 '\n",
      "                                               '28.58 37.43 60.03 47.26\\n'\n",
      "                                               'Samba∗ 25.32 29.47 36.86 66.09 '\n",
      "                                               '39.24 51.45 60.12 27.20 38.68 '\n",
      "                                               '58.22 47.23\\n'\n",
      "                                               'Gated DeltaNet-H2∗ 24.19 28.09 '\n",
      "                                               '36.77 66.43 40.79 52.17 59.55 '\n",
      "                                               '29.09 39.04 58.56 47.69\\n'\n",
      "                                               'Titans (LMM) 25.03 28.99 35.21 '\n",
      "                                               '65.85 40.91 52.19 59.97 29.20 '\n",
      "                                               '38.74 60.85 47.83\\n'\n",
      "                                               'Titans (MAC)∗ 25.61 27.73 '\n",
      "                                               '36.92 66.39 41.18 52.80 60.24 '\n",
      "                                               '29.69 40.07 61.93 48.65\\n'\n",
      "                                               'Titans (MAG)∗ 23.59 27.81 '\n",
      "                                               '37.24 66.80 40.92 53.21 60.01 '\n",
      "                                               '29.45 39.91 61.28 48.60\\n'\n",
      "                                               'Titans (MAL)∗ 23.93 27.89 '\n",
      "                                               '36.84 66.29 40.74 52.26 59.85 '\n",
      "                                               '29.71 38.92 58.40 47.87\\n'\n",
      "                                               '760M params / 30B tokens\\n'\n",
      "                                               'Transformer++ 25.21 27.64 '\n",
      "                                               '35.78 66.92 42.19 51.95 60.38 '\n",
      "                                               '32.46 39.51 60.37 48.69\\n'\n",
      "                                               'RetNet 26.08 24.45 34.51 67.19 '\n",
      "                                               '41.63 52.09 63.17 32.78 38.36 '\n",
      "                                               '57.92 48.46\\n'\n",
      "                                               'Mamba 28.12 23.96 32.80 66.04 '\n",
      "                                               '39.15 52.38 61.49 30.34 37.96 '\n",
      "                                               '57.62 47.22\\n'\n",
      "                                               'Mamba2 22.94 28.37 33.54 67.90 '\n",
      "                                               '42.71 49.77 63.48 31.09 40.06 '\n",
      "                                               '58.15 48.34\\n'\n",
      "                                               'DeltaNet 24.37 24.60 37.06 '\n",
      "                                               '66.93 41.98 50.65 64.87 31.39 '\n",
      "                                               '39.88 59.02 48.97\\n'\n",
      "                                               'TTT 24.17 23.51 34.74 67.25 '\n",
      "                                               '43.92 50.99 64.53 33.81 40.16 '\n",
      "                                               '59.58 47.32\\n'\n",
      "                                               'Gated DeltaNet 21.18 22.09 '\n",
      "                                               '35.54 68.01 44.95 50.73 66.87 '\n",
      "                                               '33.09 39.21 59.14 49.69\\n'\n",
      "                                               'Samba∗ 20.63 22.71 39.72 69.19 '\n",
      "                                               '47.35 52.01 66.92 33.20 38.98 '\n",
      "                                               '61.24 51.08\\n'\n",
      "                                               'Gated DeltaNet-H2∗ 19.88 20.83 '\n",
      "                                               '39.18 68.95 48.22 52.57 67.01 '\n",
      "                                               '35.49 39.39 61.11 51.49\\n'\n",
      "                                               'Titans (LMM) 20.04 21.96 37.40 '\n",
      "                                               '69.28 48.46 52.27 66.31 35.84 '\n",
      "                                               '40.13 62.76 51.56\\n'\n",
      "                                               'Titans (MAC) 19.93 20.12 39.62 '\n",
      "                                               '70.46 49.01 53.18 67.86 36.01 '\n",
      "                                               '41.87 62.05 52.51\\n'\n",
      "                                               'Titans (MAG) 18.61 19.86 40.98 '\n",
      "                                               '70.25 48.94 52.89 68.23 36.19 '\n",
      "                                               '40.38 62.11 52.50\\n'\n",
      "                                               'Titans (MAL) 19.07 20.33 40.05 '\n",
      "                                               '69.99 48.82 53.02 67.54 35.65 '\n",
      "                                               '30.98 61.72 50.97\\n'\n",
      "                                               'the “haystack”). In this part, '\n",
      "                                               'we use Single NIAH (S-NIAH) '\n",
      "                                               'task from RULER benchmark '\n",
      "                                               '(Hsieh et al. 2024) and '\n",
      "                                               'evaluate\\n'\n",
      "                                               'Titans and baselines on '\n",
      "                                               'sequences with length 2K, 4K, '\n",
      "                                               '8K, and 16K. The results are '\n",
      "                                               'reported in Table 2. Neural '\n",
      "                                               'Memory\\n'\n",
      "                                               'module achieves the best '\n",
      "                                               'results compare to baselines '\n",
      "                                               'in all three tasks. We '\n",
      "                                               'attribute this superior '\n",
      "                                               'performance to three\\n'\n",
      "                                               'key differences of Titans with '\n",
      "                                               'existing sequence models: (1) '\n",
      "                                               'Compared to TTT, our Neural '\n",
      "                                               'Memory can better handle the\\n'\n",
      "                                               'memory capacity by using '\n",
      "                                               'momentum and also the '\n",
      "                                               'forgetting mechanism (i.e., '\n",
      "                                               'weight decay). Therefore, with '\n",
      "                                               'increasing\\n'\n",
      "                                               'the sequence length, the '\n",
      "                                               'performance of Neural Memory '\n",
      "                                               'does not drop and show a '\n",
      "                                               'consistent trend; (2) Compared '\n",
      "                                               'to\\n'\n",
      "                                               'Mamba2, which has the gating '\n",
      "                                               '(forgetting) mechanism, Titans '\n",
      "                                               'have deep non-linear memory, '\n",
      "                                               'resulting in better memory\\n'\n",
      "                                               'management. Also, contrary to '\n",
      "                                               'our neural memory and '\n",
      "                                               'DeltaNet, Mamba2 is not '\n",
      "                                               'capable of removing a memory '\n",
      "                                               'and so\\n'\n",
      "                                               '13\\n'\n",
      "                                               'Table 2: Performance of Titans '\n",
      "                                               'and baselines on S-NIAH task '\n",
      "                                               'from RULER benchmark. The best '\n",
      "                                               'results among simple\\n'\n",
      "                                               'and hybrid models are '\n",
      "                                               'highlighted.\\n'\n",
      "                                               'Model S-NIAH-PK S-NIAH-N '\n",
      "                                               'S-NIAH-W\\n'\n",
      "                                               '2K 4K 8K 16K 2K 4K 8K 16K 2K '\n",
      "                                               '4K 8K 16K\\n'\n",
      "                                               'TTT 98.4 98.8 98.0 88.4 60.2 '\n",
      "                                               '36.6 10.2 4.4 78.8 28.0 4.4 '\n",
      "                                               '0.0\\n'\n",
      "                                               'Mamba2 98.6 61.4 31.0 5.4 98.4 '\n",
      "                                               '55.8 14.2 0.0 42.2 4.2 0.0 '\n",
      "                                               '0.0\\n'\n",
      "                                               'DeltaNet 96.8 98.8 98.6 71.4 '\n",
      "                                               '47.2 15.4 12.8 5.4 46.2 20.0 '\n",
      "                                               '1.6 0.0\\n'\n",
      "                                               'Titans (LMM)99.8 98.4 98.2 '\n",
      "                                               '96.2 100.0 99.8 93.4 80.2 90.4 '\n",
      "                                               '89.4 85.8 80.6\\n'\n",
      "                                               'Titans (MAC) 99.2 98.8 99.0 '\n",
      "                                               '98.4 99.6 98.2 97.6 97.4 98.2 '\n",
      "                                               '98.2 95.6 95.2\\n'\n",
      "                                               'Titans (MAG)99.4 98.0 97.4 '\n",
      "                                               '97.4 99.2 98.8 97.2 98.6 98.0 '\n",
      "                                               '98.0 90.2 88.2\\n'\n",
      "                                               'Titans (MAL) 98.8 98.6 98.8 '\n",
      "                                               '97.8 99.8 98.1 96.8 96.4 98.0 '\n",
      "                                               '97.4 92.0 90.4\\n'\n",
      "                                               '(a) Few-shot Setup\\n'\n",
      "                                               ' (b) Fine-Tuning Setup\\n'\n",
      "                                               'Figure 6: Performance of '\n",
      "                                               'Titans and baselines on '\n",
      "                                               'BABILong benchmark. Titans '\n",
      "                                               '(MAC) outperforms all '\n",
      "                                               'baselines, including\\n'\n",
      "                                               'extremely large models, e.g., '\n",
      "                                               'GPT4.\\n'\n",
      "                                               'we can see a significant drop '\n",
      "                                               'in performance when increasing '\n",
      "                                               'the sequence length; (3) '\n",
      "                                               'Compared to DeltaNet, although '\n",
      "                                               'it\\n'\n",
      "                                               'is capable of removing memory '\n",
      "                                               'using delta rule, it cannot '\n",
      "                                               'erase the memory, lacking '\n",
      "                                               'forgetting mechanism. Finally, '\n",
      "                                               'As\\n'\n",
      "                                               'expected we can see on par or '\n",
      "                                               'better results when using '\n",
      "                                               'Titans variants, where the '\n",
      "                                               'best results correspond to '\n",
      "                                               'MAC.\\n'\n",
      "                                               '5.4 BABILong Benchmark\\n'\n",
      "                                               'In the previous section we '\n",
      "                                               'discussed the results on a '\n",
      "                                               'simple NIAH tasks where a '\n",
      "                                               'single needle needs to be '\n",
      "                                               'retrieved.\\n'\n",
      "                                               'Although Titans showed better '\n",
      "                                               'performance compared to '\n",
      "                                               'baselines, their true '\n",
      "                                               'advantage over very long '\n",
      "                                               'sequences is still\\n'\n",
      "                                               'hidden. To this end, in this '\n",
      "                                               'section, we use a harder task '\n",
      "                                               'from BABILong benchmark (Yuri '\n",
      "                                               'Kuratov et al. 2024), in '\n",
      "                                               'which\\n'\n",
      "                                               'the model needs to reason '\n",
      "                                               'across facts distributed in '\n",
      "                                               'extremely long documents. We '\n",
      "                                               'follow the original '\n",
      "                                               'experimental setup\\n'\n",
      "                                               'and training process in the '\n",
      "                                               'benchmark. There are two '\n",
      "                                               'settings: (1) Few-shot '\n",
      "                                               'setting, in which we use large '\n",
      "                                               'pre-trained\\n'\n",
      "                                               'models, and (2) fine-tuning '\n",
      "                                               'setting, where we fine-tune '\n",
      "                                               'the MAC variant of Titans to '\n",
      "                                               'compare it with other '\n",
      "                                               'fine-tuned\\n'\n",
      "                                               'baselines. The results for '\n",
      "                                               'few-shot setting are reported '\n",
      "                                               'in Figure 6a. In this setup, '\n",
      "                                               'we can see Titans outperform '\n",
      "                                               'all\\n'\n",
      "                                               'baselines–i.e., Mamba2.8B (Gu '\n",
      "                                               'and Dao 2024), RWKV-6-7B '\n",
      "                                               '(Peng, Goldstein, et al. '\n",
      "                                               '2024), RecurrentGemma-9B '\n",
      "                                               '(Botev et al.\\n'\n",
      "                                               '2024), Gemma-9B (Team et al. '\n",
      "                                               '2024), Llama3.1-8B (Touvron et '\n",
      "                                               'al. 2023), GPT-4, and '\n",
      "                                               'GPT4o-mini (Achiam et al. '\n",
      "                                               '2023). These\\n'\n",
      "                                               'results are achieved while '\n",
      "                                               'Titans (MAC) is having much '\n",
      "                                               'less number of parameters than '\n",
      "                                               'baselines.\\n'\n",
      "                                               'In the fine-tuning setup, we '\n",
      "                                               'compare the small fine-tuned '\n",
      "                                               'version of Titans (MAC) with: '\n",
      "                                               '(i) the fine-tuned version of '\n",
      "                                               'small\\n'\n",
      "                                               'models (almost the same number '\n",
      "                                               'of parameters as Titans) such '\n",
      "                                               'as Mamba (Gu and Dao 2024), '\n",
      "                                               'RMT (Bulatov, Yury Kuratov,\\n'\n",
      "                                               'and Burtsev 2022), (ii) large '\n",
      "                                               'models with '\n",
      "                                               'Retrieval-Augmented Generation '\n",
      "                                               '(RAG) (P. Lewis et al. 2020) '\n",
      "                                               'such as Llama3.1-\\n'\n",
      "                                               '8B (Touvron et al. 2023), and '\n",
      "                                               '(iii) extremely large models '\n",
      "                                               'such as GPT-4 (Achiam et al. '\n",
      "                                               '2023), GPT4o-mini, Qwen2.5-72B '\n",
      "                                               '(A.\\n'\n",
      "                                               'Yang et al. 2024), and '\n",
      "                                               'Llama3.1-70B (Touvron et al. '\n",
      "                                               '2023). Baseline results are '\n",
      "                                               'reported by (Yuri Kuratov et '\n",
      "                                               'al. 2024). The\\n'\n",
      "                                               'results of Titans and '\n",
      "                                               'baselines are reported in '\n",
      "                                               'Figure 6b. Titans outperform '\n",
      "                                               'all models even extremely '\n",
      "                                               'large models like\\n'\n",
      "                                               'GPT4. Also, compared to '\n",
      "                                               'Transformer-based with memory '\n",
      "                                               'models like RMT, Titans show '\n",
      "                                               'better performance mainly due\\n'\n",
      "                                               'to their powerful memory. That '\n",
      "                                               'is, RMT compress the '\n",
      "                                               'historical data into 16 size '\n",
      "                                               'vector-valued memory, while '\n",
      "                                               'Titans with\\n'\n",
      "                                               'in-context online memory '\n",
      "                                               'learner are capable of '\n",
      "                                               'encoding the past into the '\n",
      "                                               'parameters of the model. '\n",
      "                                               'Interestingly, even\\n'\n",
      "                                               '14\\n'\n",
      "                                               '(a) 170M Parameters\\n'\n",
      "                                               ' (b) 360M Parameters\\n'\n",
      "                                               ' (c) 760M Parameters\\n'\n",
      "                                               'Figure 7: The effect of memory '\n",
      "                                               'depth on the perplexity. '\n",
      "                                               'Deeper long-term memory '\n",
      "                                               'results in better scaling in '\n",
      "                                               'longer\\n'\n",
      "                                               'sequences.\\n'\n",
      "                                               'Table 3: Performance on '\n",
      "                                               'long-term forecasting. The '\n",
      "                                               'best results are highlighted '\n",
      "                                               '.\\n'\n",
      "                                               'Neural MemorySimba '\n",
      "                                               'iTransformerRLinear '\n",
      "                                               'PatchTSTCrossformerTiDE '\n",
      "                                               'TimesNet DLinear\\n'\n",
      "                                               'MSE MAE MSE MAE MSE MAE MSE '\n",
      "                                               'MAE MSE MAE MSE MAE MSE MAE '\n",
      "                                               'MSE MAE MSE MAE\\n'\n",
      "                                               'ETTm1 0.358 0.387 '\n",
      "                                               '0.3830.3960.407 0.410 '\n",
      "                                               '0.4140.4070.3870.4000.5130.496 '\n",
      "                                               '0.4190.4190.4000.4060.4030.407\\n'\n",
      "                                               'ETTm2 0.261 0.309 '\n",
      "                                               '0.2710.3270.288 0.332 '\n",
      "                                               '0.2860.3270.2810.3260.7570.610 '\n",
      "                                               '0.3580.4040.2910.3330.3500.401\\n'\n",
      "                                               'ETTh1 0.420 0.421 '\n",
      "                                               '0.4410.4320.454 0.447 '\n",
      "                                               '0.4460.4340.4690.4540.5290.522 '\n",
      "                                               '0.5410.5070.4580.4500.4560.452\\n'\n",
      "                                               'ETTh2 0.336 0.382 '\n",
      "                                               '0.3610.3910.383 0.407 '\n",
      "                                               '0.3740.3980.3870.4070.9420.684 '\n",
      "                                               '0.6110.5500.4140.4270.5590.515\\n'\n",
      "                                               'ECL 0.162 0.261 '\n",
      "                                               '0.1690.2740.178 0.270 '\n",
      "                                               '0.2190.2980.2050.2900.2440.334 '\n",
      "                                               '0.2510.3440.1920.2950.2120.300\\n'\n",
      "                                               'Traffic 0.415 0.289 '\n",
      "                                               '0.4930.2910.428 0.282 '\n",
      "                                               '0.6260.3780.4810.3040.5500.304 '\n",
      "                                               '0.7600.4730.6200.3360.6250.383\\n'\n",
      "                                               'Weather0.231 0.265 '\n",
      "                                               '0.2550.2800.258 0.278 '\n",
      "                                               '0.2720.2910.2590.2810.2590.315 '\n",
      "                                               '0.2710.3200.2590.2870.2650.317\\n'\n",
      "                                               'augmenting Llama3.1-8B model '\n",
      "                                               'with RAG performs worse than '\n",
      "                                               'Titans with about ×70 less '\n",
      "                                               'parameters.\\n'\n",
      "                                               '5.5 The Effect of Deep Memory\\n'\n",
      "                                               'In this section, we evaluate '\n",
      "                                               'the effect of deep memory in '\n",
      "                                               'both wall-clock training time '\n",
      "                                               'and model performance2. To '\n",
      "                                               'this\\n'\n",
      "                                               'end, we focus on different '\n",
      "                                               'variants of our neural memory '\n",
      "                                               'module, where 𝐿M= 1,2,3,4. We '\n",
      "                                               'also use Mamba as a baseline\\n'\n",
      "                                               'for the model performance. For '\n",
      "                                               'a fair comparison, we use the '\n",
      "                                               'same training process for all '\n",
      "                                               'models and train them on a\\n'\n",
      "                                               'subset of the Pile dataset (L. '\n",
      "                                               'Gao et al. 2020).\\n'\n",
      "                                               'We report the perplexity of '\n",
      "                                               'our models and baselines as '\n",
      "                                               'the function of the sequence '\n",
      "                                               'length in Figure 7. '\n",
      "                                               'Interestingly, with\\n'\n",
      "                                               'the increase of memory depth, '\n",
      "                                               '𝐿M, the model can achieve '\n",
      "                                               'better perplexity over all '\n",
      "                                               'sequence length. Also, deeper '\n",
      "                                               'memory\\n'\n",
      "                                               'modules are more robust to the '\n",
      "                                               'sequence length when the model '\n",
      "                                               'has less number of parameters. '\n",
      "                                               'With the increase of the\\n'\n",
      "                                               'number of parameters, all '\n",
      "                                               'models show better performance '\n",
      "                                               'on longer sequences.\\n'\n",
      "                                               'Figure 8: The effect of memory '\n",
      "                                               'depth on\\n'\n",
      "                                               'training throughput\\n'\n",
      "                                               'We also evaluate the effect of '\n",
      "                                               'memory depth ( 𝐿M = 1,2,3,4) '\n",
      "                                               'on the training\\n'\n",
      "                                               'throughput. We report the '\n",
      "                                               'training throughput (the '\n",
      "                                               'number of tokens per\\n'\n",
      "                                               'second) as the function of '\n",
      "                                               'sequence length in Figure 8. '\n",
      "                                               'All models scale linearly\\n'\n",
      "                                               'with respect to the context '\n",
      "                                               'length (i.e., constant trend '\n",
      "                                               'in the number of tokens\\n'\n",
      "                                               'per second with respect to '\n",
      "                                               'sequence length). Also, by '\n",
      "                                               'increasing the memory\\n'\n",
      "                                               'depth, as expected, we can see '\n",
      "                                               'a linear trend that a deeper '\n",
      "                                               'memory results in\\n'\n",
      "                                               'a slower training. Therefore, '\n",
      "                                               'it is not always efficient to '\n",
      "                                               'use deeper memory\\n'\n",
      "                                               'modules, showing a trade-off '\n",
      "                                               'between effectiveness and '\n",
      "                                               'efficiency.\\n'\n",
      "                                               '5.6 Time Series Forecasting\\n'\n",
      "                                               'To show the effectiveness of '\n",
      "                                               'our memory module in a broader '\n",
      "                                               'tasks, we also evaluate its '\n",
      "                                               'performance in time series\\n'\n",
      "                                               'forecasting tasks. To this '\n",
      "                                               'end, we use Simba framework '\n",
      "                                               '(Patro and Agneeswaran 2024) '\n",
      "                                               'for time series forecasting, '\n",
      "                                               'and\\n'\n",
      "                                               '2Note that, in this '\n",
      "                                               'experiment, we only focus on '\n",
      "                                               'the neural memory module to '\n",
      "                                               'evaluate the effect of memory '\n",
      "                                               'depth in the memorization '\n",
      "                                               'process.\\n'\n",
      "                                               'Combining neural memory with '\n",
      "                                               'attention as we do in Titans '\n",
      "                                               'variants, can additionally '\n",
      "                                               'enhance the performance of the '\n",
      "                                               'model over long sequences.\\n'\n",
      "                                               '15\\n'\n",
      "                                               'Table 4: Downstream evaluation '\n",
      "                                               'of pre-trained DNA models on '\n",
      "                                               'GenomicsBenchmarks (Grešová et '\n",
      "                                               'al. 2023). We report\\n'\n",
      "                                               'top-1 classification accuracy '\n",
      "                                               '(%).\\n'\n",
      "                                               'Model Enhancer Cohn Enhancer '\n",
      "                                               'Ens Human Reg. Non-TATA '\n",
      "                                               'Promoters Human OCR Ens.\\n'\n",
      "                                               'CNN 69.5 68.9 93.3 84.6 68.0\\n'\n",
      "                                               'DNABERT 74.0 85.7 88.1 85.6 '\n",
      "                                               '75.1\\n'\n",
      "                                               'GPT 70.5 83.5 91.5 87.7 73.0\\n'\n",
      "                                               'HyenaDNA 74.2 89.2 93.8 96.6 '\n",
      "                                               '80.9\\n'\n",
      "                                               'Transformer++ 73.4 89.5 89.9 '\n",
      "                                               '94.4 79.5\\n'\n",
      "                                               'Mamba 73.0 - - 96.6 -\\n'\n",
      "                                               'Based 74.6 89.5 89.5 96.8 '\n",
      "                                               '79.0\\n'\n",
      "                                               'Neural Memory Module 75.2 89.6 '\n",
      "                                               '89.3 96.6 79.9\\n'\n",
      "                                               'replace its Mamba module with '\n",
      "                                               'our neural memory. We report '\n",
      "                                               'the results on common time '\n",
      "                                               'series forecasting benchmark\\n'\n",
      "                                               'datasets–ETT, ECL, Traffic, '\n",
      "                                               'and Weather (H. Zhou et al. '\n",
      "                                               '2021). The results are '\n",
      "                                               'reported in Table 3. Our '\n",
      "                                               'neural memory\\n'\n",
      "                                               'module is outperforming all '\n",
      "                                               'baselines, including '\n",
      "                                               'Mamba-based, linear-based, and '\n",
      "                                               'Transformer-based '\n",
      "                                               'architectures.\\n'\n",
      "                                               '5.7 DNA Modeling\\n'\n",
      "                                               'In order to understand the '\n",
      "                                               'capability of Titans beyond '\n",
      "                                               'natural language, we further '\n",
      "                                               'evaluate the performance of '\n",
      "                                               'our\\n'\n",
      "                                               'neural memory module on DNA '\n",
      "                                               'modeling tasks. To this end, '\n",
      "                                               'we evaluate pre-trained models '\n",
      "                                               'on the downstream tasks\\n'\n",
      "                                               'in GenomicsBenchmarks (Grešová '\n",
      "                                               'et al. 2023). We follow the '\n",
      "                                               'same experimental setups from '\n",
      "                                               'Nguyen et al. (2024), and\\n'\n",
      "                                               're-use the reported results of '\n",
      "                                               'baselines by Arora et al. '\n",
      "                                               '(2024). The performance of '\n",
      "                                               'Titans (LMM) and baselines are '\n",
      "                                               'reported\\n'\n",
      "                                               'in Table 4. We find that LMM '\n",
      "                                               'is competitive with '\n",
      "                                               'state-of-the-art architectures '\n",
      "                                               'across different downstream '\n",
      "                                               'genomics\\n'\n",
      "                                               'tasks.\\n'\n",
      "                                               '5.8 Efficiency\\n'\n",
      "                                               'Figure 9: Training throughput '\n",
      "                                               'compari-\\n'\n",
      "                                               'son of Titans and baselines.\\n'\n",
      "                                               'In this part, we compare the '\n",
      "                                               'efficiency of our neural '\n",
      "                                               'memory as well as Titans\\n'\n",
      "                                               'with state-of-the-art sequence '\n",
      "                                               'models. The training '\n",
      "                                               'throughput of models for\\n'\n",
      "                                               'different sequence length '\n",
      "                                               '×batch size are reported in '\n",
      "                                               'Figure 9. Comparing\\n'\n",
      "                                               'recurrent models, including '\n",
      "                                               'our neural memory module, we '\n",
      "                                               'can see our memory\\n'\n",
      "                                               'module is slightly slower than '\n",
      "                                               'Mamba2 and Gated DeltaNet, '\n",
      "                                               'mainly due to: (1)\\n'\n",
      "                                               'having deep memory and more '\n",
      "                                               'expressive transition process '\n",
      "                                               '(memory update),\\n'\n",
      "                                               'and (2) highly optimized '\n",
      "                                               'kernel in the implementation '\n",
      "                                               'of Mamba2. Interestingly,\\n'\n",
      "                                               'Titans (MAL) are faster than '\n",
      "                                               'baselines as well as the '\n",
      "                                               'memory module. The\\n'\n",
      "                                               'main reason for this better '\n",
      "                                               'throughput is the highly '\n",
      "                                               'optimized kernel of Flash-\\n'\n",
      "                                               'Attention (Dao 2024), which is '\n",
      "                                               'used for implementing SWA and '\n",
      "                                               'full attention\\n'\n",
      "                                               'module in Titans.\\n'\n",
      "                                               '5.9 Ablation Study\\n'\n",
      "                                               'Finally, we perform ablation '\n",
      "                                               'studies on the different '\n",
      "                                               'architectural choices in '\n",
      "                                               'Titans. We consider our neural '\n",
      "                                               'memory\\n'\n",
      "                                               'module as a base model and '\n",
      "                                               'then changing one component at '\n",
      "                                               'a time: (1) replacing deep '\n",
      "                                               'memory with linear memory,\\n'\n",
      "                                               'removing (2) convolution, (3) '\n",
      "                                               'momentum in the surprise '\n",
      "                                               'measure, (4) weight decay (or '\n",
      "                                               'forgot mechanism), and (5) '\n",
      "                                               'persistent\\n'\n",
      "                                               'memory. The results are '\n",
      "                                               'reported in Table 5. All '\n",
      "                                               'components of neural memory '\n",
      "                                               'design are positively '\n",
      "                                               'contributing to its\\n'\n",
      "                                               'performance, where the '\n",
      "                                               'greatest contribution comes '\n",
      "                                               'from weight decay, momentum, '\n",
      "                                               'convolution, and persistent '\n",
      "                                               'memory,\\n'\n",
      "                                               'respectively.\\n'\n",
      "                                               'The Effect of Architectural '\n",
      "                                               'Design. To evaluate the effect '\n",
      "                                               'of architecture design, we '\n",
      "                                               'compare the performance of '\n",
      "                                               'three\\n'\n",
      "                                               'represented variants of Titans '\n",
      "                                               'in three aspects of (i) '\n",
      "                                               'language modeling, (ii) '\n",
      "                                               'commen-sense reasoning, and '\n",
      "                                               '(iii) long context\\n'\n",
      "                                               'NIAH (BABILong) tasks. The '\n",
      "                                               'results are reported in Table '\n",
      "                                               '5. We find that MAC and MAG '\n",
      "                                               'have close performance in\\n'\n",
      "                                               'language modeling and '\n",
      "                                               'common-sense reasoning tasks, '\n",
      "                                               'while MAC achieve '\n",
      "                                               'significantly better '\n",
      "                                               'performance in long-context\\n'\n",
      "                                               'NIAH. Both of these models '\n",
      "                                               'achieve better performance '\n",
      "                                               'than MAL. These results along '\n",
      "                                               'with Figure 9, show a '\n",
      "                                               'trade-off\\n'\n",
      "                                               'between fast training and more '\n",
      "                                               'expressive design.\\n'\n",
      "                                               '16\\n'\n",
      "                                               'Table 5: Ablation Study on '\n",
      "                                               'Titans. All components of '\n",
      "                                               'Titans are positively '\n",
      "                                               'contributing to its '\n",
      "                                               'performance.\\n'\n",
      "                                               'Model Language Modeling '\n",
      "                                               'Reasoning Long Context\\n'\n",
      "                                               'ppl↓ acc↑ acc↑\\n'\n",
      "                                               'LMM 27.01 47.83 92.68\\n'\n",
      "                                               '+Attn(MAC) 26.67 48.65 97.95\\n'\n",
      "                                               '+Attn(MAG) 25.70 48.60 96.70\\n'\n",
      "                                               '+Attn(MAL) 25.91 47.87 96.91\\n'\n",
      "                                               'Linear Memory 28.49 46.97 '\n",
      "                                               '85.34\\n'\n",
      "                                               'w/o Convolution 28.73 45.82 '\n",
      "                                               '90.28\\n'\n",
      "                                               'w/o Momentum 28.98 45.49 '\n",
      "                                               '87.12\\n'\n",
      "                                               'w/o Weight Decay 29.04 45.11 '\n",
      "                                               '85.60\\n'\n",
      "                                               'w/o Persistent Memory 27.63 '\n",
      "                                               '46.35 92.49\\n'\n",
      "                                               '6 Conclusion\\n'\n",
      "                                               'In this paper, we present a '\n",
      "                                               'neural long-term memory that, '\n",
      "                                               'as a meta in-context learner, '\n",
      "                                               'learns to memorize at test '\n",
      "                                               'time.\\n'\n",
      "                                               'The neural memory module is a '\n",
      "                                               'recurrent model in nature, and '\n",
      "                                               'is adaptively memorizing '\n",
      "                                               'tokens that are more '\n",
      "                                               'surprising\\n'\n",
      "                                               'or are close to surprising '\n",
      "                                               'tokens. Comparing to modern '\n",
      "                                               'recurrent models, it has more '\n",
      "                                               'expressive memory update and\\n'\n",
      "                                               'storing mechanism. Using this '\n",
      "                                               'memory, we present Titans '\n",
      "                                               'architectures, and its three '\n",
      "                                               'variants, in which we suggest '\n",
      "                                               'to\\n'\n",
      "                                               'incorporate the memory module '\n",
      "                                               'as (1) a context, (2) gating, '\n",
      "                                               'and (3) a layer. Our '\n",
      "                                               'experimental evaluation on '\n",
      "                                               'diverse tasks\\n'\n",
      "                                               'tasks validate that Titans are '\n",
      "                                               'more effective than '\n",
      "                                               'Transformers and recent modern '\n",
      "                                               'linear recurrent models, '\n",
      "                                               'specifically for\\n'\n",
      "                                               'long context. That is, Titans '\n",
      "                                               'can scale to larger than 2M '\n",
      "                                               'context window size with '\n",
      "                                               'better accuracy than '\n",
      "                                               'baselines.\\n'\n",
      "                                               'Titans are implemented in '\n",
      "                                               'Pytorch and JAX and we intend '\n",
      "                                               'to make the code we used to '\n",
      "                                               'train and evaluate our models\\n'\n",
      "                                               'available soon.\\n'\n",
      "                                               '17\\n'\n",
      "                                               'References\\n'\n",
      "                                               '[1] Josh Achiam, Steven Adler, '\n",
      "                                               'Sandhini Agarwal, Lama Ahmad, '\n",
      "                                               'Ilge Akkaya, Florencia Leoni '\n",
      "                                               'Aleman, Diogo\\n'\n",
      "                                               'Almeida, Janko Altenschmidt, '\n",
      "                                               'Sam Altman, Shyamal Anadkat, '\n",
      "                                               'et al. “Gpt-4 technical '\n",
      "                                               'report”. In: arXiv preprint\\n'\n",
      "                                               'arXiv:2303.08774 (2023).\\n'\n",
      "                                               '[2] Yaroslav Aksenov, Nikita '\n",
      "                                               'Balagansky, Sofia Maria Lo '\n",
      "                                               'Cicero Vaina, Boris '\n",
      "                                               'Shaposhnikov, Alexey '\n",
      "                                               'Gorbatovski, and\\n'\n",
      "                                               'Daniil Gavrilov. “Linear '\n",
      "                                               'Transformers with Learnable '\n",
      "                                               'Kernel Functions are Better '\n",
      "                                               'In-Context Models”. In: arXiv\\n'\n",
      "                                               'preprint arXiv:2402.10644 '\n",
      "                                               '(2024).\\n'\n",
      "                                               '[3] Marcin Andrychowicz, Misha '\n",
      "                                               'Denil, Sergio Gomez, Matthew W '\n",
      "                                               'Hoffman, David Pfau, Tom '\n",
      "                                               'Schaul, Brendan\\n'\n",
      "                                               'Shillingford, and Nando De '\n",
      "                                               'Freitas. “Learning to learn by '\n",
      "                                               'gradient descent by gradient '\n",
      "                                               'descent”. In: Advances in\\n'\n",
      "                                               'neural information processing '\n",
      "                                               'systems 29 (2016).\\n'\n",
      "                                               '[4] Cem Anil, Yuhuai Wu, '\n",
      "                                               'Anders Andreassen, Aitor '\n",
      "                                               'Lewkowycz, Vedant Misra, Vinay '\n",
      "                                               'Ramasesh, Ambrose Slone,\\n'\n",
      "                                               'Guy Gur-Ari, Ethan Dyer, and '\n",
      "                                               'Behnam Neyshabur. “Exploring '\n",
      "                                               'length generalization in large '\n",
      "                                               'language models”. In:\\n'\n",
      "                                               'Advances in Neural Information '\n",
      "                                               'Processing Systems 35 (2022), '\n",
      "                                               'pp. 38546–38556.\\n'\n",
      "                                               '[5] Simran Arora, Sabri '\n",
      "                                               'Eyuboglu, Michael Zhang, Aman '\n",
      "                                               'Timalsina, Silas Alberti, '\n",
      "                                               'James Zou, Atri Rudra, and '\n",
      "                                               'Christo-\\n'\n",
      "                                               'pher Re. “Simple linear '\n",
      "                                               'attention language models '\n",
      "                                               'balance the recall-throughput '\n",
      "                                               'tradeoff”. In:Forty-first '\n",
      "                                               'International\\n'\n",
      "                                               'Conference on Machine Learning '\n",
      "                                               '. 2024. url: '\n",
      "                                               'https://openreview.net/forum?id=e93ffDcpH3.\\n'\n",
      "                                               '[6] Dzmitry Bahdanau. “Neural '\n",
      "                                               'machine translation by jointly '\n",
      "                                               'learning to align and '\n",
      "                                               'translate”. In: arXiv '\n",
      "                                               'preprint\\n'\n",
      "                                               'arXiv:1409.0473 (2014).\\n'\n",
      "                                               '[7] Reza Bayat, Mohammad '\n",
      "                                               'Pezeshki, Elvis Dohmatob, '\n",
      "                                               'David Lopez-Paz, and Pascal '\n",
      "                                               'Vincent. “The Pitfalls of '\n",
      "                                               'Memo-\\n'\n",
      "                                               'rization: When Memorization '\n",
      "                                               'Hurts Generalization”. In: '\n",
      "                                               'arXiv preprint '\n",
      "                                               'arXiv:2412.07684 (2024).\\n'\n",
      "                                               '[8] Maximilian Beck, Korbinian '\n",
      "                                               'Pöppel, Markus Spanring, '\n",
      "                                               'Andreas Auer, Oleksandra '\n",
      "                                               'Prudnikova, Michael Kopp,\\n'\n",
      "                                               'Günter Klambauer, Johannes '\n",
      "                                               'Brandstetter, and Sepp '\n",
      "                                               'Hochreiter. “xLSTM: Extended '\n",
      "                                               'Long Short-Term Memory”. In:\\n'\n",
      "                                               'arXiv preprint '\n",
      "                                               'arXiv:2405.04517 (2024).\\n'\n",
      "                                               '[9] Ali Behrouz, Michele '\n",
      "                                               'Santacatterina, and Ramin '\n",
      "                                               'Zabih. “Mambamixer: Efficient '\n",
      "                                               'selective state space models '\n",
      "                                               'with\\n'\n",
      "                                               'dual token and channel '\n",
      "                                               'selection”. In: arXiv preprint '\n",
      "                                               'arXiv:2403.19888 (2024).\\n'\n",
      "                                               '[10] Vincent-Pierre Berges, '\n",
      "                                               'Barlas Oğuz, Daniel Haziza, '\n",
      "                                               'Wen-tau Yih, Luke Zettlemoyer, '\n",
      "                                               'and Gargi Gosh. “Memory\\n'\n",
      "                                               'Layers at Scale”. In: arXiv '\n",
      "                                               'preprint arXiv:2412.09764 '\n",
      "                                               '(2024).\\n'\n",
      "                                               '[11] Alberto Bietti, Vivien '\n",
      "                                               'Cabannes, Diane Bouchacourt, '\n",
      "                                               'Herve Jegou, and Leon Bottou. '\n",
      "                                               '“Birth of a transformer: A\\n'\n",
      "                                               'memory viewpoint”. In: '\n",
      "                                               'Advances in Neural Information '\n",
      "                                               'Processing Systems 36 (2024).\\n'\n",
      "                                               '[12] Yonatan Bisk, Rowan '\n",
      "                                               'Zellers, Jianfeng Gao, Yejin '\n",
      "                                               'Choi, et al. “Piqa: Reasoning '\n",
      "                                               'about physical commonsense in\\n'\n",
      "                                               'natural language”. In: '\n",
      "                                               'Proceedings of the AAAI '\n",
      "                                               'conference on artificial '\n",
      "                                               'intelligence . Vol. 34. 05. '\n",
      "                                               '2020, pp. 7432–7439.\\n'\n",
      "                                               '[13] Aleksandar Botev, Soham '\n",
      "                                               'De, Samuel L Smith, Anushan '\n",
      "                                               'Fernando, George-Cristian '\n",
      "                                               'Muraru, Ruba Haroun, Leonard\\n'\n",
      "                                               'Berrada, Razvan Pascanu, Pier '\n",
      "                                               'Giuseppe Sessa, Robert '\n",
      "                                               'Dadashi, et al. '\n",
      "                                               '“RecurrentGemma: Moving Past '\n",
      "                                               'Transformers\\n'\n",
      "                                               'for Efficient Open Language '\n",
      "                                               'Models”. In: arXiv preprint '\n",
      "                                               'arXiv:2404.07839 (2024).\\n'\n",
      "                                               '[14] Léon Bottou and Vladimir '\n",
      "                                               'Vapnik. “Local learning '\n",
      "                                               'algorithms”. In: Neural '\n",
      "                                               'computation 4.6 (1992), pp. '\n",
      "                                               '888–900.\\n'\n",
      "                                               '[15] Aydar Bulatov, Yuri '\n",
      "                                               'Kuratov, Yermek Kapushev, and '\n",
      "                                               'Mikhail S Burtsev. “Scaling '\n",
      "                                               'transformer to 1m tokens and\\n'\n",
      "                                               'beyond with rmt”. In: arXiv '\n",
      "                                               'preprint arXiv:2304.11062 '\n",
      "                                               '(2023).\\n'\n",
      "                                               '[16] Aydar Bulatov, Yury '\n",
      "                                               'Kuratov, and Mikhail Burtsev. '\n",
      "                                               '“Recurrent memory '\n",
      "                                               'transformer”. In: Advances in '\n",
      "                                               'Neural\\n'\n",
      "                                               'Information Processing Systems '\n",
      "                                               '35 (2022), pp. 11079–11091.\\n'\n",
      "                                               '[17] Edoardo Cetin, Qi Sun, '\n",
      "                                               'Tianyu Zhao, and Yujin Tang. '\n",
      "                                               '“An Evolved Universal '\n",
      "                                               'Transformer Memory”. In: '\n",
      "                                               'arXiv\\n'\n",
      "                                               'preprint arXiv:2410.13166 '\n",
      "                                               '(2024).\\n'\n",
      "                                               '[18] Beidi Chen, Tri Dao, Eric '\n",
      "                                               'Winsor, Zhao Song, Atri Rudra, '\n",
      "                                               'and Christopher Ré. '\n",
      "                                               '“Scatterbrain: Unifying sparse '\n",
      "                                               'and\\n'\n",
      "                                               'low-rank attention”. In: '\n",
      "                                               'Advances in Neural Information '\n",
      "                                               'Processing Systems 34 (2021), '\n",
      "                                               'pp. 17413–17426.\\n'\n",
      "                                               '[19] Krzysztof Marcin '\n",
      "                                               'Choromanski, Valerii '\n",
      "                                               'Likhosherstov, David Dohan, '\n",
      "                                               'Xingyou Song, Andreea Gane, '\n",
      "                                               'Tamas Sarlos,\\n'\n",
      "                                               'Peter Hawkins, Jared Quincy '\n",
      "                                               'Davis, Afroz Mohiuddin, Lukasz '\n",
      "                                               'Kaiser, David Benjamin '\n",
      "                                               'Belanger, Lucy J Colwell, and\\n'\n",
      "                                               'Adrian Weller. “Rethinking '\n",
      "                                               'Attention with Performers”. '\n",
      "                                               'In: International Conference '\n",
      "                                               'on Learning Representations .\\n'\n",
      "                                               '2021. url: '\n",
      "                                               'https://openreview.net/forum?id=Ua6zuk0WRH.\\n'\n",
      "                                               '[20] Christopher Clark, Kenton '\n",
      "                                               'Lee, Ming-Wei Chang, Tom '\n",
      "                                               'Kwiatkowski, Michael Collins, '\n",
      "                                               'and Kristina Toutanova.\\n'\n",
      "                                               '“BoolQ: Exploring the '\n",
      "                                               'Surprising Difficulty of '\n",
      "                                               'Natural Yes/No Questions”. In: '\n",
      "                                               'Proceedings of the 2019 '\n",
      "                                               'Conference\\n'\n",
      "                                               'of the North American Chapter '\n",
      "                                               'of the Association for '\n",
      "                                               'Computational Linguistics: '\n",
      "                                               'Human Language Technologies,\\n'\n",
      "                                               'Volume 1 (Long and Short '\n",
      "                                               'Papers) . Ed. by Jill '\n",
      "                                               'Burstein, Christy Doran, and '\n",
      "                                               'Thamar Solorio. Minneapolis, '\n",
      "                                               'Minnesota:\\n'\n",
      "                                               'Association for Computational '\n",
      "                                               'Linguistics, June 2019, pp. '\n",
      "                                               '2924–2936. doi: '\n",
      "                                               '10.18653/v1/N19-1300. url: '\n",
      "                                               'https:\\n'\n",
      "                                               '//aclanthology.org/N19-1300/.\\n'\n",
      "                                               '18\\n'\n",
      "                                               '[21] Peter Clark, Isaac '\n",
      "                                               'Cowhey, Oren Etzioni, Tushar '\n",
      "                                               'Khot, Ashish Sabharwal, '\n",
      "                                               'Carissa Schoenick, and Oyvind '\n",
      "                                               'Tafjord.\\n'\n",
      "                                               '“Think you have solved '\n",
      "                                               'question answering? try arc, '\n",
      "                                               'the ai2 reasoning challenge”. '\n",
      "                                               'In:arXiv preprint '\n",
      "                                               'arXiv:1803.05457\\n'\n",
      "                                               '(2018).\\n'\n",
      "                                               '[22] Nelson Cowan. “What are '\n",
      "                                               'the differences between '\n",
      "                                               'long-term, short-term, and '\n",
      "                                               'working memory?” In: Progress '\n",
      "                                               'in\\n'\n",
      "                                               'brain research 169 (2008), pp. '\n",
      "                                               '323–338.\\n'\n",
      "                                               '[23] Zihang Dai, Zhilin Yang, '\n",
      "                                               'Yiming Yang, Jaime G. '\n",
      "                                               'Carbonell, Quoc Viet Le, and '\n",
      "                                               'Ruslan Salakhutdinov. '\n",
      "                                               '“Transformer-\\n'\n",
      "                                               'XL: Attentive Language Models '\n",
      "                                               'beyond a Fixed-Length '\n",
      "                                               'Context”. In: ACL (1). Ed. by '\n",
      "                                               'Anna Korhonen, David R.\\n'\n",
      "                                               'Traum, and Lluís Màrquez. '\n",
      "                                               'Association for Computational '\n",
      "                                               'Linguistics, 2019, pp. '\n",
      "                                               '2978–2988.isbn: '\n",
      "                                               '978-1-950737-48-2.\\n'\n",
      "                                               '[24] Tri Dao. '\n",
      "                                               '“FlashAttention-2: Faster '\n",
      "                                               'Attention with Better '\n",
      "                                               'Parallelism and Work '\n",
      "                                               'Partitioning”. In: The Twelfth '\n",
      "                                               'Inter-\\n'\n",
      "                                               'national Conference on '\n",
      "                                               'Learning Representations . '\n",
      "                                               '2024. url: '\n",
      "                                               'https://openreview.net/forum?id=mZn2Xyh9Ec.\\n'\n",
      "                                               '[25] Tri Dao, Dan Fu, Stefano '\n",
      "                                               'Ermon, Atri Rudra, and '\n",
      "                                               'Christopher Ré. '\n",
      "                                               '“FlashAttention: Fast and '\n",
      "                                               'Memory-Efficient\\n'\n",
      "                                               'Exact Attention with '\n",
      "                                               'IO-Awareness”. In:Advances in '\n",
      "                                               'Neural Information Processing '\n",
      "                                               'Systems . Ed. by S. Koyejo, '\n",
      "                                               'S.\\n'\n",
      "                                               'Mohamed, A. Agarwal, D. '\n",
      "                                               'Belgrave, K. Cho, and A. Oh. '\n",
      "                                               'Vol. 35. Curran Associates, '\n",
      "                                               'Inc., 2022, pp. 16344–16359. '\n",
      "                                               'url:\\n'\n",
      "                                               'https://proceedings.neurips.cc/paper_files/paper/2022/file/67d57c32e20fd0a7a302cb81d36e40d5-\\n'\n",
      "                                               'Paper-Conference.pdf.\\n'\n",
      "                                               '[26] Tri Dao and Albert Gu. '\n",
      "                                               '“Transformers are SSMs: '\n",
      "                                               'Generalized models and '\n",
      "                                               'efficient algorithms through '\n",
      "                                               'structured\\n'\n",
      "                                               'state space duality”. In: '\n",
      "                                               'arXiv preprint '\n",
      "                                               'arXiv:2405.21060 (2024).\\n'\n",
      "                                               '[27] Abhimanyu Das, Weihao '\n",
      "                                               'Kong, Andrew Leach, Shaan K '\n",
      "                                               'Mathur, Rajat Sen, and Rose '\n",
      "                                               'Yu. “Long-term Forecasting\\n'\n",
      "                                               'with TiDE: Time-series Dense '\n",
      "                                               'Encoder”. In: Transactions on '\n",
      "                                               'Machine Learning Research '\n",
      "                                               '(2023). issn: 2835-8856. url:\\n'\n",
      "                                               'https://openreview.net/forum?id=pCbC3aQB5W.\\n'\n",
      "                                               '[28] Soham De, Samuel L Smith, '\n",
      "                                               'Anushan Fernando, Aleksandar '\n",
      "                                               'Botev, George Cristian-Muraru, '\n",
      "                                               'Albert Gu, Ruba\\n'\n",
      "                                               'Haroun, Leonard Berrada, '\n",
      "                                               'Yutian Chen, Srivatsan '\n",
      "                                               'Srinivasan, et al. “Griffin: '\n",
      "                                               'Mixing gated linear '\n",
      "                                               'recurrences with\\n'\n",
      "                                               'local attention for efficient '\n",
      "                                               'language models”. In: arXiv '\n",
      "                                               'preprint arXiv:2402.19427 '\n",
      "                                               '(2024).\\n'\n",
      "                                               '[29] Juechu Dong, Boyuan Feng, '\n",
      "                                               'Driss Guessous, Yanbo Liang, '\n",
      "                                               'and Horace He. “Flex '\n",
      "                                               'Attention: A Programming '\n",
      "                                               'Model\\n'\n",
      "                                               'for Generating Optimized '\n",
      "                                               'Attention Kernels”. In: arXiv '\n",
      "                                               'preprint arXiv:2412.05496 '\n",
      "                                               '(2024).\\n'\n",
      "                                               '[30] Xin Dong, Yonggan Fu, '\n",
      "                                               'Shizhe Diao, Wonmin Byeon, '\n",
      "                                               'Zijia Chen, Ameya Sunil '\n",
      "                                               'Mahabaleshwarkar, Shih-Yang '\n",
      "                                               'Liu,\\n'\n",
      "                                               'Matthijs Van Keirsbilck, '\n",
      "                                               'Min-Hung Chen, Yoshi Suhara, '\n",
      "                                               'et al. “Hymba: A Hybrid-head '\n",
      "                                               'Architecture for Small\\n'\n",
      "                                               'Language Models”. In: arXiv '\n",
      "                                               'preprint arXiv:2411.13676 '\n",
      "                                               '(2024).\\n'\n",
      "                                               '[31] Stefan Elfwing, Eiji '\n",
      "                                               'Uchibe, and Kenji Doya. '\n",
      "                                               '“Sigmoid-weighted linear units '\n",
      "                                               'for neural network function '\n",
      "                                               'approxi-\\n'\n",
      "                                               'mation in reinforcement '\n",
      "                                               'learning”. In: Neural networks '\n",
      "                                               '107 (2018), pp. 3–11.\\n'\n",
      "                                               '[32] Yukun Feng, Feng Li, '\n",
      "                                               'Ziang Song, Boyuan Zheng, and '\n",
      "                                               'Philipp Koehn. “Learn to '\n",
      "                                               'remember: Transformer with\\n'\n",
      "                                               'recurrent memory for '\n",
      "                                               'document-level machine '\n",
      "                                               'translation”. In: arXiv '\n",
      "                                               'preprint arXiv:2205.01546 '\n",
      "                                               '(2022).\\n'\n",
      "                                               '[33] Daniel Y Fu, Tri Dao, '\n",
      "                                               'Khaled Kamal Saab, Armin W '\n",
      "                                               'Thomas, Atri Rudra, and '\n",
      "                                               'Christopher Re. “Hungry '\n",
      "                                               'Hungry\\n'\n",
      "                                               'Hippos: Towards Language '\n",
      "                                               'Modeling with State Space '\n",
      "                                               'Models”. In:The Eleventh '\n",
      "                                               'International Conference on '\n",
      "                                               'Learning\\n'\n",
      "                                               'Representations. 2023. url: '\n",
      "                                               'https://openreview.net/forum?id=COZDy0WYGg.\\n'\n",
      "                                               '[34] Yossi Gandelsman, Yu Sun, '\n",
      "                                               'Xinlei Chen, and Alexei Efros. '\n",
      "                                               '“Test-time training with '\n",
      "                                               'masked autoencoders”. In:\\n'\n",
      "                                               'Advances in Neural Information '\n",
      "                                               'Processing Systems 35 (2022), '\n",
      "                                               'pp. 29374–29385.\\n'\n",
      "                                               '[35] Leo Gao, Stella Biderman, '\n",
      "                                               'Sid Black, Laurence Golding, '\n",
      "                                               'Travis Hoppe, Charles Foster, '\n",
      "                                               'Jason Phang, Horace He,\\n'\n",
      "                                               'Anish Thite, Noa Nabeshima, et '\n",
      "                                               'al. “The pile: An 800gb '\n",
      "                                               'dataset of diverse text for '\n",
      "                                               'language modeling”. In: arXiv\\n'\n",
      "                                               'preprint arXiv:2101.00027 '\n",
      "                                               '(2020).\\n'\n",
      "                                               '[36] Felix A Gers, Jürgen '\n",
      "                                               'Schmidhuber, and Fred Cummins. '\n",
      "                                               '“Learning to forget: Continual '\n",
      "                                               'prediction with LSTM”. In:\\n'\n",
      "                                               'Neural computation 12.10 '\n",
      "                                               '(2000), pp. 2451–2471.\\n'\n",
      "                                               '[37] Alex Graves, Greg Wayne, '\n",
      "                                               'and Ivo Danihelka. Neural '\n",
      "                                               'Turing Machines . 2014. arXiv: '\n",
      "                                               '1410.5401 [cs.NE]. url:\\n'\n",
      "                                               'https://arxiv.org/abs/1410.5401.\\n'\n",
      "                                               '[38] Klaus Greff, Rupesh K '\n",
      "                                               'Srivastava, Jan Koutník, Bas R '\n",
      "                                               'Steunebrink, and Jürgen '\n",
      "                                               'Schmidhuber. “LSTM: A search '\n",
      "                                               'space\\n'\n",
      "                                               'odyssey”. In: IEEE '\n",
      "                                               'transactions on neural '\n",
      "                                               'networks and learning systems '\n",
      "                                               '28.10 (2016), pp. 2222–2232.\\n'\n",
      "                                               '[39] Katarína Grešová, '\n",
      "                                               'Vlastimil Martinek, David '\n",
      "                                               'Čechák, Petr Šimeček, and '\n",
      "                                               'Panagiotis Alexiou. “Genomic '\n",
      "                                               'benchmarks:\\n'\n",
      "                                               'a collection of datasets for '\n",
      "                                               'genomic sequence '\n",
      "                                               'classification”. In: BMC '\n",
      "                                               'Genomic Data 24.1 (2023), p. '\n",
      "                                               '25.\\n'\n",
      "                                               '[40] Albert Gu and Tri Dao. '\n",
      "                                               '“Mamba: Linear-Time Sequence '\n",
      "                                               'Modeling with Selective State '\n",
      "                                               'Spaces”. In: First Conference\\n'\n",
      "                                               'on Language Modeling . 2024. '\n",
      "                                               'url: '\n",
      "                                               'https://openreview.net/forum?id=tEYskw1VY2.\\n'\n",
      "                                               '[41] Albert Gu, Karan Goel, '\n",
      "                                               'and Christopher Re. '\n",
      "                                               '“Efficiently Modeling Long '\n",
      "                                               'Sequences with Structured '\n",
      "                                               'State Spaces”.\\n'\n",
      "                                               'In: International Conference '\n",
      "                                               'on Learning Representations . '\n",
      "                                               '2022. url: https : / / '\n",
      "                                               'openreview . net / forum ? id '\n",
      "                                               '=\\n'\n",
      "                                               'uYLFoz1vlAC.\\n'\n",
      "                                               '19\\n'\n",
      "                                               '[42] Chi Han, Qifan Wang, Hao '\n",
      "                                               'Peng, Wenhan Xiong, Yu Chen, '\n",
      "                                               'Heng Ji, and Sinong Wang. '\n",
      "                                               '“LM-Infinite: Zero-Shot\\n'\n",
      "                                               'Extreme Length Generalization '\n",
      "                                               'for Large Language Models”. '\n",
      "                                               'In: Proceedings of the 2024 '\n",
      "                                               'Conference of the North\\n'\n",
      "                                               'American Chapter of the '\n",
      "                                               'Association for Computational '\n",
      "                                               'Linguistics: Human Language '\n",
      "                                               'Technologies (Volume 1: Long\\n'\n",
      "                                               'Papers). Ed. by Kevin Duh, '\n",
      "                                               'Helena Gomez, and Steven '\n",
      "                                               'Bethard. Mexico City, Mexico: '\n",
      "                                               'Association for Computational\\n'\n",
      "                                               'Linguistics, June 2024, pp. '\n",
      "                                               '3991–4008. doi: '\n",
      "                                               '10.18653/v1/2024.naacl-long.222. '\n",
      "                                               'url: https://aclanthology.\\n'\n",
      "                                               'org/2024.naacl-long.222.\\n'\n",
      "                                               '[43] Ramin Hasani, Mathias '\n",
      "                                               'Lechner, Tsun-Hsuan Wang, '\n",
      "                                               'Makram Chahine, Alexander '\n",
      "                                               'Amini, and Daniela Rus. '\n",
      "                                               '“Liquid\\n'\n",
      "                                               'Structural State-Space '\n",
      "                                               'Models”. In: The Eleventh '\n",
      "                                               'International Conference on '\n",
      "                                               'Learning Representations . '\n",
      "                                               '2023. url:\\n'\n",
      "                                               'https://openreview.net/forum?id=g4OTKRKfS7R.\\n'\n",
      "                                               '[44] Zexue He, Leonid '\n",
      "                                               'Karlinsky, Donghyun Kim, '\n",
      "                                               'Julian McAuley, Dmitry Krotov, '\n",
      "                                               'and Rogerio Feris. “CAMELoT:\\n'\n",
      "                                               'Towards Large Language Models '\n",
      "                                               'with Training-Free '\n",
      "                                               'Consolidated Associative '\n",
      "                                               'Memory”. In: arXiv preprint\\n'\n",
      "                                               'arXiv:2402.13449 (2024).\\n'\n",
      "                                               '[45] Donald Olding Hebb. The '\n",
      "                                               'organization of behavior: A '\n",
      "                                               'neuropsychological theory . '\n",
      "                                               'Psychology press, 2005.\\n'\n",
      "                                               '[46] John J Hopfield. “Neural '\n",
      "                                               'networks and physical systems '\n",
      "                                               'with emergent collective '\n",
      "                                               'computational abilities.” In:\\n'\n",
      "                                               'Proceedings of the national '\n",
      "                                               'academy of sciences 79.8 '\n",
      "                                               '(1982), pp. 2554–2558.\\n'\n",
      "                                               '[47] Kurt Hornik, Maxwell '\n",
      "                                               'Stinchcombe, and Halbert '\n",
      "                                               'White. “Multilayer feedforward '\n",
      "                                               'networks are universal '\n",
      "                                               'approxi-\\n'\n",
      "                                               'mators”. In: Neural networks '\n",
      "                                               '2.5 (1989), pp. 359–366.\\n'\n",
      "                                               '[48] Cheng-Ping Hsieh, Simeng '\n",
      "                                               'Sun, Samuel Kriman, Shantanu '\n",
      "                                               'Acharya, Dima Rekesh, Fei Jia, '\n",
      "                                               'and Boris Ginsburg.\\n'\n",
      "                                               '“RULER: What’s the Real '\n",
      "                                               'Context Size of Your '\n",
      "                                               'Long-Context Language Models?” '\n",
      "                                               'In: First Conference on '\n",
      "                                               'Language\\n'\n",
      "                                               'Modeling. 2024. url: '\n",
      "                                               'https://openreview.net/forum?id=kIoBbc76Sy.\\n'\n",
      "                                               '[49] DeLesley Hutchins, Imanol '\n",
      "                                               'Schlag, Yuhuai Wu, Ethan Dyer, '\n",
      "                                               'and Behnam Neyshabur. '\n",
      "                                               '“Block-recurrent '\n",
      "                                               'transformers”.\\n'\n",
      "                                               'In: Advances in neural '\n",
      "                                               'information processing systems '\n",
      "                                               '35 (2022), pp. 33248–33261.\\n'\n",
      "                                               '[50] Kazuki Irie, Róbert '\n",
      "                                               'Csordás, and Jürgen '\n",
      "                                               'Schmidhuber. “The dual form of '\n",
      "                                               'neural networks revisited: '\n",
      "                                               'Connecting test\\n'\n",
      "                                               'time predictions to training '\n",
      "                                               'patterns via spotlights of '\n",
      "                                               'attention”. In: International '\n",
      "                                               'Conference on Machine Learning '\n",
      "                                               '.\\n'\n",
      "                                               'PMLR. 2022, pp. 9639–9659.\\n'\n",
      "                                               '[51] Kazuki Irie, Imanol '\n",
      "                                               'Schlag, Róbert Csordás, and '\n",
      "                                               'Jürgen Schmidhuber. “Going '\n",
      "                                               'beyond linear transformers '\n",
      "                                               'with\\n'\n",
      "                                               'recurrent fast weight '\n",
      "                                               'programmers”. In: Advances in '\n",
      "                                               'neural information processing '\n",
      "                                               'systems 34 (2021), pp. '\n",
      "                                               '7703–7717.\\n'\n",
      "                                               '[52] Vidit Jain and Erik '\n",
      "                                               'Learned-Miller. “Online domain '\n",
      "                                               'adaptation of a pre-trained '\n",
      "                                               'cascade of classifiers”. In: '\n",
      "                                               'CVPR\\n'\n",
      "                                               '2011. IEEE. 2011, pp. '\n",
      "                                               '577–584.\\n'\n",
      "                                               '[53] Albert Q Jiang, Alexandre '\n",
      "                                               'Sablayrolles, Arthur Mensch, '\n",
      "                                               'Chris Bamford, Devendra Singh '\n",
      "                                               'Chaplot, Diego de las\\n'\n",
      "                                               'Casas, Florian Bressand, '\n",
      "                                               'Gianna Lengyel, Guillaume '\n",
      "                                               'Lample, Lucile Saulnier, et '\n",
      "                                               'al. “Mistral 7B”. In: arXiv '\n",
      "                                               'preprint\\n'\n",
      "                                               'arXiv:2310.06825 (2023).\\n'\n",
      "                                               '[54] Praneeth Kacham, Vahab '\n",
      "                                               'Mirrokni, and Peilin Zhong. '\n",
      "                                               '“PolySketchFormer: Fast '\n",
      "                                               'Transformers via Sketching '\n",
      "                                               'Polyno-\\n'\n",
      "                                               'mial Kernels”. In: Forty-first '\n",
      "                                               'International Conference on '\n",
      "                                               'Machine Learning . 2024. url: '\n",
      "                                               'https://openreview.net/\\n'\n",
      "                                               'forum?id=ghYrfdJfjK.\\n'\n",
      "                                               '[55] Jared Kaplan, Sam '\n",
      "                                               'McCandlish, Tom Henighan, Tom '\n",
      "                                               'B Brown, Benjamin Chess, Rewon '\n",
      "                                               'Child, Scott Gray, Alec\\n'\n",
      "                                               'Radford, Jeffrey Wu, and Dario '\n",
      "                                               'Amodei. “Scaling laws for '\n",
      "                                               'neural language models”. '\n",
      "                                               'In:arXiv preprint '\n",
      "                                               'arXiv:2001.08361\\n'\n",
      "                                               '(2020).\\n'\n",
      "                                               '[56] Angelos Katharopoulos, '\n",
      "                                               'Apoorv Vyas, Nikolaos Pappas, '\n",
      "                                               'and François Fleuret. '\n",
      "                                               '“Transformers are rnns: Fast '\n",
      "                                               'au-\\n'\n",
      "                                               'toregressive transformers with '\n",
      "                                               'linear attention”. In: '\n",
      "                                               'International conference on '\n",
      "                                               'machine learning . PMLR. '\n",
      "                                               '2020,\\n'\n",
      "                                               'pp. 5156–5165.\\n'\n",
      "                                               '[57] Urvashi Khandelwal, Omer '\n",
      "                                               'Levy, Dan Jurafsky, Luke '\n",
      "                                               'Zettlemoyer, and Mike Lewis. '\n",
      "                                               '“Generalization through\\n'\n",
      "                                               'Memorization: Nearest Neighbor '\n",
      "                                               'Language Models”. In: '\n",
      "                                               'International Conference on '\n",
      "                                               'Learning Representations . '\n",
      "                                               '2020.\\n'\n",
      "                                               'url: '\n",
      "                                               'https://openreview.net/forum?id=HklBjCEKvH.\\n'\n",
      "                                               '[58] Yuri Kuratov, Aydar '\n",
      "                                               'Bulatov, Petr Anokhin, Ivan '\n",
      "                                               'Rodkin, Dmitry Igorevich '\n",
      "                                               'Sorokin, Artyom Sorokin, and '\n",
      "                                               'Mikhail\\n'\n",
      "                                               'Burtsev. “BABILong: Testing '\n",
      "                                               'the Limits of LLMs with Long '\n",
      "                                               'Context '\n",
      "                                               'Reasoning-in-a-Haystack”. In: '\n",
      "                                               'The Thirty-\\n'\n",
      "                                               'eight Conference on Neural '\n",
      "                                               'Information Processing Systems '\n",
      "                                               'Datasets and Benchmarks Track '\n",
      "                                               '. 2024. url: https:\\n'\n",
      "                                               '//openreview.net/forum?id=u7m2CG84BQ.\\n'\n",
      "                                               '[59] Hung Le, Truyen Tran, and '\n",
      "                                               'Svetha Venkatesh. '\n",
      "                                               '“Self-attentive associative '\n",
      "                                               'memory”. In:International '\n",
      "                                               'conference on\\n'\n",
      "                                               'machine learning . PMLR. 2020, '\n",
      "                                               'pp. 5682–5691.\\n'\n",
      "                                               '[60] Patrick Lewis, Ethan '\n",
      "                                               'Perez, Aleksandra Piktus, '\n",
      "                                               'Fabio Petroni, Vladimir '\n",
      "                                               'Karpukhin, Naman Goyal, '\n",
      "                                               'Heinrich Küttler,\\n'\n",
      "                                               'Mike Lewis, Wen-tau Yih, Tim '\n",
      "                                               'Rocktäschel, et al. '\n",
      "                                               '“Retrieval-augmented '\n",
      "                                               'generation for '\n",
      "                                               'knowledge-intensive nlp\\n'\n",
      "                                               'tasks”. In: Advances in Neural '\n",
      "                                               'Information Processing Systems '\n",
      "                                               '33 (2020), pp. 9459–9474.\\n'\n",
      "                                               '20\\n'\n",
      "                                               '[61] Danny Leybzon and '\n",
      "                                               'Corentin Kervadec. “Learning, '\n",
      "                                               'Forgetting, Remembering: '\n",
      "                                               'Insights From Tracking LLM '\n",
      "                                               'Mem-\\n'\n",
      "                                               'orization During Training”. '\n",
      "                                               'In: Proceedings of the 7th '\n",
      "                                               'BlackboxNLP Workshop: '\n",
      "                                               'Analyzing and Interpreting '\n",
      "                                               'Neural\\n'\n",
      "                                               'Networks for NLP . 2024, pp. '\n",
      "                                               '43–57.\\n'\n",
      "                                               '[62] Zhe Li, Shiyi Qi, Yiduo '\n",
      "                                               'Li, and Zenglin Xu. '\n",
      "                                               '“Revisiting long-term time '\n",
      "                                               'series forecasting: An '\n",
      "                                               'investigation on linear\\n'\n",
      "                                               'mapping”. In: arXiv preprint '\n",
      "                                               'arXiv:2305.10721 (2023).\\n'\n",
      "                                               '[63] Bo Liu, Rui Wang, Lemeng '\n",
      "                                               'Wu, Yihao Feng, Peter Stone, '\n",
      "                                               'and Qiang Liu. “Longhorn: '\n",
      "                                               'State space models are '\n",
      "                                               'amortized\\n'\n",
      "                                               'online learners”. In: arXiv '\n",
      "                                               'preprint arXiv:2407.14207 '\n",
      "                                               '(2024).\\n'\n",
      "                                               '[64] Nelson F Liu, Kevin Lin, '\n",
      "                                               'John Hewitt, Ashwin Paranjape, '\n",
      "                                               'Michele Bevilacqua, Fabio '\n",
      "                                               'Petroni, and Percy Liang.\\n'\n",
      "                                               '“Lost in the middle: How '\n",
      "                                               'language models use long '\n",
      "                                               'contexts”. In: Transactions of '\n",
      "                                               'the Association for '\n",
      "                                               'Computational\\n'\n",
      "                                               'Linguistics 12 (2024), pp. '\n",
      "                                               '157–173.\\n'\n",
      "                                               '[65] Yong Liu, Tengge Hu, '\n",
      "                                               'Haoran Zhang, Haixu Wu, Shiyu '\n",
      "                                               'Wang, Lintao Ma, and Mingsheng '\n",
      "                                               'Long. “itransformer:\\n'\n",
      "                                               'Inverted transformers are '\n",
      "                                               'effective for time series '\n",
      "                                               'forecasting”. In: arXiv '\n",
      "                                               'preprint arXiv:2310.06625 '\n",
      "                                               '(2023).\\n'\n",
      "                                               '[66] George Mandler. “The '\n",
      "                                               'structure of value: Accounting '\n",
      "                                               'for taste”. In: Affect and '\n",
      "                                               'cognition . Psychology Press, '\n",
      "                                               '2014,\\n'\n",
      "                                               'pp. 3–36.\\n'\n",
      "                                               '[67] Harsh Mehta, Ankit Gupta, '\n",
      "                                               'Ashok Cutkosky, and Behnam '\n",
      "                                               'Neyshabur. “Long Range '\n",
      "                                               'Language Modeling via\\n'\n",
      "                                               'Gated State Spaces”. In: The '\n",
      "                                               'Eleventh International '\n",
      "                                               'Conference on Learning '\n",
      "                                               'Representations . 2023. url: '\n",
      "                                               'https:\\n'\n",
      "                                               '//openreview.net/forum?id=5MkYIYCbva.\\n'\n",
      "                                               '[68] Stephen Merity, Caiming '\n",
      "                                               'Xiong, James Bradbury, and '\n",
      "                                               'Richard Socher. “Pointer '\n",
      "                                               'Sentinel Mixture Models”. In:\\n'\n",
      "                                               'International Conference on '\n",
      "                                               'Learning Representations . '\n",
      "                                               '2017. url: '\n",
      "                                               'https://openreview.net/forum?id=Byj72udxe.\\n'\n",
      "                                               '[69] William Merrill, Jackson '\n",
      "                                               'Petty, and Ashish Sabharwal. '\n",
      "                                               '“The Illusion of State in '\n",
      "                                               'State-Space Models”. In: '\n",
      "                                               'Forty-first\\n'\n",
      "                                               'International Conference on '\n",
      "                                               'Machine Learning . 2024. url: '\n",
      "                                               'https://openreview.net/forum?id=QZgo9JZpLq.\\n'\n",
      "                                               '[70] Ravi Teja Mullapudi, '\n",
      "                                               'Steven Chen, Keyi Zhang, Deva '\n",
      "                                               'Ramanan, and Kayvon '\n",
      "                                               'Fatahalian. “Online model '\n",
      "                                               'distillation\\n'\n",
      "                                               'for efficient video '\n",
      "                                               'inference”. In: Proceedings of '\n",
      "                                               'the IEEE/CVF International '\n",
      "                                               'conference on computer vision '\n",
      "                                               '. 2019,\\n'\n",
      "                                               'pp. 3573–3582.\\n'\n",
      "                                               '[71] Tsendsuren Munkhdalai, '\n",
      "                                               'Manaal Faruqui, and Siddharth '\n",
      "                                               'Gopal. “Leave no context '\n",
      "                                               'behind: Efficient infinite '\n",
      "                                               'context\\n'\n",
      "                                               'transformers with '\n",
      "                                               'infini-attention”. In: arXiv '\n",
      "                                               'preprint arXiv:2404.07143 '\n",
      "                                               '(2024).\\n'\n",
      "                                               '[72] Tsendsuren Munkhdalai, '\n",
      "                                               'Alessandro Sordoni, Tong Wang, '\n",
      "                                               'and Adam Trischler. '\n",
      "                                               '“Metalearned neural memory”. '\n",
      "                                               'In:\\n'\n",
      "                                               'Advances in Neural Information '\n",
      "                                               'Processing Systems 32 (2019).\\n'\n",
      "                                               '[73] Tsendsuren Munkhdalai and '\n",
      "                                               'Hong Yu. “Neural semantic '\n",
      "                                               'encoders”. In: Proceedings of '\n",
      "                                               'the conference. Association '\n",
      "                                               'for\\n'\n",
      "                                               'Computational Linguistics. '\n",
      "                                               'Meeting . Vol. 1. NIH Public '\n",
      "                                               'Access. 2017, p. 397.\\n'\n",
      "                                               '[74] Eric Nguyen, Michael '\n",
      "                                               'Poli, Marjan Faizi, Armin '\n",
      "                                               'Thomas, Michael Wornow, Callum '\n",
      "                                               'Birch-Sykes, Stefano '\n",
      "                                               'Massaroli,\\n'\n",
      "                                               'Aman Patel, Clayton Rabideau, '\n",
      "                                               'Yoshua Bengio, et al. '\n",
      "                                               '“Hyenadna: Long-range genomic '\n",
      "                                               'sequence modeling at single\\n'\n",
      "                                               'nucleotide resolution”. In: '\n",
      "                                               'Advances in neural information '\n",
      "                                               'processing systems 36 (2024).\\n'\n",
      "                                               '[75] A Nichol. “On first-order '\n",
      "                                               'meta-learning algorithms”. In: '\n",
      "                                               'arXiv preprint '\n",
      "                                               'arXiv:1803.02999 (2018).\\n'\n",
      "                                               '[76] Yuqi Nie, Nam H Nguyen, '\n",
      "                                               'Phanwadee Sinthong, and Jayant '\n",
      "                                               'Kalagnanam. “A time series is '\n",
      "                                               'worth 64 words:\\n'\n",
      "                                               'Long-term forecasting with '\n",
      "                                               'transformers”. In: arXiv '\n",
      "                                               'preprint arXiv:2211.14730 '\n",
      "                                               '(2022).\\n'\n",
      "                                               '[77] Hideyuki Okano, Tomoo '\n",
      "                                               'Hirano, and Evan Balaban. '\n",
      "                                               '“Learning and memory”. '\n",
      "                                               'In:Proceedings of the National '\n",
      "                                               'Academy\\n'\n",
      "                                               'of Sciences 97.23 (2000), pp. '\n",
      "                                               '12403–12404.\\n'\n",
      "                                               '[78] Antonio Orvieto, Samuel L '\n",
      "                                               'Smith, Albert Gu, Anushan '\n",
      "                                               'Fernando, Caglar Gulcehre, '\n",
      "                                               'Razvan Pascanu, and Soham De.\\n'\n",
      "                                               '“Resurrecting recurrent neural '\n",
      "                                               'networks for long sequences”. '\n",
      "                                               'In: International Conference '\n",
      "                                               'on Machine Learning .\\n'\n",
      "                                               'PMLR. 2023, pp. 26670–26698.\\n'\n",
      "                                               '[79] Denis Paperno, Germán '\n",
      "                                               'Kruszewski, Angeliki '\n",
      "                                               'Lazaridou, Ngoc Quan Pham, '\n",
      "                                               'Raffaella Bernardi, Sandro '\n",
      "                                               'Pezzelle,\\n'\n",
      "                                               'Marco Baroni, Gemma Boleda, '\n",
      "                                               'and Raquel Fernández. “The '\n",
      "                                               'LAMBADA dataset: Word '\n",
      "                                               'prediction requiring a broad\\n'\n",
      "                                               'discourse context”. '\n",
      "                                               'In:Proceedings of the 54th '\n",
      "                                               'Annual Meeting of the '\n",
      "                                               'Association for Computational '\n",
      "                                               'Linguistics (Volume\\n'\n",
      "                                               '1: Long Papers) . Ed. by '\n",
      "                                               'Katrin Erk and Noah A. Smith. '\n",
      "                                               'Berlin, Germany: Association '\n",
      "                                               'for Computational '\n",
      "                                               'Linguistics,\\n'\n",
      "                                               'Aug. 2016, pp. 1525–1534. doi: '\n",
      "                                               '10.18653/v1/P16-1144. url: '\n",
      "                                               'https://aclanthology.org/P16-1144/.\\n'\n",
      "                                               '[80] Badri N. Patro and Vijay '\n",
      "                                               'S. Agneeswaran. SiMBA: '\n",
      "                                               'Simplified Mamba-Based '\n",
      "                                               'Architecture for Vision and '\n",
      "                                               'Multivariate\\n'\n",
      "                                               'Time series . 2024. arXiv: '\n",
      "                                               '2403.15360 [cs.CV].\\n'\n",
      "                                               '[81] Guilherme Penedo, Hynek '\n",
      "                                               'Kydlíček, Loubna Ben allal, '\n",
      "                                               'Anton Lozhkov, Margaret '\n",
      "                                               'Mitchell, Colin Raffel, '\n",
      "                                               'Leandro\\n'\n",
      "                                               'Von Werra, and Thomas Wolf. '\n",
      "                                               '“The FineWeb Datasets: '\n",
      "                                               'Decanting the Web for the '\n",
      "                                               'Finest Text Data at Scale”. '\n",
      "                                               'In:\\n'\n",
      "                                               'The Thirty-eight Conference on '\n",
      "                                               'Neural Information Processing '\n",
      "                                               'Systems Datasets and '\n",
      "                                               'Benchmarks Track . 2024. url:\\n'\n",
      "                                               'https://openreview.net/forum?id=n6SCkn2QaG.\\n'\n",
      "                                               '[82] Bo Peng. RWKV-LM. Version '\n",
      "                                               '1.0.0. Aug. 2021. doi: '\n",
      "                                               '10.5281/zenodo.5196577 . url: '\n",
      "                                               'https://github.com/\\n'\n",
      "                                               'BlinkDL/RWKV-LM.\\n'\n",
      "                                               '21\\n'\n",
      "                                               '[83] Bo Peng, Eric Alcaide, '\n",
      "                                               'Quentin Gregory Anthony, Alon '\n",
      "                                               'Albalak, Samuel Arcadinho, '\n",
      "                                               'Stella Biderman, Huanqi Cao,\\n'\n",
      "                                               'Xin Cheng, Michael Nguyen '\n",
      "                                               'Chung, Leon Derczynski, '\n",
      "                                               'Xingjian Du, Matteo Grella, '\n",
      "                                               'Kranthi Kiran GV, Xuzheng He,\\n'\n",
      "                                               'Haowen Hou, Przemyslaw '\n",
      "                                               'Kazienko, Jan Kocon, Jiaming '\n",
      "                                               'Kong, Bartłomiej Koptyra, '\n",
      "                                               'Hayden Lau, Jiaju Lin, '\n",
      "                                               'Krishna\\n'\n",
      "                                               'Sri Ipsit Mantri, Ferdinand '\n",
      "                                               'Mom, Atsushi Saito, Guangyu '\n",
      "                                               'Song, Xiangru Tang, Johan S. '\n",
      "                                               'Wind, Stanisław Woźniak,\\n'\n",
      "                                               'Zhenyuan Zhang, Qinghua Zhou, '\n",
      "                                               'Jian Zhu, and Rui-Jie Zhu. '\n",
      "                                               '“RWKV: Reinventing RNNs for '\n",
      "                                               'the Transformer Era”.\\n'\n",
      "                                               'In: The 2023 Conference on '\n",
      "                                               'Empirical Methods in Natural '\n",
      "                                               'Language Processing . 2023. '\n",
      "                                               'url: https://openreview.\\n'\n",
      "                                               'net/forum?id=7SaXczaBpG.\\n'\n",
      "                                               '[84] Bo Peng, Daniel '\n",
      "                                               'Goldstein, Quentin Anthony, '\n",
      "                                               'Alon Albalak, Eric Alcaide, '\n",
      "                                               'Stella Biderman, Eugene Cheah, '\n",
      "                                               'Xingjian\\n'\n",
      "                                               'Du, Teddy Ferdinan, Haowen '\n",
      "                                               'Hou, et al. “Eagle and finch: '\n",
      "                                               'Rwkv with matrix-valued states '\n",
      "                                               'and dynamic recurrence”.\\n'\n",
      "                                               'In: arXiv preprint '\n",
      "                                               'arXiv:2404.05892 (2024).\\n'\n",
      "                                               '[85] DL Prados and SC Kak. '\n",
      "                                               '“Neural network capacity using '\n",
      "                                               'delta rule”. In: Electronics '\n",
      "                                               'Letters 25.3 (1989), pp. '\n",
      "                                               '197–199.\\n'\n",
      "                                               '[86] Zhen Qin, Yiran Zhong, '\n",
      "                                               'and Hui Deng. “Exploring '\n",
      "                                               'Transformer Extrapolation”. '\n",
      "                                               'In: Proceedings of the AAAI\\n'\n",
      "                                               'Conference on Artificial '\n",
      "                                               'Intelligence . Vol. 38. 17. '\n",
      "                                               '2024, pp. 18897–18905.\\n'\n",
      "                                               '[87] Liliang Ren, Yang Liu, '\n",
      "                                               'Yadong Lu, Yelong Shen, Chen '\n",
      "                                               'Liang, and Weizhu Chen. '\n",
      "                                               '“Samba: Simple Hybrid State '\n",
      "                                               'Space\\n'\n",
      "                                               'Models for Efficient Unlimited '\n",
      "                                               'Context Language Modeling”. '\n",
      "                                               'In: arXiv preprint '\n",
      "                                               'arXiv:2406.07522 (2024).\\n'\n",
      "                                               '[88] Ivan Rodkin, Yuri '\n",
      "                                               'Kuratov, Aydar Bulatov, and '\n",
      "                                               'Mikhail Burtsev. “Associative '\n",
      "                                               'recurrent memory transformer”. '\n",
      "                                               'In:\\n'\n",
      "                                               'arXiv preprint '\n",
      "                                               'arXiv:2407.04841 (2024).\\n'\n",
      "                                               '[89] Aurko Roy, Mohammad '\n",
      "                                               'Saffar, Ashish Vaswani, and '\n",
      "                                               'David Grangier. “Efficient '\n",
      "                                               'content-based sparse attention '\n",
      "                                               'with\\n'\n",
      "                                               'routing transformers”. In: '\n",
      "                                               'Transactions of the '\n",
      "                                               'Association for Computational '\n",
      "                                               'Linguistics 9 (2021), pp. '\n",
      "                                               '53–68.\\n'\n",
      "                                               '[90] Keisuke Sakaguchi, Ronan '\n",
      "                                               'Le Bras, Chandra Bhagavatula, '\n",
      "                                               'and Yejin Choi. “Winogrande: '\n",
      "                                               'An adversarial winograd\\n'\n",
      "                                               'schema challenge at scale”. '\n",
      "                                               'In: Communications of the ACM '\n",
      "                                               '64.9 (2021), pp. 99–106.\\n'\n",
      "                                               '[91] Maarten Sap, Hannah '\n",
      "                                               'Rashkin, Derek Chen, Ronan Le '\n",
      "                                               'Bras, and Yejin Choi. “Social '\n",
      "                                               'IQa: Commonsense Reasoning\\n'\n",
      "                                               'about Social Interactions”. '\n",
      "                                               'In:Proceedings of the 2019 '\n",
      "                                               'Conference on Empirical '\n",
      "                                               'Methods in Natural Language '\n",
      "                                               'Processing\\n'\n",
      "                                               'and the 9th International '\n",
      "                                               'Joint Conference on Natural '\n",
      "                                               'Language Processing '\n",
      "                                               '(EMNLP-IJCNLP) . Ed. by '\n",
      "                                               'Kentaro Inui,\\n'\n",
      "                                               'Jing Jiang, Vincent Ng, and '\n",
      "                                               'Xiaojun Wan. Hong Kong, China: '\n",
      "                                               'Association for Computational '\n",
      "                                               'Linguistics, Nov. 2019,\\n'\n",
      "                                               'pp. 4463–4473. doi: '\n",
      "                                               '10.18653/v1/D19-1454. url: '\n",
      "                                               'https://aclanthology.org/D19-1454/.\\n'\n",
      "                                               '[92] Imanol Schlag, Kazuki '\n",
      "                                               'Irie, and Jürgen Schmidhuber. '\n",
      "                                               '“Linear transformers are '\n",
      "                                               'secretly fast weight '\n",
      "                                               'programmers”.\\n'\n",
      "                                               'In: International Conference '\n",
      "                                               'on Machine Learning . PMLR. '\n",
      "                                               '2021, pp. 9355–9366.\\n'\n",
      "                                               '[93] JH Schmidhuber. “Learning '\n",
      "                                               'to control fast-weight '\n",
      "                                               'memories: An alternative to '\n",
      "                                               'recurrent nets. Accepted for\\n'\n",
      "                                               'publication in”. In: Neural '\n",
      "                                               'Computation (1992).\\n'\n",
      "                                               '[94] Jürgen Schmidhuber. '\n",
      "                                               '“Reducing the ratio between '\n",
      "                                               'learning complexity and number '\n",
      "                                               'of time varying variables\\n'\n",
      "                                               'in fully recurrent nets”. In: '\n",
      "                                               'ICANN’93: Proceedings of the '\n",
      "                                               'International Conference on '\n",
      "                                               'Artificial Neural Networks\\n'\n",
      "                                               'Amsterdam, The Netherlands '\n",
      "                                               '13–16 September 1993 3 . '\n",
      "                                               'Springer. 1993, pp. 460–463.\\n'\n",
      "                                               '[95] Jürgen Schmidhuber and '\n",
      "                                               'Sepp Hochreiter. “Long '\n",
      "                                               'Short-term Memory”. In: Neural '\n",
      "                                               'Computation MIT-Press (1997).\\n'\n",
      "                                               '[96] Avi Schwarzschild, Zhili '\n",
      "                                               'Feng, Pratyush Maini, Zachary '\n",
      "                                               'C Lipton, and J Zico Kolter. '\n",
      "                                               '“Rethinking llm memorization\\n'\n",
      "                                               'through the lens of '\n",
      "                                               'adversarial compression”. In: '\n",
      "                                               'arXiv preprint '\n",
      "                                               'arXiv:2404.15146 (2024).\\n'\n",
      "                                               '[97] Jimmy T.H. Smith, Andrew '\n",
      "                                               'Warrington, and Scott '\n",
      "                                               'Linderman. “Simplified State '\n",
      "                                               'Space Layers for Sequence '\n",
      "                                               'Modeling”.\\n'\n",
      "                                               'In: The Eleventh International '\n",
      "                                               'Conference on Learning '\n",
      "                                               'Representations . 2023. url: '\n",
      "                                               'https://openreview.net/forum?\\n'\n",
      "                                               'id=Ai8Hw3AXqks.\\n'\n",
      "                                               '[98] Robin Staab, Mark Vero, '\n",
      "                                               'Mislav Balunovic, and Martin '\n",
      "                                               'Vechev. “Beyond Memorization: '\n",
      "                                               'Violating Privacy via\\n'\n",
      "                                               'Inference with Large Language '\n",
      "                                               'Models”. In: The Twelfth '\n",
      "                                               'International Conference on '\n",
      "                                               'Learning Representations . '\n",
      "                                               '2024.\\n'\n",
      "                                               'url: '\n",
      "                                               'https://openreview.net/forum?id=kmn0BhQk7p.\\n'\n",
      "                                               '[99] Sainbayar Sukhbaatar, '\n",
      "                                               'Edouard Grave, Guillaume '\n",
      "                                               'Lample, Herve Jegou, and '\n",
      "                                               'Armand Joulin. “Augmenting '\n",
      "                                               'self-\\n'\n",
      "                                               'attention with persistent '\n",
      "                                               'memory”. In: arXiv preprint '\n",
      "                                               'arXiv:1907.01470 (2019).\\n'\n",
      "                                               '[100] Sainbayar Sukhbaatar, '\n",
      "                                               'Jason Weston, Rob Fergus, et '\n",
      "                                               'al. “End-to-end memory '\n",
      "                                               'networks”. In: Advances in '\n",
      "                                               'neural\\n'\n",
      "                                               'information processing systems '\n",
      "                                               '28 (2015).\\n'\n",
      "                                               '[101] Yu Sun, Xinhao Li, Karan '\n",
      "                                               'Dalal, Jiarui Xu, Arjun '\n",
      "                                               'Vikram, Genghan Zhang, Yann '\n",
      "                                               'Dubois, Xinlei Chen, Xiaolong\\n'\n",
      "                                               'Wang, Sanmi Koyejo, et al. '\n",
      "                                               '“Learning to (learn at test '\n",
      "                                               'time): Rnns with expressive '\n",
      "                                               'hidden states”. In: arXiv '\n",
      "                                               'preprint\\n'\n",
      "                                               'arXiv:2407.04620 (2024).\\n'\n",
      "                                               '[102] Yutao Sun, Li Dong, '\n",
      "                                               'Shaohan Huang, Shuming Ma, '\n",
      "                                               'Yuqing Xia, Jilong Xue, '\n",
      "                                               'Jianyong Wang, and Furu Wei. '\n",
      "                                               '“Retentive\\n'\n",
      "                                               'network: A successor to '\n",
      "                                               'transformer for large language '\n",
      "                                               'models”. In: arXiv preprint '\n",
      "                                               'arXiv:2307.08621 (2023).\\n'\n",
      "                                               '[103] Gemma Team, Thomas '\n",
      "                                               'Mesnard, Cassidy Hardin, '\n",
      "                                               'Robert Dadashi, Surya '\n",
      "                                               'Bhupatiraju, Shreya Pathak, '\n",
      "                                               'Laurent Sifre,\\n'\n",
      "                                               'Morgane Rivière, Mihir Sanjay '\n",
      "                                               'Kale, Juliette Love, et al. '\n",
      "                                               '“Gemma: Open models based on '\n",
      "                                               'gemini research and\\n'\n",
      "                                               'technology”. In: arXiv '\n",
      "                                               'preprint arXiv:2403.08295 '\n",
      "                                               '(2024).\\n'\n",
      "                                               '22\\n'\n",
      "                                               '[104] W Scott Terry. Learning '\n",
      "                                               'and memory: Basic principles, '\n",
      "                                               'processes, and procedures . '\n",
      "                                               'Routledge, 2017.\\n'\n",
      "                                               '[105] Matteo Tiezzi, Michele '\n",
      "                                               'Casoni, Alessandro Betti, '\n",
      "                                               'Tommaso Guidi, Marco Gori, and '\n",
      "                                               'Stefano Melacci. “On the\\n'\n",
      "                                               'resurgence of recurrent models '\n",
      "                                               'for long sequences: Survey and '\n",
      "                                               'research opportunities in the '\n",
      "                                               'transformer era”. In:\\n'\n",
      "                                               'arXiv preprint '\n",
      "                                               'arXiv:2402.08132 (2024).\\n'\n",
      "                                               '[106] Hugo Touvron, Thibaut '\n",
      "                                               'Lavril, Gautier Izacard, '\n",
      "                                               'Xavier Martinet, Marie-Anne '\n",
      "                                               'Lachaux, Timothée Lacroix, '\n",
      "                                               'Baptiste\\n'\n",
      "                                               'Rozière, Naman Goyal, Eric '\n",
      "                                               'Hambro, Faisal Azhar, et al. '\n",
      "                                               '“Llama: Open and efficient '\n",
      "                                               'foundation language models”.\\n'\n",
      "                                               'In: arXiv preprint '\n",
      "                                               'arXiv:2302.13971 (2023).\\n'\n",
      "                                               '[107] Jos Van Der Westhuizen '\n",
      "                                               'and Joan Lasenby. “The '\n",
      "                                               'unreasonable effectiveness of '\n",
      "                                               'the forget gate”. In:arXiv '\n",
      "                                               'preprint\\n'\n",
      "                                               'arXiv:1804.04849 (2018).\\n'\n",
      "                                               '[108] Ashish Vaswani, Noam '\n",
      "                                               'Shazeer, Niki Parmar, Jakob '\n",
      "                                               'Uszkoreit, Llion Jones, Aidan '\n",
      "                                               'N Gomez, Łukasz Kaiser,\\n'\n",
      "                                               'and Illia Polosukhin. '\n",
      "                                               '“Attention is All you Need”. '\n",
      "                                               'In: Advances in Neural '\n",
      "                                               'Information Processing Systems '\n",
      "                                               '. Ed.\\n'\n",
      "                                               'by I. Guyon, U. Von Luxburg, '\n",
      "                                               'S. Bengio, H. Wallach, R. '\n",
      "                                               'Fergus, S. Vishwanathan, and '\n",
      "                                               'R. Garnett. Vol. 30. Cur-\\n'\n",
      "                                               'ran Associates, Inc., 2017. '\n",
      "                                               'url: https : / / proceedings . '\n",
      "                                               'neurips . cc / paper _ files / '\n",
      "                                               'paper / 2017 / file /\\n'\n",
      "                                               '3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.\\n'\n",
      "                                               '[109] Shida Wang. “LongSSM: On '\n",
      "                                               'the Length Extension of '\n",
      "                                               'State-space Models in Language '\n",
      "                                               'Modelling”. In: arXiv '\n",
      "                                               'preprint\\n'\n",
      "                                               'arXiv:2406.02080 (2024).\\n'\n",
      "                                               '[110] Yu Wang, Yifan Gao, '\n",
      "                                               'Xiusi Chen, Haoming Jiang, '\n",
      "                                               'Shiyang Li, Jingfeng Yang, '\n",
      "                                               'Qingyu Yin, Zheng Li, Xian Li, '\n",
      "                                               'Bing Yin,\\n'\n",
      "                                               'Jingbo Shang, and Julian '\n",
      "                                               'McAuley. “MEMORYLLM: Towards '\n",
      "                                               'Self-Updatable Large Language '\n",
      "                                               'Models”. In:Forty-first\\n'\n",
      "                                               'International Conference on '\n",
      "                                               'Machine Learning . 2024. url: '\n",
      "                                               'https://openreview.net/forum?id=p0lKWzdikQ.\\n'\n",
      "                                               '[111] Yu Wang, Chi Han, '\n",
      "                                               'Tongtong Wu, Xiaoxin He, '\n",
      "                                               'Wangchunshu Zhou, Nafis Sadeq, '\n",
      "                                               'Xiusi Chen, Zexue He, Wei '\n",
      "                                               'Wang,\\n'\n",
      "                                               'Gholamreza Haffari, et al. '\n",
      "                                               '“Towards LifeSpan Cognitive '\n",
      "                                               'Systems”. In: arXiv preprint '\n",
      "                                               'arXiv:2409.13265 (2024).\\n'\n",
      "                                               '[112] Zhiwei Wang, Yao Ma, '\n",
      "                                               'Zitao Liu, and Jiliang Tang. '\n",
      "                                               '“R-transformer: Recurrent '\n",
      "                                               'neural network enhanced '\n",
      "                                               'transformer”.\\n'\n",
      "                                               'In: arXiv preprint '\n",
      "                                               'arXiv:1907.05572 (2019).\\n'\n",
      "                                               '[113] Jason Weston, Sumit '\n",
      "                                               'Chopra, and Antoine Bordes. '\n",
      "                                               '“Memory networks”. In: arXiv '\n",
      "                                               'preprint arXiv:1410.3916 '\n",
      "                                               '(2014).\\n'\n",
      "                                               '[114] Bernard Widrow and '\n",
      "                                               'Marcian E Hoff. “Adaptive '\n",
      "                                               'switching circuits”. In: '\n",
      "                                               'Neurocomputing: foundations of '\n",
      "                                               'research .\\n'\n",
      "                                               '1988, pp. 123–134.\\n'\n",
      "                                               '[115] Ronald J Williams and '\n",
      "                                               'David Zipser. “A learning '\n",
      "                                               'algorithm for continually '\n",
      "                                               'running fully recurrent neural '\n",
      "                                               'networks”.\\n'\n",
      "                                               'In: Neural computation 1.2 '\n",
      "                                               '(1989), pp. 270–280.\\n'\n",
      "                                               '[116] Daniel B Willingham. '\n",
      "                                               '“Systems of memory in the '\n",
      "                                               'human brain”. In: Neuron 18.1 '\n",
      "                                               '(1997), pp. 5–8.\\n'\n",
      "                                               '[117] Chao-Yuan Wu, Christoph '\n",
      "                                               'Feichtenhofer, Haoqi Fan, '\n",
      "                                               'Kaiming He, Philipp '\n",
      "                                               'Krahenbuhl, and Ross Girshick. '\n",
      "                                               '“Long-\\n'\n",
      "                                               'term feature banks for '\n",
      "                                               'detailed video understanding”. '\n",
      "                                               'In: Proceedings of the '\n",
      "                                               'IEEE/CVF conference on '\n",
      "                                               'computer vision\\n'\n",
      "                                               'and pattern recognition . '\n",
      "                                               '2019, pp. 284–293.\\n'\n",
      "                                               '[118] Haixu Wu, Tengge Hu, '\n",
      "                                               'Yong Liu, Hang Zhou, Jianmin '\n",
      "                                               'Wang, and Mingsheng Long. '\n",
      "                                               '“TimesNet: Temporal 2D-\\n'\n",
      "                                               'Variation Modeling for General '\n",
      "                                               'Time Series Analysis”. In: The '\n",
      "                                               'Eleventh International '\n",
      "                                               'Conference on Learning\\n'\n",
      "                                               'Representations. 2023. url: '\n",
      "                                               'https://openreview.net/forum?id=ju_Uqw384Oq.\\n'\n",
      "                                               '[119] Qingyang Wu, Zhenzhong '\n",
      "                                               'Lan, Kun Qian, Jing Gu, Alborz '\n",
      "                                               'Geramifard, and Zhou Yu. '\n",
      "                                               '“Memformer: A memory-\\n'\n",
      "                                               'augmented transformer for '\n",
      "                                               'sequence modeling”. In: arXiv '\n",
      "                                               'preprint arXiv:2010.06891 '\n",
      "                                               '(2020).\\n'\n",
      "                                               '[120] Guangxuan Xiao, Yuandong '\n",
      "                                               'Tian, Beidi Chen, Song Han, '\n",
      "                                               'and Mike Lewis. “Efficient '\n",
      "                                               'Streaming Language Models\\n'\n",
      "                                               'with Attention Sinks”. In: The '\n",
      "                                               'Twelfth International '\n",
      "                                               'Conference on Learning '\n",
      "                                               'Representations . 2024. url: '\n",
      "                                               'https:\\n'\n",
      "                                               '//openreview.net/forum?id=NG7sS51zVF.\\n'\n",
      "                                               '[121] An Yang, Baosong Yang, '\n",
      "                                               'Beichen Zhang, Binyuan Hui, Bo '\n",
      "                                               'Zheng, Bowen Yu, Chengyuan Li, '\n",
      "                                               'Dayiheng Liu, Fei\\n'\n",
      "                                               'Huang, Haoran Wei, et al. '\n",
      "                                               '“Qwen2. 5 Technical Report”. '\n",
      "                                               'In:arXiv preprint '\n",
      "                                               'arXiv:2412.15115 (2024).\\n'\n",
      "                                               '[122] Songlin Yang, Jan Kautz, '\n",
      "                                               'and Ali Hatamizadeh. “Gated '\n",
      "                                               'Delta Networks: Improving '\n",
      "                                               'Mamba2 with Delta Rule”. In:\\n'\n",
      "                                               'arXiv preprint '\n",
      "                                               'arXiv:2412.06464 (2024).\\n'\n",
      "                                               '[123] Songlin Yang, Bailin '\n",
      "                                               'Wang, Yikang Shen, Rameswar '\n",
      "                                               'Panda, and Yoon Kim. “Gated '\n",
      "                                               'Linear Attention Transformers\\n'\n",
      "                                               'with Hardware-Efficient '\n",
      "                                               'Training”. In: Forty-first '\n",
      "                                               'International Conference on '\n",
      "                                               'Machine Learning . 2024. url: '\n",
      "                                               'https:\\n'\n",
      "                                               '//openreview.net/forum?id=ia5XvxFUJT.\\n'\n",
      "                                               '[124] Songlin Yang, Bailin '\n",
      "                                               'Wang, Yu Zhang, Yikang Shen, '\n",
      "                                               'and Yoon Kim. “Parallelizing '\n",
      "                                               'Linear Transformers with the\\n'\n",
      "                                               'Delta Rule over Sequence '\n",
      "                                               'Length”. In:The Thirty-eighth '\n",
      "                                               'Annual Conference on Neural '\n",
      "                                               'Information Processing Systems '\n",
      "                                               '.\\n'\n",
      "                                               '2024. url: '\n",
      "                                               'https://openreview.net/forum?id=y8Rm4VNRPH.\\n'\n",
      "                                               '[125] Luca Zancato, Arjun '\n",
      "                                               'Seshadri, Yonatan Dukler, '\n",
      "                                               'Aditya Golatkar, Yantao Shen, '\n",
      "                                               'Benjamin Bowman, Matthew '\n",
      "                                               'Trager,\\n'\n",
      "                                               'Alessandro Achille, and '\n",
      "                                               'Stefano Soatto. “B’MOJO: '\n",
      "                                               'Hybrid State Space '\n",
      "                                               'Realizations of Foundation '\n",
      "                                               'Models with\\n'\n",
      "                                               'Eidetic and Fading Memory”. '\n",
      "                                               'In: The Thirty-eighth Annual '\n",
      "                                               'Conference on Neural '\n",
      "                                               'Information Processing Systems '\n",
      "                                               '.\\n'\n",
      "                                               '2024. url: '\n",
      "                                               'https://openreview.net/forum?id=RnQdRY1h5v.\\n'\n",
      "                                               '23\\n'\n",
      "                                               '[126] Rowan Zellers, Ari '\n",
      "                                               'Holtzman, Yonatan Bisk, Ali '\n",
      "                                               'Farhadi, and Yejin Choi. '\n",
      "                                               '“HellaSwag: Can a Machine '\n",
      "                                               'Really Finish\\n'\n",
      "                                               'Your Sentence?” In: '\n",
      "                                               'Proceedings of the 57th Annual '\n",
      "                                               'Meeting of the Association for '\n",
      "                                               'Computational Linguistics . '\n",
      "                                               'Ed. by\\n'\n",
      "                                               'Anna Korhonen, David Traum, '\n",
      "                                               'and Lluís Màrquez. Florence, '\n",
      "                                               'Italy: Association for '\n",
      "                                               'Computational Linguistics, '\n",
      "                                               'July\\n'\n",
      "                                               '2019, pp. 4791–4800. doi: '\n",
      "                                               '10.18653/v1/P19-1472. url: '\n",
      "                                               'https://aclanthology.org/P19-1472/.\\n'\n",
      "                                               '[127] Ailing Zeng, Muxi Chen, '\n",
      "                                               'Lei Zhang, and Qiang Xu. “Are '\n",
      "                                               'transformers effective for '\n",
      "                                               'time series forecasting?” In:\\n'\n",
      "                                               'Proceedings of the AAAI '\n",
      "                                               'conference on artificial '\n",
      "                                               'intelligence . Vol. 37. 2023, '\n",
      "                                               'pp. 11121–11128.\\n'\n",
      "                                               '[128] Hao Zhang, Alexander C '\n",
      "                                               'Berg, Michael Maire, and '\n",
      "                                               'Jitendra Malik. “SVM-KNN: '\n",
      "                                               'Discriminative nearest '\n",
      "                                               'neighbor\\n'\n",
      "                                               'classification for visual '\n",
      "                                               'category recognition”. In: '\n",
      "                                               '2006 IEEE Computer Society '\n",
      "                                               'Conference on Computer Vision '\n",
      "                                               'and\\n'\n",
      "                                               'Pattern Recognition (CVPR’06) '\n",
      "                                               '. Vol. 2. IEEE. 2006, pp. '\n",
      "                                               '2126–2136.\\n'\n",
      "                                               '[129] Jianyu Zhang, Niklas '\n",
      "                                               'Nolte, Ranajoy Sadhukhan, '\n",
      "                                               'Beidi Chen, and Léon Bottou. '\n",
      "                                               '“Memory Mosaics”. In:arXiv '\n",
      "                                               'preprint\\n'\n",
      "                                               'arXiv:2405.06394 (2024).\\n'\n",
      "                                               '[130] Yunhao Zhang and Junchi '\n",
      "                                               'Yan. “Crossformer: Transformer '\n",
      "                                               'utilizing cross-dimension '\n",
      "                                               'dependency for multivariate\\n'\n",
      "                                               'time series forecasting”. In: '\n",
      "                                               'The eleventh international '\n",
      "                                               'conference on learning '\n",
      "                                               'representations . 2023.\\n'\n",
      "                                               '[131] Haoyi Zhou, Shanghang '\n",
      "                                               'Zhang, Jieqi Peng, Shuai '\n",
      "                                               'Zhang, Jianxin Li, Hui Xiong, '\n",
      "                                               'and Wancai Zhang. “Informer:\\n'\n",
      "                                               'Beyond efficient transformer '\n",
      "                                               'for long sequence time-series '\n",
      "                                               'forecasting”. In: Proceedings '\n",
      "                                               'of the AAAI conference on\\n'\n",
      "                                               'artificial intelligence . Vol. '\n",
      "                                               '35. 12. 2021, pp. '\n",
      "                                               '11106–11115.\\n'\n",
      "                                               '[132] Luisa Zintgraf, Kyriacos '\n",
      "                                               'Shiarli, Vitaly Kurin, Katja '\n",
      "                                               'Hofmann, and Shimon Whiteson. '\n",
      "                                               '“Fast context adaptation via\\n'\n",
      "                                               'meta-learning”. In: '\n",
      "                                               'International Conference on '\n",
      "                                               'Machine Learning . PMLR. 2019, '\n",
      "                                               'pp. 7693–7702.\\n'\n",
      "                                               '24\\n'\n",
      "                                               'A Related Work\\n'\n",
      "                                               'There are diverse perspectives '\n",
      "                                               'that can independently lead to '\n",
      "                                               'the design of Titans or its '\n",
      "                                               'components. Accordingly, to\\n'\n",
      "                                               'further situate our work in a '\n",
      "                                               'broader context, we review '\n",
      "                                               'three categories of studies:\\n'\n",
      "                                               'A.1 Linear Recurrent Models\\n'\n",
      "                                               'Recently, to address the '\n",
      "                                               'computational cost of '\n",
      "                                               'Transformers in both training '\n",
      "                                               'and inference, linear '\n",
      "                                               'recurrent models\\n'\n",
      "                                               'have attracted much attention '\n",
      "                                               '(Tiezzi et al. 2024), mainly '\n",
      "                                               'due to their fast inference '\n",
      "                                               'and training. The first '\n",
      "                                               'generation\\n'\n",
      "                                               'of models–such as RetNet '\n",
      "                                               '(Yutao Sun et al. 2023), LRU '\n",
      "                                               '(Orvieto et al. 2023), RWKV '\n",
      "                                               '(Peng, Alcaide, et al. 2023), '\n",
      "                                               'S5 (J. T.\\n'\n",
      "                                               'Smith, Warrington, and '\n",
      "                                               'Linderman 2023), and S4 (Gu, '\n",
      "                                               'Goel, and Re 2022)–uses '\n",
      "                                               'data-independent transition '\n",
      "                                               'matrix/decay\\n'\n",
      "                                               'mechanism. The second '\n",
      "                                               'generation of such models '\n",
      "                                               'started to incorporate gating '\n",
      "                                               'mechanism, a widely used '\n",
      "                                               'techniques\\n'\n",
      "                                               'in traditional RNNs (Gers, '\n",
      "                                               'Jürgen Schmidhuber, and '\n",
      "                                               'Cummins 2000; Greff et al. '\n",
      "                                               '2016; Van Der Westhuizen and '\n",
      "                                               'Lasenby\\n'\n",
      "                                               '2018), into such linear '\n",
      "                                               'architectures–e.g., Griffin '\n",
      "                                               '(De et al. 2024), SSMs '\n",
      "                                               '(Behrouz, Santacatterina, and '\n",
      "                                               'Zabih 2024; Dao\\n'\n",
      "                                               'and Gu 2024; Gu and Dao 2024; '\n",
      "                                               'Hasani et al. 2023), RWKV6 '\n",
      "                                               '(Peng, Goldstein, et al. '\n",
      "                                               '2024). The third generation of '\n",
      "                                               'linear\\n'\n",
      "                                               'recurrent models are based on '\n",
      "                                               'more complex memory updating '\n",
      "                                               'rule based on meta-learning, '\n",
      "                                               'online learning, and/or\\n'\n",
      "                                               'delta-rule, resulting in more '\n",
      "                                               'expressive and effective '\n",
      "                                               'models such as: Longhorn (B. '\n",
      "                                               'Liu et al. 2024), Gated '\n",
      "                                               'DeltaNet (S. Yang,\\n'\n",
      "                                               'Kautz, and Hatamizadeh 2024), '\n",
      "                                               'TTT (Yu Sun et al. 2024), and '\n",
      "                                               'DeltaNet (S. Yang, B. Wang, Yu '\n",
      "                                               'Zhang, et al. 2024). Our\\n'\n",
      "                                               'LMM model can be seen as the '\n",
      "                                               'next generation of such '\n",
      "                                               'models, in which we '\n",
      "                                               'incorporate the token flow '\n",
      "                                               'into the memory\\n'\n",
      "                                               'updating mechanism, having '\n",
      "                                               'more powerful memory updating '\n",
      "                                               'process. See Appendix C for a '\n",
      "                                               'detailed discussion of\\n'\n",
      "                                               'different recurrent models and '\n",
      "                                               'Titans.\\n'\n",
      "                                               'A.2 Transformer-based '\n",
      "                                               'Architectures\\n'\n",
      "                                               'Transformers. Transformers '\n",
      "                                               '(Vaswani et al. 2017) as the '\n",
      "                                               'de facto backbone for many '\n",
      "                                               'deep learning models are based '\n",
      "                                               'on\\n'\n",
      "                                               'attention mechanism (Bahdanau '\n",
      "                                               '2014). They, however, suffer '\n",
      "                                               'from quadratic computational '\n",
      "                                               'cost, limiting their ability\\n'\n",
      "                                               'to scale to long context '\n",
      "                                               'window. To improve the memory '\n",
      "                                               'consumption and throughput of '\n",
      "                                               'softmax attention for longer\\n'\n",
      "                                               'sequences, various studies '\n",
      "                                               'focused on I/O aware '\n",
      "                                               'implementations of attention '\n",
      "                                               '(Dao 2024; Dao, D. Fu, et al. '\n",
      "                                               '2022), designing\\n'\n",
      "                                               'more efficient attention '\n",
      "                                               'mechanisms by sparsifying the '\n",
      "                                               'attention matrix (B. Chen et '\n",
      "                                               'al. 2021; Choromanski et al. '\n",
      "                                               '2021; Dai\\n'\n",
      "                                               'et al. 2019; J. Dong et al. '\n",
      "                                               '2024; Roy et al. 2021), '\n",
      "                                               'approximating the softmax '\n",
      "                                               '(Arora et al. 2024), or '\n",
      "                                               'developing kernel-based\\n'\n",
      "                                               '(linear) attentions (Aksenov '\n",
      "                                               'et al. 2024; Kacham, Mirrokni, '\n",
      "                                               'and P. Zhong 2024; Schlag, '\n",
      "                                               'Irie, and Jürgen Schmidhuber '\n",
      "                                               '2021;\\n'\n",
      "                                               'S. Yang, B. Wang, Shen, et al. '\n",
      "                                               '2024).\\n'\n",
      "                                               'Segment-based Transformers. '\n",
      "                                               'Another line of research to '\n",
      "                                               'improve the efficiency of '\n",
      "                                               'Transformers is segment-based '\n",
      "                                               'or\\n'\n",
      "                                               'Chunk Transformers (Dai et al. '\n",
      "                                               '2019). The main drawback of '\n",
      "                                               'chunk Transformers is that '\n",
      "                                               'segments are fully separated '\n",
      "                                               'and\\n'\n",
      "                                               'so the context window is '\n",
      "                                               'limited to the length of the '\n",
      "                                               'chunks. To address this issue, '\n",
      "                                               'various studies discuss the '\n",
      "                                               'importance\\n'\n",
      "                                               'of a memory so it can help the '\n",
      "                                               'model to transfer information '\n",
      "                                               'across chunks (Bulatov, Yuri '\n",
      "                                               'Kuratov, et al. 2023; '\n",
      "                                               'Bulatov,\\n'\n",
      "                                               'Yury Kuratov, and Burtsev '\n",
      "                                               '2022; Feng et al. 2022; '\n",
      "                                               'Hutchins et al. 2022; Rodkin '\n",
      "                                               'et al. 2024; Z. Wang et al. '\n",
      "                                               '2019; Q. Wu\\n'\n",
      "                                               'et al. 2020; Zancato et al. '\n",
      "                                               '2024). The key differences of '\n",
      "                                               'Titans with these models are: '\n",
      "                                               '(1) The memory in such models '\n",
      "                                               'are\\n'\n",
      "                                               'simple small size vectors, '\n",
      "                                               'lacking expressive power to '\n",
      "                                               'compress complex information; '\n",
      "                                               '(2) The memory module lacks '\n",
      "                                               'forget\\n'\n",
      "                                               'mechanism, leading to a fast '\n",
      "                                               'memory overflow; (3) only '\n",
      "                                               'focus on momentary surprise, '\n",
      "                                               'missing the information flow. '\n",
      "                                               'More\\n'\n",
      "                                               'specifically, recalling '\n",
      "                                               'Recurrent Memory Transformers '\n",
      "                                               '(RMT) (Bulatov, Yuri Kuratov, '\n",
      "                                               'et al. 2023; Bulatov, Yury '\n",
      "                                               'Kuratov,\\n'\n",
      "                                               'and Burtsev 2022; Rodkin et '\n",
      "                                               'al. 2024), one can treat '\n",
      "                                               'Titans (MAC) as the '\n",
      "                                               'generalization of RMT, where '\n",
      "                                               'we use a neural\\n'\n",
      "                                               'memory module instead of a '\n",
      "                                               'vector-valued small size '\n",
      "                                               'memory.\\n'\n",
      "                                               'Memory for Large Language '\n",
      "                                               'Models. Another interesting '\n",
      "                                               'research direction has been to '\n",
      "                                               'incorporate external memory\\n'\n",
      "                                               'modules to LLMs after training '\n",
      "                                               '(Z. He et al. 2024; Khandelwal '\n",
      "                                               'et al. 2020; Y. Wang, Y. Gao, '\n",
      "                                               'et al. 2024). Such models\\n'\n",
      "                                               'are different from our '\n",
      "                                               'approach as we incorporate the '\n",
      "                                               'memory as a part of initial '\n",
      "                                               'architecture and so we train '\n",
      "                                               'it in\\n'\n",
      "                                               'an end-to-end manner. Also, '\n",
      "                                               'most of these explicit memory '\n",
      "                                               'modules suffer from the same '\n",
      "                                               'limitations as chunk-based\\n'\n",
      "                                               'Transformers (mentioned '\n",
      "                                               'above). For a detailed '\n",
      "                                               'discussion of such models, we '\n",
      "                                               'refer to the recent study of '\n",
      "                                               'Y. Wang, Han,\\n'\n",
      "                                               'et al. (2024).\\n'\n",
      "                                               '25\\n'\n",
      "                                               'A.3 Test Time Training and '\n",
      "                                               'Fast Weight Programs\\n'\n",
      "                                               'Memory Design and Augmentation '\n",
      "                                               'with Memory. In the '\n",
      "                                               'literature, a substantial '\n",
      "                                               'research effort have been '\n",
      "                                               'toward\\n'\n",
      "                                               'designing memory modules that '\n",
      "                                               'are capable of either '\n",
      "                                               'memorizing the knowledge '\n",
      "                                               'abstraction (e.g., persistent '\n",
      "                                               'mem-\\n'\n",
      "                                               'ory) (Sukhbaatar, Grave, et '\n",
      "                                               'al. 2019), or memorizing the '\n",
      "                                               'data-dependent information '\n",
      "                                               '(also known as contextual '\n",
      "                                               'memory),\\n'\n",
      "                                               'through recurrence (Bulatov, '\n",
      "                                               'Yury Kuratov, and Burtsev '\n",
      "                                               '2022; Rodkin et al. 2024; '\n",
      "                                               'Zancato et al. 2024), '\n",
      "                                               'Transformers (Berges\\n'\n",
      "                                               'et al. 2024; Cetin et al. '\n",
      "                                               '2024; Feng et al. 2022; Le, '\n",
      "                                               'Tran, and Venkatesh 2020; '\n",
      "                                               'Munkhdalai, Faruqui, and Gopal '\n",
      "                                               '2024; J. Zhang\\n'\n",
      "                                               'et al. 2024), gradient (Irie, '\n",
      "                                               'Csordás, and Jürgen '\n",
      "                                               'Schmidhuber 2022; Munkhdalai, '\n",
      "                                               'Sordoni, et al. 2019), or '\n",
      "                                               'other learning\\n'\n",
      "                                               'paradigms (Sukhbaatar, Weston, '\n",
      "                                               'Fergus, et al. 2015; Weston, '\n",
      "                                               'Chopra, and Bordes 2014). '\n",
      "                                               'These memory models, however,\\n'\n",
      "                                               'either (1) are based on '\n",
      "                                               'momentary surprise, missing '\n",
      "                                               'the data flow and events, (2) '\n",
      "                                               'lack forget mechanisms to '\n",
      "                                               'remove\\n'\n",
      "                                               'the memory, leading to a fast '\n",
      "                                               'memory overflow (3) are '\n",
      "                                               'fixed-size shallow (matrix '\n",
      "                                               'valued) memory, resulting in '\n",
      "                                               'poor\\n'\n",
      "                                               'performance in long context, '\n",
      "                                               'and (4) are based on fixed '\n",
      "                                               'parameters at test time, '\n",
      "                                               'lacking test time adaption.\\n'\n",
      "                                               'Fast Weight Programs. The idea '\n",
      "                                               'of seeing linear layers as the '\n",
      "                                               'key-value (associative) memory '\n",
      "                                               'system backs to fast\\n'\n",
      "                                               'weight programs, in which '\n",
      "                                               'dynamic fast programs are '\n",
      "                                               'incorporated into recurrent '\n",
      "                                               'neural networks to serve as '\n",
      "                                               'writable\\n'\n",
      "                                               'memory (Schlag, Irie, and '\n",
      "                                               'Jürgen Schmidhuber 2021; JH '\n",
      "                                               'Schmidhuber 1992; Jürgen '\n",
      "                                               'Schmidhuber 1993). The two '\n",
      "                                               'learning\\n'\n",
      "                                               'rules of Hebbian (Hebb 2005) '\n",
      "                                               'and delta (Prados and Kak '\n",
      "                                               '1989) are the most popular '\n",
      "                                               'learning rules for fast weight '\n",
      "                                               'programs,\\n'\n",
      "                                               'which have been extensively '\n",
      "                                               'explored in various studies '\n",
      "                                               '(Irie, Schlag, et al. 2021; '\n",
      "                                               'Munkhdalai, Sordoni, et al. '\n",
      "                                               '2019;\\n'\n",
      "                                               'Munkhdalai and H. Yu 2017; '\n",
      "                                               'Schlag, Irie, and Jürgen '\n",
      "                                               'Schmidhuber 2021; JH '\n",
      "                                               'Schmidhuber 1992; S. Yang, '\n",
      "                                               'Kautz, and\\n'\n",
      "                                               'Hatamizadeh 2024; S. Yang, B. '\n",
      "                                               'Wang, Yu Zhang, et al. 2024). '\n",
      "                                               'All these models, however, are '\n",
      "                                               'based on momentary surprise,\\n'\n",
      "                                               'missing the token flow in the '\n",
      "                                               'sequences (see Section 3.1), '\n",
      "                                               'and most of them lacks a '\n",
      "                                               'forgetting gate, resulting in '\n",
      "                                               'a poor\\n'\n",
      "                                               'memory management.\\n'\n",
      "                                               'Test Time Training. The key '\n",
      "                                               'ideas of learning at test time '\n",
      "                                               'or learning to learn (i.e., '\n",
      "                                               '(Andrychowicz et al. 2016)) '\n",
      "                                               'backs to\\n'\n",
      "                                               'very early studies on local '\n",
      "                                               'learning Bottou and Vapnik '\n",
      "                                               '1992, in which each test data '\n",
      "                                               'sample is trained on its '\n",
      "                                               'neighbors\\n'\n",
      "                                               'before making a prediction '\n",
      "                                               '(Gandelsman et al. 2022; H. '\n",
      "                                               'Zhang et al. 2006). This '\n",
      "                                               'approach further has shown '\n",
      "                                               'promising\\n'\n",
      "                                               'performance in vision tasks '\n",
      "                                               '(Jain and Learned-Miller 2011; '\n",
      "                                               'Mullapudi et al. 2019), mostly '\n",
      "                                               'due to their ability to '\n",
      "                                               'mitigate\\n'\n",
      "                                               'out-of-distribution samples. '\n",
      "                                               'The most similar studies to '\n",
      "                                               'ours in this direction are MNM '\n",
      "                                               '(Munkhdalai, Sordoni, et al. '\n",
      "                                               '2019)\\n'\n",
      "                                               'and TTT-layer (Yu Sun et al. '\n",
      "                                               '2024), which we discussed the '\n",
      "                                               'key differences in Appendix '\n",
      "                                               'C.\\n'\n",
      "                                               'B Language Modeling and '\n",
      "                                               'Common-sense Reasoning '\n",
      "                                               'Datasets\\n'\n",
      "                                               'Following recent studies on '\n",
      "                                               'linear recurrent models (Dao '\n",
      "                                               'and Gu 2024; S. Yang, Kautz, '\n",
      "                                               'and Hatamizadeh 2024; S. '\n",
      "                                               'Yang,\\n'\n",
      "                                               'B. Wang, Yu Zhang, et al. '\n",
      "                                               '2024), we use Wikitext (Merity '\n",
      "                                               'et al. 2017), LMB (Paperno et '\n",
      "                                               'al. 2016), PIQA (Bisk et al. '\n",
      "                                               '2020),\\n'\n",
      "                                               'HellaSwag (Zellers et al. '\n",
      "                                               '2019), WinoGrande (Sakaguchi '\n",
      "                                               'et al. 2021), ARC-easy (ARC-e) '\n",
      "                                               'and ARC-challenge (ARC-c) (P.\\n'\n",
      "                                               'Clark et al. 2018), SIQA (Sap '\n",
      "                                               'et al. 2019), and BoolQ (C. '\n",
      "                                               'Clark et al. 2019). Also, the '\n",
      "                                               'baselines results for 400M '\n",
      "                                               'models are\\n'\n",
      "                                               'from the reported results by '\n",
      "                                               'S. Yang, Kautz, and '\n",
      "                                               'Hatamizadeh (2024).\\n'\n",
      "                                               'C Long-term Memory Module '\n",
      "                                               '(LMM) as a Sequence Model\\n'\n",
      "                                               'In this section, we discuss '\n",
      "                                               'how LMM as a sequence model is '\n",
      "                                               'connected to modern linear '\n",
      "                                               'recurrent models. For the '\n",
      "                                               'sake\\n'\n",
      "                                               'of simplicity, we start with a '\n",
      "                                               'linear memory, where M𝑡 = 𝑊𝑡 '\n",
      "                                               '∈R𝑑in ×𝑑in . In this case, our '\n",
      "                                               'objective function becomes\\n'\n",
      "                                               'ℓ(M; 𝑥𝑡)= 1\\n'\n",
      "                                               '2 ∥M𝑡k𝑡 −v𝑡∥2\\n'\n",
      "                                               '2, in which we use gradient '\n",
      "                                               'descent with momentum and '\n",
      "                                               'weight decay for the '\n",
      "                                               'optimization.\\n'\n",
      "                                               'Accordingly, revisiting the '\n",
      "                                               'recurrent formula in Equation '\n",
      "                                               '13:\\n'\n",
      "                                               'M𝑡 = diag (1 −𝛼𝑡)M𝑡 +𝑆𝑡 (32)\\n'\n",
      "                                               '𝑆𝑡 = diag (𝜂𝑡)𝑆𝑡−1 −diag '\n",
      "                                               '(𝜃𝑡)\\x00M𝑡−1k⊤\\n'\n",
      "                                               '𝑡 k𝑡 −v⊤\\n'\n",
      "                                               '𝑡 k𝑡\\n'\n",
      "                                               '\\x01 . (33)\\n'\n",
      "                                               'LMM is Generalized Gated '\n",
      "                                               'DeltaNet. As discussed by S. '\n",
      "                                               'Yang, Kautz, and Hatamizadeh '\n",
      "                                               '(2024), DeltaNet (S. Yang, B. '\n",
      "                                               'Wang,\\n'\n",
      "                                               'Yu Zhang, et al. 2024) can '\n",
      "                                               'alternatively be interpreted '\n",
      "                                               'as an online learning problem '\n",
      "                                               'that optimizes the L= 1\\n'\n",
      "                                               '2 ∥S𝑡k𝑡 −v𝑡∥2\\n'\n",
      "                                               '2,\\n'\n",
      "                                               'resulting in:\\n'\n",
      "                                               'S𝑡+1 = S𝑡 −𝜃𝑡∇L= S𝑡\\n'\n",
      "                                               '\\x00I −𝜃𝑡k𝑡k⊤\\n'\n",
      "                                               '𝑡\\n'\n",
      "                                               '\\x01 +𝜃𝑡v𝑡k⊤\\n'\n",
      "                                               '𝑡 . (34)\\n'\n",
      "                                               '26\\n'\n",
      "                                               'In this formulation, Gated '\n",
      "                                               'DeltaNet is the same as above '\n",
      "                                               'but with an additional weight '\n",
      "                                               'decay term (S. Yang, Kautz, '\n",
      "                                               'and\\n'\n",
      "                                               'Hatamizadeh 2024). Comparing '\n",
      "                                               'Equation 32 and Equation 34, '\n",
      "                                               'we can see that setting 𝜂𝑡 = 0 '\n",
      "                                               'results in both formulations '\n",
      "                                               'to\\n'\n",
      "                                               'be equivalent. Accordingly, we '\n",
      "                                               'can say LMM is generalizing '\n",
      "                                               'the very recent study of Gated '\n",
      "                                               'DeltaNet (S. Yang, Kautz, and\\n'\n",
      "                                               'Hatamizadeh 2024) from three '\n",
      "                                               'aspects:\\n'\n",
      "                                               '• Momentum-based Rule: The '\n",
      "                                               'Delta Rule is based on '\n",
      "                                               'momentary surprise, meaning '\n",
      "                                               'that the flow of tokens '\n",
      "                                               'cannot\\n'\n",
      "                                               'affect the memory update rule. '\n",
      "                                               'LMM, however, is based on a '\n",
      "                                               'momentum rule, which consider '\n",
      "                                               'both past and\\n'\n",
      "                                               'momentary surprise.\\n'\n",
      "                                               '• Deep Memory: While Gated '\n",
      "                                               'DeltaNet is limited to a '\n",
      "                                               'linear (matrix-valued) memory '\n",
      "                                               'as it requires finding the '\n",
      "                                               'closed\\n'\n",
      "                                               'recurrence form, LMM allows '\n",
      "                                               'using deep memory module by '\n",
      "                                               'using a gradient-based '\n",
      "                                               'formulation, resulting in '\n",
      "                                               'higher\\n'\n",
      "                                               'expressive power.\\n'\n",
      "                                               '• Non-Linear Recurrence: While '\n",
      "                                               'DeltaNet and Gated DeltaNet '\n",
      "                                               'are based on linear '\n",
      "                                               'recurrence, our LMM is using\\n'\n",
      "                                               'inter-chunk non-linear '\n",
      "                                               'recurrence and intra-chunk '\n",
      "                                               'linear recurrence. This design '\n",
      "                                               'allows LMM having a higher\\n'\n",
      "                                               'expressive power.\\n'\n",
      "                                               'Here, we discussed Gated '\n",
      "                                               'DeltaNet as a sample of recent '\n",
      "                                               'generation of recurrent '\n",
      "                                               'models. Similar approaches '\n",
      "                                               'such\\n'\n",
      "                                               'as RWKV-7 (Peng 2021) are also '\n",
      "                                               'using the same formulation and '\n",
      "                                               'loss function, and so LMM is '\n",
      "                                               'generalizing all such\\n'\n",
      "                                               'models.\\n'\n",
      "                                               'LMM is Generalized Longhorn. '\n",
      "                                               'Similar to DeltaNet, Longhorn '\n",
      "                                               '(B. Liu et al. 2024) uses the '\n",
      "                                               'same loss function but it\\n'\n",
      "                                               'derives the closed form using '\n",
      "                                               'implicit online learning:\\n'\n",
      "                                               'S𝑡+1 = S𝑡\\n'\n",
      "                                               '\\x00I −𝛿𝑡k𝑡k⊤\\n'\n",
      "                                               '𝑡\\n'\n",
      "                                               '\\x01 +𝛿𝑡v𝑡k⊤\\n'\n",
      "                                               '𝑡 , (35)\\n'\n",
      "                                               'where 𝛿𝑡 = 𝜃𝑡\\n'\n",
      "                                               '1+𝜃𝑡 k𝑡 k⊤\\n'\n",
      "                                               '𝑡\\n'\n",
      "                                               '. It, however, lacks a '\n",
      "                                               'forgetting gate, resulting in '\n",
      "                                               'a faster memory overflow. '\n",
      "                                               'Therefore, in addition two\\n'\n",
      "                                               'the abovementioned aspects of '\n",
      "                                               '(1) Momentum-based Rule, (2) '\n",
      "                                               'Deep Memory, and (3) '\n",
      "                                               'Non-Linear Recurrence, LMM '\n",
      "                                               'has\\n'\n",
      "                                               'the advantage of using an '\n",
      "                                               'additional (4) Forget Gate, '\n",
      "                                               'leading to a better memory '\n",
      "                                               'management.\\n'\n",
      "                                               'LMM is Generalized TTT Layer. '\n",
      "                                               'To the best of our knowledge, '\n",
      "                                               'TTT (Yu Sun et al. 2024), is '\n",
      "                                               'the only modern linear\\n'\n",
      "                                               'recurrent models with a '\n",
      "                                               'gradient-based updating rule. '\n",
      "                                               'In addition to different '\n",
      "                                               'architectural designs and also '\n",
      "                                               'objective\\n'\n",
      "                                               'functions, our LMM has three '\n",
      "                                               'key differences with presented '\n",
      "                                               'TTT layers (Yu Sun et al. '\n",
      "                                               '2024):\\n'\n",
      "                                               '1. Forgetting Mechanism : TTT '\n",
      "                                               'layers are updating memory at '\n",
      "                                               'each time, without having the '\n",
      "                                               'chance to forget the\\n'\n",
      "                                               'past data. Accordingly, when '\n",
      "                                               'fixing the memory size, the '\n",
      "                                               'model cannot manage the memory '\n",
      "                                               'for long sequences. A\\n'\n",
      "                                               'forget mechanism, such as '\n",
      "                                               'LMM’s, allows clearing the '\n",
      "                                               'memory when very past '\n",
      "                                               'information is not needed '\n",
      "                                               'anymore.\\n'\n",
      "                                               'We show that in a general '\n",
      "                                               'case, this forget mechanism is '\n",
      "                                               'equivalent to weight decay and '\n",
      "                                               'provide a fast method to\\n'\n",
      "                                               'incorporate it into the '\n",
      "                                               'parallel training.\\n'\n",
      "                                               '2. Momentum-based Update Rule '\n",
      "                                               ': TTT layers are based on '\n",
      "                                               'momentary surprise, meaning '\n",
      "                                               'that the flow of tokens\\n'\n",
      "                                               'cannot affect the memory '\n",
      "                                               'update rule. LMM, however, is '\n",
      "                                               'based on a momentum rule, '\n",
      "                                               'which consider both past and\\n'\n",
      "                                               'momentary surprise. See '\n",
      "                                               'Section 3.1 for the motivation '\n",
      "                                               'of this design.\\n'\n",
      "                                               '3. Deep Memory : While '\n",
      "                                               'TTT-layers allows for deeper '\n",
      "                                               'memory, the '\n",
      "                                               'advantages/disadvantages of '\n",
      "                                               'such deeper memory\\n'\n",
      "                                               'modules have not been '\n",
      "                                               'experimentally evaluated.\\n'\n",
      "                                               'To the best of our knowledge, '\n",
      "                                               'our neural long-term memory '\n",
      "                                               'module is the first linear '\n",
      "                                               'recurrent model with '\n",
      "                                               'momentum-\\n'\n",
      "                                               'based update rule.\\n'\n",
      "                                               'Finally, as a key difference '\n",
      "                                               'with all the above and other '\n",
      "                                               'recent linear recurrent '\n",
      "                                               'studies, note that the hybrid '\n",
      "                                               'variants of\\n'\n",
      "                                               'modern linear models–such as '\n",
      "                                               'Griffin (De et al. 2024), '\n",
      "                                               'DeltaNet (S. Yang, B. Wang, Yu '\n",
      "                                               'Zhang, et al. 2024), Gated '\n",
      "                                               'DeltaNet (S.\\n'\n",
      "                                               'Yang, Kautz, and Hatamizadeh '\n",
      "                                               '2024), H3 (D. Y. Fu et al. '\n",
      "                                               '2023), Mamba2 (Dao and Gu '\n",
      "                                               '2024), Samba (Ren et al. '\n",
      "                                               '2024), etc.–all\\n'\n",
      "                                               'are based on sequential '\n",
      "                                               'layer-wise design. We present '\n",
      "                                               'Titans to show how effectively '\n",
      "                                               'one can incorporate such '\n",
      "                                               'memory\\n'\n",
      "                                               'modules into an architecture.\\n'\n",
      "                                               '27'}}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'status': 'success',\n",
       " 'extracted_info': {'document_id': 'arXiv_2501.00663v1',\n",
       "  'title': 'Titans: Learning to Memorize at Test Time',\n",
       "  'publication_date': '2024-12-31',\n",
       "  'authors': ['Ali Behrouz', 'Peilin Zhong', 'Vahab Mirrokni'],\n",
       "  'Key_words': ['Titans',\n",
       "   'neural memory',\n",
       "   'long-term memory',\n",
       "   'attention',\n",
       "   'recurrent models',\n",
       "   'language modeling',\n",
       "   'test-time learning'],\n",
       "  'key_points': ['Introduces a new neural long-term memory module that learns to memorize historical context.',\n",
       "   'Proposes Titans, a family of architectures combining short-term attention and long-term memory.',\n",
       "   \"Demonstrates Titans' scalability to context windows larger than 2M tokens.\",\n",
       "   'Shows Titans outperform Transformers and modern linear recurrent models in various tasks.',\n",
       "   'Presents three variants of Titans: Memory as a Context (MAC), Memory as Gating (MAG), and Memory as a Layer (MAL).'],\n",
       "  'summary': 'The paper introduces Titans, a novel family of architectures that integrate a neural long-term memory module with attention mechanisms to address the limitations of Transformers in handling long contexts. The proposed models demonstrate superior performance in language modeling, reasoning, and long-context tasks, scaling effectively to over 2M tokens.',\n",
       "  'methodology': \"The study designs a neural long-term memory module inspired by human memory systems, incorporating mechanisms for surprise-based learning, momentum, and adaptive forgetting. It evaluates Titans' performance across diverse tasks, including language modeling, reasoning, and time series forecasting, using benchmarks like RULER and BABILong.\",\n",
       "  'processed_timestamp': '2024-01-21T10:00:00.000Z'}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow_run(pdf_path=pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e873f1a4-784c-413f-822b-8da2a4ec0165",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
